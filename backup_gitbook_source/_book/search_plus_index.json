{"./":{"url":"./","title":"前言","keywords":"","body":"LSF handbook 停更与迁移通知 本文档网站于 2022-10-01 之后，将不在更新，仅保留用于访问。 最新的访问地址，见该仓库的 github 主页。（因为还没定好用哪个工具帮忙托管，先暂定 github pages 吧） 更详细的信息，也可通过下一页的迁移通知进行了解，谢谢！ 内容简介 主要内容是 IBM 官方 LSF manual 的文档翻译，具体内容涉及 LSF 的产品介绍、安装升级、用户操作、作业调度、集群运维、功能开发及拓展等。 其次结合译者的工作需求，会有一些相关知识点的增补，与实际操作经验的总结。大致包含 Linux 运行环境的常见服务配置、vim 编辑器操作、系统性能调优、队列日志分析、EDA 作业优化、同类调度器（Slurm/PBS）的功能对比等等。 重点章节 依照 Part > Chapter > Section > Subsection > Article 的行文结构 Part I 入门介绍篇 chapter1 LSF 介绍 重点： LSF 快速入门章节 chapter2 安装、升级与迁移 Part II 基础操作篇 chapter3 用户操作基础 重点：文件目录，LSF 守护程序与进程，作业生命周期，调度策略 chapter4 管理员操作基础 重点：重要配置文件、服务的启动，资源管理等，日志排错 Part III 作业调度篇 chapter5 作业调度管理 重点：LSF daemons 相关， bsub 命令参数及功能 Part IV 集群运维篇 chapter6 集群维护管理 重点： chapter7 参考文档 重点： Part V 功能拓展篇 chapter8 LSF 拓展 chapter9 最佳实践与建议 chapter10 LSF licence scheduler Part VI 经验总结篇 chapter11 Linux 操作进阶 重点：常见服务操作、免密、文件服务器、bash 脚本编程规范、vim 编辑器等 chapter12 实际实施经验 重点：日志分析，高级调度策略实施等 chapter13 调度器产品对比、行业领域结合等 重点：Slurm，PBS等 译作初衷 IBM 旗下的作业调度系统 LSF， 作为一款在 HPC 领域内应用广泛的商业调度器，其 manual 是针对多种商业客户而编写的，文档受众主要是各大中小型企业的集群管理者，其次则为数量更多的集群使用者，与少部分功能开发者。但实际上，因为每个企业 / 非企业级用户的软硬件基础架构，与业务场景会有不同，所以，作为集群的管理者，除了需要熟悉官网中介绍的功能操作外，也有必要结合实际的工作需求，基于所在行业，进行实际经验的总结与梳理等。 故而，本 LSF 中文手册是从集群管理及二次开发者的角度出发，基于 LSF manual，进行的一些翻译与增补，鉴于译者水平精力有限，出现错误纰漏之处在所难免，希望读者不吝批评指正。 版本 基于 版本为 LSF 10.1.0 的 LSF manual。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-27 22:49:02 "},"NOTIFICATION.html":{"url":"NOTIFICATION.html","title":"迁移通知","keywords":"","body":"迁移通知 更换编辑及部署工具 因为部分图片链接失效，网页响应时间过长等原因，导致严重影响阅读体验，所以，决定放弃该站点使用的 gitbook 工具，改用 mkdocs 来进行编辑更新，并使用 github pages 部署。 当前部署的版本，标记了一个 v1.0.0 的 tag，作为备份。 放弃全文翻译，改成\"点评＋经验\"的方式 同时，由于本站点主要内容源于官方 LSF manual，全文翻译的方式，既不能保证准确还原 manual 本义，也没实际必要（无论是对译者还是对读者），加之本人并不计划分配过多时间精力给这个项目，所以，会改用更合适的方式来不定期更新，大致如下： 对于基础性的内容，尽量全文准确翻译； 对于 reference 性质的内容，只是索引到原网址，但会根据使用经验给出阅读建议，即划重点； 根据个人实际经验及兴趣，有选择地翻译介绍某些工具，如 k8s，API 等； 分享整理一些搜集到的，不错的资源。 意见与参与 最后，如果你看到这个页面，说明你大概率正在查阅了解 LSF 的相关资料 ，如果有任何意见或建议，可以发 issues联系，如果有独到的使用经验或资源，欢迎分享，一起更新 （fork & pull request），共同践行开源理念，谢谢！ 访问地址 仓库地址：https://github.com/baiyongan/lsf-handbook 阅读地址：https://baiyongan.github.io/lsf-handbook 托管的工具，暂定是 github pages，如果国内访问网速太慢的话，应该还会改成其它的，最新阅读地址，请访问仓库地址进行确认。 新的网站，结构上需要较大调整，在 2022-10-01 左右部署好，然后会集中几次，更新必要的内容，剩下的就随缘更新了，欢迎接棒！ © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-27 22:18:39 "},"chapter1/LSF_introduction.html":{"url":"chapter1/LSF_introduction.html","title":"Chapter 1 LSF 介绍","keywords":"","body":"Chapter 1 LSF 介绍 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section1/brief_introduction.html":{"url":"chapter1/section1/brief_introduction.html","title":"1.1 LSF 简介","keywords":"","body":"1.1 LSF 简介 Spectrum LSF: 高效的集群管理系统 计算机通过执行程序，帮助科研人员进行科学研究。通常，计算机的使用者不关心程序的执行过程，他们只希望更快更有效地获取运算结果。而为了提供强大的计算能力，大量的计算资源以集群的形式出现。集群系统的使用和有效管理都面临着挑战。 LSF（Load Sharing Facility）是一款分布式集群管理系统软件，负责计算资源的管理和批处理作业的调度。它给用户提供统一的集群资源访问接口，让用户透明地访问整个集群资源。同时提供了丰富的功能和可定制的策略。LSF 具有良好的可伸缩性和高可用性，支持几乎所有的主流操作系统。它通常是高性能计算环境中不可或缺的基础软件。 LSF 虽然是一款商业软件，但它同时也提供免费的社区版供大家下载和使用。 简单的使用 LSF 的使用者可以大约分为两类，普通用户和集群系统管理员。普通用户可以通过命令，将计算程序提交给集群执行，获取计算结果。系统管理员可以通过配置文件和管理命令，管理集群以及统计计算资源的使用情况。 图 1. LSF 结构图 普通用户提交可执行程序或脚本给 LSF。LSF 将已提交的程序称为作业。作业在LSF 的队列 (Queue) 里排队 (PEND) ，等待调度。 清单 1. 提交作业 lsfrhel01 # bsub –R \"linux\" sleep 1000 Job is submitted to default queue . lsfrhel01 # bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1 tom PEND normal lsfrhel01 *eep 1000 May 9 15:42 LSF 根据配置的调度策略，把作业分配到最合适的计算节点上执行 (RUN) 。用户可以通过命令行查看，控制作业的执行过程。除此之外，LSF 还为用户提供了作业修改，需求描述，作业控制等多种命令行工具。 清单 2. 查看运行作业 lsfrhel01 # bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1 tom RUN normal lsfrhel01 lsfrhel02 *eep 1000 May 9 15:42 系统管理员通常需要了解整个集群系统中作业和资源的使用状况，LSF 提供的命令帮助管理员快速直观地看到系统概况：系统中队列的状态，机器的状态，作业的资源使用概况，等等。除此之外，LSF 还为管理员提供了丰富的集群配置，控制，管理等功能。 清单 3. 查看 LSF 系统信息 lsfrhel01 # bqueues normal QUEUE_NAME PRIO STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SUSP normal 30 Open:Active - - - - 1 0 1 0 lsfrhel01 # bhosts HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV lsfrhel01 ok - 4 0 0 0 0 0 lsfrhel02 ok - 8 1 1 0 0 0 lsfrhel01 # bacct Accounting information about jobs that are: - submitted by users tom, - accounted on all projects. - completed normally or exited - executed on all hosts. - submitted to all queues. - accounted on all service classes. ------------------------------------------------------------------------------ SUMMARY: ( time unit: second ) Total number of done jobs: 1 Total number of exited jobs: 4 Total CPU time consumed: 4.4 Average CPU time consumed: 0.9 Maximum CPU time of a job: 4.3 Minimum CPU time of a job: 0.0 Total wait time in queues: 276962.0 Average wait time in queue:55392.4 Maximum wait time in queue:276953.0 Minimum wait time in queue: 2.0 Average turnaround time: 60739 (seconds/job) Maximum turnaround time: 276953 Minimum turnaround time: 12 Average hog factor of a job: 0.00 ( cpu time / turnaround time ) Maximum hog factor of a job: 0.00 Minimum hog factor of a job: 0.00 Average expansion factor of a job: 55610.95 ( turnaround time / run time ) Maximum expansion factor of a job: 276953.00 Minimum expansion factor of a job: 1.00 Total Run time consumed: 6981 Average Run time consumed: 1396 Maximum Run time of a job: 6914 Minimum Run time of a job: 0 Total throughput: 409.09 (jobs/hour) during 0.01 hours Beginning time: May 9 15:39 Ending time: May 9 15:40 基本概念 LSF 是资源管理的工具，它管理的主要对象有三个方面：机器（节点），作业和用户。 一般情况下，集群中的机器都是对等的，称为服务节点（Sever Host）。它们既可以提交作业，也可以执行作业。按照职能的不同，服务节点可以有不同的身份。管理作业调度资源的节点称为主节点（Master host），一个集群只有一个主节点。对于一次作业提交来说，提交作业的节点为提交节点（Submission host），被分配并执行作业的节点是执行节点（Execution host）。 图 2. 节点角色 为了便于机器的管理，LSF 提供节点组（Host group）的概念。任意的机器集合可以被作为整体命名，通过名字在集群范围内整体引用。一种典型的用法是将节点按照内存大小进行分类：大内存节点组和小内存节点组。不同的作业可以请求不同的节点组，LSF 按照请求，分配该节点组中的机器来执行作业。节点组还有一个灵活的特性：为方便管理，同一台机器可以定义到不同的节点组。 图 3. 节点分组 作业占用机器资源进行计算。每一个机器被划分成若干个槽位（slot） 。每一个槽位通常可以容纳一个作业运行。 通常情况下，每一个槽位和一个CPU 核心（core）相对应。槽位是一个容易混淆的概念，为便于理解，可以将槽位等同于CPU 核心来看待。对于并行作业来说，比如MPI 作业，每一个都需要占用更多的槽位加速计算。 作业的管理是以队列（Queue）为单位的，调度的策略也是按照队列定制的。作为作业的容器，可以配置多个队列定制不同策略。LSF 按照队列的优先级，从高到低调度每一个队列中的作业，为其分配资源。高优先级队列的作业对资源的使用具有优先权。 图 4. 调度方式 　　 用户是作业提交者，也是资源的真正使用者。对于资源和作业的管理，不少策略都是以用户为单位。为了更好管理用户，LSF 引入用户组（user group）。若干用户集合可以统一命名，并使用该命名统一引用。例如，一个人属于一个部门，一个部门可以作为一个用户组，这种情况下就可以使用部门名字命名和引用。 体系结构 LSF 采用传统的客户机服务器模式，主节点负责整个集群的管理，从节点负责管理运行其上的作业的管理。每一个服务节点包含三个后台进程： Sbatchd 批处理作业管理进程。负责在本节点上执行作业，监控作业状态以及收集其使用的资源。同时根据用户请求或者系统策略触发，控制作业的状态，比如发送SIGSTOP 给作业、挂起作业运行等。 RES 远程执行服务进程。负责执行远程客户请求的任务，主要用于并行作业的远程任务启动、监控以及控制。 LIM 采集本节点的负载信息进程。将收集到的资源负载信息周期上报给主节点上的LIM。主节点的LIM 拥有整个集群资源状态，为其它服务提供集群系统范围内的资源当前快照。 主管理节点在这个基础上，包括两个额外的进程： Mbatchd 集群管理服务进程。它是 LSF 的中心：通过主节点的 LIM 获取机器以及相关资源信息，处理远程用户的作业提交请求，委托 mbschd 将作业调度到相关资源上，发送调度结果到指定机器，并通过和 sbatchd 的交互，监控作业使用的资源，控制作业的执行过程。 Mbschd 调度策略服务进程。从 mbachd 获取作业和资源信息，根据定制策略，为 mbatchd 提供作业调度服务。 图 5. 体系结构 当一个新作业提交的时候，集群管理服务进程检查作业的合法性，并将作业放入到指定的队列中等待调度。经过调度的作业，将获得执行机器上的slot、内存等资源。LSF 负责保留相关资源并将作业指派到已分配机器上执行。作业的状态和实际使用的资源通过批处理作业管理进程报告给集群管理进程。 集群系统构建在分布式网络节点上，节点的失效和网路设备的故障都会导致集群系统的基础环境改变。集群系统的高可用性功能可以保证即使在底层设备故障，LSF 系统仍然可以提供可靠稳定的服务。LSF 采用主备模式实现系统的故障恢复。LSF 的主节点将运行时事件信息写入网络文件系统。当主节点失效后，相关备节点进行选举，选出新的主节点。新的主节点从事件信息文件中恢复作业信息、机器状态，最终获取集群控制权，恢复LSF 状态。 为了更进一步扩展 LSF 的资源管理范围和方式，在单一集群基础上，LSF 还提供多集群互联技术（Multi-Cluster）。多集群通过互联，共享跨集群资源，提供更强大的计算能力。下面是一种典型的多集群架构，集群 1 负责接收作业提交和转发，集群 2 和 集群 3 负责作业执行。 图 6. 多集群结构 资源调度 LSF 收集每一个节点的处理器、内存、交换区、临时存储区等资源信息。主节点掌握全局资源信息。资源的管理和调度以这些信息为基础。 清单 4. 查看节点资源负载信息 lsfrhel01 # lshosts HOST_NAME type model cpuf ncpus maxmem maxswp server RESOURCES lsfrhel01 X86_64 PC6000 116.1 2 1.4G 1.4G Yes (mg) lsfrhel02 X86_64 PC6000 116.1 2 1.4G 1.4G Yes () lsfrhel01 # lsload HOST_NAME status r15s r1m r15m ut pg ls it tmp swp mem lsfrhel01 ok 0.4 0.0 0.0 0% 0.0 1 179 10G 1.4G 1.1G lsfrhel02 ok 1.5 0.3 0.3 1% 0.1 2 1 2391M 1.2G 603M 作业在使用资源的时候，主要考虑三个方面：如何根据作业的资源描述选择合适的执行机器，如何预留机器资源减少运行时的资源竞争，以及如何限制作业对资源的使用避免作业过度消耗资源。 首先是选择执行机。每一个作业会有不同的资源需求。可以通过 LSF 定义的资源描述模式请求资源，运行节点由LSF 根据作业对资源描述来匹配。当一个作业需要一定量内存的时候，\"select[mem>512]\" 表示选择内存大于512M 的机器来运行作业。调度器为其选择拥有合适资源的机器。 其次是确保执行机上的资源分配。既然我们选择了大内存的机器，那么别人的作业也可以选择大内存的机器。如果很多作业都在大内存机器上运行，资源竞争会导致内存短缺，作业最终无法占用到请求的资源。为了更好的确保资源，我们需要在作业运行时保留这些资源不会再分配给别人的作业。\"rusage[mem=512]\" 表示保留 512M 内存给作业，节点当前可用内存将会有 512M 分配给该作业，节点可分配资源将减少512M 的内存。通过 LSF 的策略，可用内存的分配得到控制，已经保留的内存将不会再分配给其它的作业。 最后是保证资源不被过度消耗。资源的保留是通过 LSF 的策略保证，但是作业的进程在运行时，却尚未受 LSF 控制。我们需要在执行节点上，对作业的系统资源设置限制，保证作业（进程）本身不会吃掉过多的内存。通常我们可以设置一个资源上限，防止作业过度消耗资源。比如，内存限制 '1024M'，防止作业使用超过1G 的内存。 清单 5. 提交内存需求的作业 $ bsub -M 1024 -R \"select[mem>512] rusage[mem=512]\" sleep 1000 Job is submitted to default queue . $ bjobs -l 644 Job , User , Project , Status , Queue , Command Wed Dec 28 16:04:00: Submitted from host , CWD , Requested Resources 512] rusage[mem=512]>; MEMLIMIT 1 G Wed Dec 28 16:04:00: Started 1 Task(s) on Host(s) , Allocated 1 Slot (s) on Host(s) , Execution Home , Exe cution CWD ; Wed Dec 28 16:05:00: Resource usage collected. The CPU time used is 4 seconds. MEM: 6 Mbytes; SWAP: 0 Mbytes; NTHREAD: 4 PGID: 23106; PIDs: 23106 23108 23110 SCHEDULING PARAMETERS: r15s r1m r15m ut pg io ls it tmp swp mem loadSched - - - - - - - - - - - loadStop - - - - - - - - - - - RESOURCE REQUIREMENT DETAILS: Combined: select[(mem>512) && (type == local)] order[r15s:pg] rusage[mem=512.0 0] Effective: select[(mem>512) && (type == local)] order[r15s:pg] rusage[mem=512. 00] 对于资源的选择、预留和限制，内存只是一个例子。 LSF 还支持很多种资源，比如 CPU、交换区、临时目录大小等等。虽然这些默认管理的资源始终是有限的，但是 LSF 还提供了资源管理的扩展机制。用户可以统过编写自己的 ELIM 来收集自定义的资源。比如，收集系统的网络带宽以及网络负载，作业可以通过资源描述，选择拥有相应带宽的节点执行作业。资源的种类很多，LSF 提供布尔类型、字符串类型和数字类型来描述多种多样的资源。 定制策略 LSF 提供了非常丰富的调度策略供 LSF 系统管理员选择配置。策略针对作业和资源，从提高资源利用率、优化资源分配、提高用户满意度等各个方面为资源的使用者和管理者提供可配置策略。 默认的策略是先来先服务（FCFS）。顾名思义，先提交的作业先调度，优先获得资源。排在后面的作业必须等待前面的作业运行完成才能开始。后提交作业可能在很长时间得不到机会运行。考虑到资源分配的公平问题，比如部门之间应该平等使用资源，公平共享（Fairshare）策略得以引入。通过配置以部门为单位的用户组平等分享资源，不同用户的作业优先考虑资源所有权。避免了先来先服务导致的资源不平等，从而优化资源分配。 公平共享是从资源分配公平性上考虑问题的，它达到的是一个最终资源使用公平的平衡。但是在使用过程中，如果其他人或者部门没有作业，那么整个集群的资源将会被分配给目前已经提交的作业。作业执行期间，即便新的作业拥有更多的资源优先使用权，也要等待其他作业执行完毕，释放资源后才能执行。这样在短时期内资源使用是不公平的。对于拥有资源使用权却无法执行的情况，资源保障（Resource Guarantee）策略可以为用户保留资源，随时可用。这是在牺牲资源利用率基础上提高用户满意度。 对于并行作业，通常要求很多的 slot，当系统资源紧张的时候，即使等待很久依然无法运行。当系统释放一个 slot 的时候，这个作业因为发现资源不够，便放弃对资源的优先权。排在后面的只需要一个 slot 的作业将获取资源执行。这个过程反复着，并行作业始终无法获取足够资源。为了解决这个问题引入了资源保留策略。虽然当前资源不够运行的，但是这个资源暂时被并行作业占有，等待后面不断释放的资源，直到 slot 满足作业的需求。 但是长时间的等待还是会浪费这些空闲资源，因此又加入回填策略。在不影响并行作业启动时间的前提下，可以将短作业优先调度到已经被保留的 slot 上。 对于特权或者紧急的用户或者应用，当系统资源完全使用的时候。为了尽快执行，抢占（Preemption）策略可以通过抢占已执行作业的资源，执行自己的作业。 除此之外，为了满足用户需求，LSF 提供了资源预留（Advanced Reservation）、SLA 等等。为了提高资源利用率，LSF 提供了NUMA绑定（Affinity）策略等等。对于详尽的策略，可以参考 LSF 管理员手册。LSF 的很多策略都可以自由组合，通过管理员的配置，最终形成丰富的，满足各种需求的定制策略。 即便所有的策略都不能满足你的要求，LSF 的调度策略还实现了插件机制。新的扩展可以通过 LSF 提供的API 实现新的策略。 总结 在高性能计算领域，作业管理系统是一种成熟的系统软件技术。LSF 作为其中的佼佼者，提供了统一的访问接口，丰富的调度策略，灵活的配置和部署。LSF 的体系结构和部署方式让它可以有效管理一定规模的集群系统，目前商业版可以支持多达数千台节点和数百万作业的管理。LSF 的故障恢复机制足以保证集群系统的高可用性。在分布式计算技术不断发展的今天，作为传统的批处理管理软件，依然发挥着重要的作用。 参考资源 下载 IBM Spectrum LSF Community Edition，安装，试用LSF 的基本功能 参考 IBM Spectrum LSF Knowledge Center,了解更多LSF 的使用和集群管理方法 参考 IBM Spectrum LSF 首页,察看更多LSF 产品和技术的最新信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section2/system_requirements_and_compatibility.html":{"url":"chapter1/section2/system_requirements_and_compatibility.html","title":"1.2 LSF 系统要求与兼容性","keywords":"","body":"1.2 LSF 系统要求与兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section2/operating_system_support.html":{"url":"chapter1/section2/operating_system_support.html","title":"操作系统支持","keywords":"","body":"操作系统支持 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section2/master_host_selection.html":{"url":"chapter1/section2/master_host_selection.html","title":"主机选择","keywords":"","body":"主机选择 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section2/server_host_compatibility.html":{"url":"chapter1/section2/server_host_compatibility.html","title":"服务器主机兼容性","keywords":"","body":"服务器主机兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section2/add-on_compatibility.html":{"url":"chapter1/section2/add-on_compatibility.html","title":"附加兼容性","keywords":"","body":"附加兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section2/API_compatibility.html":{"url":"chapter1/section2/API_compatibility.html","title":"API 兼容性","keywords":"","body":"API 兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section3/limitations.html":{"url":"chapter1/section3/limitations.html","title":"1.3 局限性","keywords":"","body":"1.3 局限性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section4/release_notes.html":{"url":"chapter1/section4/release_notes.html","title":"1.4 版本更新说明","keywords":"","body":"1.6 版本更新说明 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter1/section5/LSF_quick_reference.html":{"url":"chapter1/section5/LSF_quick_reference.html","title":"1.5 LSF 快速上手","keywords":"","body":"1.7 LSF 快速上手 本文主要是关于 LSF 命令、守护进程、配置文件、日志文件以及重要的集群配置参数的快速介绍。 Unix 及 Linux 系统下的安装目录示意图 守护进程的错误日志文件 守护进程的错误日志文件，存储在由 LSF_LOGDIR 变量定义的文件目录下面，该变量值在 lsf.conf 文件中指定。 LSF base system daemon log files LSF batch system daemon log files pim.log.host_name mbatchd.log.host_name res.log.host_name sbatchd.log.host_name lim.log.host_name mbschd.log.host_name 如果在 ego.conf 文件中定义了变量 EGO_LOGDIR ，那么 lim.log.host_name 文件则存储在由变量 EGO_LOGDIR 指定的文件目录中。 配置文件 lsf.conf, lsf.shared, 和 lsf.cluster.cluster_name 文件， 位于由 LSF_CONFDIR 变量定义的文件目录下面，该变量值在 lsf.conf 文件中指定。 lsb.params, lsb.queues, lsb.modules, 和 lsb.resources 文件，位于 LSB_CONFDIR/cluster_name/configdir/ 目录下面。 文件 描述 install.config LSF 的安装与配置选项 lsf.conf 描述集群配置和操作的通用环境配置文件 lsf.shared 所有群集共享的定义文件。用于定义群集名称、主机类型、主机型号和站点定义的资源 lsf.cluster.cluster_name 用于定义主机、管理员和站点定义的共享资源的位置的集群配置文件 lsb.applications 定义应用程序配置文件，来指定同类型作业的公共参数 lsb.params 配置 LSF 批处理参数 lsb.queues 批处理队列配置文件 lsb.resources 配置资源分配限制、导出和资源使用限制 lsb.serviceclasses 将LSF集群中的服务级别协议（SLA）定义为服务类，该服务类定义了 SLA 的属性 lsb.users 配置用户组、用户和用户组的分层公平共享，以及用户和用户组的作业槽数限制 lsf.conf 配置文件中的集群配置参数 参数 描述 Unix 默认值 LSF_BINDIR 包含 LSF 用户命令的目录，由相同类型的所有主机共享 LSF_TOP/version/OStype/bin LSF_CONFDIR 所有 LSF 配置文件的目录 LSF_TOP/conf LSF_ENVDIR 包含 lsf.conf 文件的目录。必须由root 用户所拥有。 /etc (if LSF_CONFDIR is not defined) LSF_INCLUDEDIR 包含 LSF API 头文件 lsf.h 和 lsbatch.h的目录 LSF_TOP/version/include LSF_LIBDIR LSF 库，由相同类型的所有主机共享 LSF_TOP/version/OStype/lib LSF_LOGDIR （可选）LSF 守护程序日志的目录。必须由 root 拥有。 /tmp LSF_LOG_MASK 从 LSF 命令记录错误消息的级别 LOG_WARNING LSF_MANDIR 包含 LSF 手册页的目录 LSF_TOP/version/man LSF_MISC 示例C程序和Shell脚本，以及外部LIM（elim）的模板 LSF_TOP/version/misc LSF_SERVERDIR 由 LSF 守护程序启动的所有服务器二进制文件和 Shell 脚本以及外部可执行文件的目录。必须由root拥有，并由相同类型的所有主机共享 LSF_TOP/version/OStype/etc LSF_TOP 顶级安装目录。 LSF_TOP 的路径必须共享，并且集群中的所有主机都可以访问。它不能是根目录（/）。 Not definedRequired for installation LSB_CONFDIR LSF批处理配置目录的目录，包含用户和主机列表，操作参数和批处理队列 LSF_CONFDIR/lsbatch LSF_LIVE_CONFDIR LSF 实时重新配置目录的目录，该目录由 bconf 命令编写。 LSB_SHAREDIR/cluster_name/live_confdir LSF_SHAREDIR 每个集群的 LSF 批处理作业历史记录和记帐日志文件的目录，必须由首要的 LSF 管理员拥有 LSF_TOP/work LSF_LIM_PORT 用于与 lim 守护程序进行通信的 TCP 服务端口 7879 LSF_RES_PORT 用于与 res 守护程序通信的 TCP 服务端口 6878 LSF_MBD_PORT 用于与 mbatchd 守护程序进行通信的 TCP 服务端口 6881 LSF_SBD_PORT 用于与 sbatchd 守护程序进行通信的TCP服务端口 6882 管理命令 注：只有 LSF 管理员和 root 用户可以使用这些命令。 命令 描述 lsadmin LSF 管理员工具，用于控制 LSF 集群中 LIM 和 RES 守护程序的运行，lsadmin help 显示所有子命令 lsfinstall 使用 install.config 输入文件安装 LSF lsfrestart 在本地集群中的所有主机上重新启动 LSF 守护程序 lsfshutdown 关闭本地集群中所有主机上的 LSF 守护程序 lsfstartup 在本地集群中的所有主机上启动 LSF 守护程序 badmin 用于控制 LSF 批处理系统（sbatchd，mbatchd，主机和队列）的 LSF 管理工具 badmin help 显示所有子命令 bconf 更改活动内存中的 LSF 配置 守护进程 进程名 描述 lim Load Information Manager (LIM): 负载信息管理器：收集有关集群中所有服务器主机的负载和资源信息，并通过 LSLIB 为应用程序提供主机选择服务。 LIM 维护有关静态系统资源和动态负载索引的信息 mbatchd Master Batch Daemon (MBD): 主批处理守护程序：接受并保存所有批处理作业。 MBD通过联系 主LIM 定期检查所有服务器主机上的负载索引。 mbschd Master Batch Scheduler Daemon：主批处理调度守护程序：执行LSF的调度功能，并将作业调度决策发送到 MBD 以进行调度。 该服务在 LSF 主服务器主机上运行 sbatchd Slave Batch Daemon (SBD)：从属批处理守护程序（SBD）：接受来自 MBD 的作业执行请求，并监视作业进度。 控制作业执行，强制执行批处理策略，将作业状态报告给 MBD，然后启动 MBD。 pim Process Information Manager (PIM): 进程信息管理器（PIM）：监视提交的作业在运行时所使用的资源。 PIM 用于强制执行资源限制和负载阈值以及公平分配调度。 res Remote Execution Server (RES): 远程执行服务器（RES）：接受来自所有负载共享应用程序的远程执行请求，并处理远程主机上的I / O以进行负载共享过程。 用户命令 查看集群信息的命令 命令 描述 bhosts 显示主机及其静态和动态资源 blimits 显示正在运行的作业的资源分配限制的相关信息 bparams 显示可调批次系统参数的相关信息 bqueues 显示批处理队列的相关信息 busers 显示用户和用户组的相关信息 lshosts 显示主机及其静态资源信息 lsid 显示当前的 LSF 版本号，集群名称和主控主机名 lsinfo 显示负载分担配置信息 lsload 显示主机的动态负载索引 监测作业与任务的命令 命令 描述 bacct 报告已完成的 LSF 作业的会计统计数据 bapp 显示附加到应用程序配置文件的作业的相关信息 bhist 显示作业的相关历史信息 bjobs 显示作业的相关信息 bpeek 显示未完成作业的标准输出和标准错误 bsla 显示服务类配置的相关信息，以用于面向目标的服务级别协议调度 bstatus 读取或设置外部作业状态消息和数据文件 提交与控制作业的命令 命令 描述 bbot 移动正在等待的作业到队列的末尾 bchkpnt 检查点可检查的工作 bkill 向作业发送信号，一般用于结束作业 bmig 迁移可检查点的或可重新运行的作业 bmod 修改作业的提交选项 brequeue 杀死并重新安排作业 bresize 释放槽位并取消挂起的作业调整大小分配请求 brestart 重新启动检查点作业 bresume 恢复暂停的作业 bstop 暂停作业 bsub 提交作业 bswitch 将未完成的作业从一个队列移至另一队列 btop 移动正在等待的作业到队列首部 bsub 命令 bsub [options] command [arguments] 命令中的部分选项 选项 描述 -ar 指定作业可自动调整大小 -H 提交时将工作保持在 PSUSP 状态 **-I\\ -Ip\\ -Is** 提交批处理交互式作业。 -Ip 创建伪终端。 -is 在shell模式下创建一个伪终端。 -K 提交作业并等待作业完成 -r 使作业可重新运行 -x 排他执行 -app application_profile_name 将作业提交到指定的应用程序配置文件 -b begin_time 在[[month：] day：]：minute 格式的指定日期和时间或之后调度作业 -C core_limit 为属于此作业的所有进程设置每个进程（soft）核心文件的大小限制（KB） -c cpu_time[/host_name \\ /host_model] 限制作业可以使用的总CPU时间。 CPU时间的格式为[hour:]minutes -cwd \"current_working_directory\" 指定作业的当前工作目录 -D data_limit 为属于作业的每个进程设置每个进程（soft）数据段的大小限制（KB） -E \"pre_exec_command [arguments]\" 在作业运行之前，在执行主机上运行指定的pre-exec命令 -Ep \"post_exec_command [arguments]\" 作业完成后，在执行主机上运行指定的post-exec命令 -e error_file 将标准错误输出附加到文件 -eo error_file 将作业的标准错误输出覆盖到指定文件 -F file_limit 为属于作业的每个进程设置每个进程（soft）文件大小限制（KB） -f \"local_file op[remote_file]\" ... 在本地（提交）主机和远程（执行）主机之间复制文件。op是>， 之一 -i input_file \\ -is input_file 从指定文件获取作业的标准输入 -J \"job_name[index_list]%job_slot_limit\" 为作业分配指定的名称。 作业数组 index_list 的格式 start [-end [：step]]，而 ％job_slot_limit 是可以同时运行的最大作业数。 -k \"chkpnt_dir [chkpnt_period][method=method_name]\" 使作业可检查，并指定检查点目录，以分钟为单位的时间段和方法 -M mem_limit 设置每个进程（soft）的内存限制（KB） -m \"host_name [@cluster_name][[!] \\ +[pref_level]] \\ host_group[[!] \\ +[pref_level]] \\ compute_unit[[!] \\ +[pref_level]]...\" 在指定的主机之一上运行作业。 主机或组的名称后的加号（+）表示首选项。 可选地有，正整数表示偏好级别。 数字越高表示偏好越大。 -n min_proc[,max_proc] 指定并行作业所需的最小和最大处理器数量 -o output_file 将标准输出附加到文件 -oo output_file 将作业的标准输出覆盖到指定文件 -p process_limit 限制整个作业的进程数量 -q \"queue_name ...\" 将作业提交到指定的队列之一 -R \"res_req\" [-R \"res_req\" ...] 指定主机资源要求 -S stack_limit 为属于作业的每个进程设置每个进程（soft）堆栈段大小限制（KB） -sla service_class_name 指定要在其中运行作业的服务类 -T thread_limit 设置整个作业的并发线程数限制 -t term_time 以 [[month：] day：] hour：minute 的形式指定作业终止的截止日期 -v swap_limit 设置整个作业的总进程虚拟内存限制（KB） -W run_time[/host_name \\ /host_model] 以[hour：] minute形式设置作业的运行时限制 -h 将命令用法打印到 stderr 并退出 -V 将 LSF 发行版本打印到 stderr 并退出 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter2/install_upgrade_and_migrate.html":{"url":"chapter2/install_upgrade_and_migrate.html","title":"Chapter 2 安装、升级与迁移","keywords":"","body":"Chapter 2 安装、升级与迁移 规划您的安装，并安装新的 IBM Spectrum LSF 集群产品，或在 UNIX，Linux 或 Microsoft Windows 主机上升级LSF。 为了使您的集群保证最新状态，建议使用 IBM Fix Central 中的最新修订包。 在 IBM Knowledge Center上的 LSF 安装指南 中可以找到从 Fix Central 应用 Fices 的详细步骤。 在 UNIX 和 Linux 上安装 IBM Spectrum LSF 规划安装，并在 UNIX 或 Linux 主机上安装新的生产版 IBM Spectrum LSF 集群。 在 Windows 上安装 IBM Spectrum LSF 规划安装，并在 Microsoft Windows 主机上安装新的生产版 IBM Spectrum LSF 集群。 使用 IBM Spectrum Cluster Foundation 从裸机安装和部署 IBM Spectrum LSF 使用 IBM Spectrum Cluster Foundation 在受支持的Linux主机上，裸机安装和部署新的生产版 IBM Spectrum LSF 集群。IBM Spectrum Cluster Foundation 是面向技术计算用户的，功能强大的集群管理框架。它提供了一套全面的功能，可帮助从基础架构级别管理硬件和软件。 它使操作系统和软件组件的部署以及复杂的活动（如应用程序集群的创建和系统维护）变得自动化。 升级和迁移 IBM Spectrum LSF 升级 UNIX 或 Linux 集群，或将现有集群迁移到 UNIX，Linux 或 Windows 上的 LSF 10.1版本。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Install on UNIX and Linux.html":{"url":"chapter2/section1/Install on UNIX and Linux.html","title":"2.1 在 UNIX 与 Linux 上安装","keywords":"","body":"2.1 在 UNIX 与 Linux 上安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Example installation directory structure.html":{"url":"chapter2/section1/Example installation directory structure.html","title":"安装目录结构示意","keywords":"","body":"安装目录结构示意 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Planning your installation.html":{"url":"chapter2/section1/Planning your installation.html","title":"规划安装","keywords":"","body":"规划安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/EGO in the LSF cluster.html":{"url":"chapter2/section1/EGO in the LSF cluster.html","title":"LSF 集群中的 EGO","keywords":"","body":"LSF 集群中的 EGO © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Master host selection.html":{"url":"chapter2/section1/Master host selection.html","title":"主节点选择","keywords":"","body":"主节点选择 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Preparing your systems for installation.html":{"url":"chapter2/section1/Preparing your systems for installation.html","title":"准备系统进行安装","keywords":"","body":"准备系统进行安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Installing a new LSF cluster.html":{"url":"chapter2/section1/Installing a new LSF cluster.html","title":"安装新的 LSF 集群 (lsfinstall)","keywords":"","body":"安装新的 LSF 集群 (lsfinstall) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Getting fixes from IBM Fix Central.html":{"url":"chapter2/section1/Getting fixes from IBM Fix Central.html","title":"从 IBM Fix Central 中获取修订","keywords":"","body":"从 IBM Fix Central 中获取修订 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Configuring a cluster.html":{"url":"chapter2/section1/Configuring a cluster.html","title":"配置集群","keywords":"","body":"配置集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/If you install LSF as a non-root user.html":{"url":"chapter2/section1/If you install LSF as a non-root user.html","title":"以非 root 用户安装 LSF","keywords":"","body":"非 root 用户安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Adding hosts to the cluster.html":{"url":"chapter2/section1/Adding hosts to the cluster.html","title":"往集群中添加节点","keywords":"","body":"往集群中添加节点 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/LSF HPC features.html":{"url":"chapter2/section1/LSF HPC features.html","title":"LSF HPC 特征","keywords":"","body":"LSF HPC 特性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Optional LSF HPC features configuration.html":{"url":"chapter2/section1/Optional LSF HPC features configuration.html","title":"可选的LSF HPC 功能配置","keywords":"","body":"可选的LSF HPC 功能配置 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/Registering service ports.html":{"url":"chapter2/section1/Registering service ports.html","title":"注册服务端口","keywords":"","body":"注册服务端口 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/install.config.html":{"url":"chapter2/section1/install.config.html","title":"install.config 文件","keywords":"","body":"install.config 文件 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section1/slave.config.html":{"url":"chapter2/section1/slave.config.html","title":"slave.config 文件","keywords":"","body":"slave.config 文件 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Install on Windows.html":{"url":"chapter2/section2/Install on Windows.html","title":"2.2 在 Windows 上安装","keywords":"","body":"2.2 在 Windows 上安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Example installation directory structures.html":{"url":"chapter2/section2/Example installation directory structures.html","title":"安装目录结构示意","keywords":"","body":"安装目录结构示意 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/EGO in the LSF cluster.html":{"url":"chapter2/section2/EGO in the LSF cluster.html","title":"LSF 集群中的 EGO","keywords":"","body":"LSF 集群中的 EGO © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Planning and preparing your systems for installation.html":{"url":"chapter2/section2/Planning and preparing your systems for installation.html","title":"准备系统进行安装","keywords":"","body":"准备系统进行安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Master host selection.html":{"url":"chapter2/section2/Master host selection.html","title":"主节点选择","keywords":"","body":"主节点选择 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Entitlement files.html":{"url":"chapter2/section2/Entitlement files.html","title":"Entitlement files 文件","keywords":"","body":"Entitlement files 文件 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Installing a new LSF cluster.html":{"url":"chapter2/section2/Installing a new LSF cluster.html","title":"安装新的 LSF 集群","keywords":"","body":"安装新的 LSF 集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section2/Installation parameter quick reference.html":{"url":"chapter2/section2/Installation parameter quick reference.html","title":"安装参数快速参考","keywords":"","body":"安装参数快速参考 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/Install LSF with IBM Spectrum Cluster Foundation.html":{"url":"chapter2/section3/Install LSF with IBM Spectrum Cluster Foundation.html","title":"2.3 使用 IBM Spectrum Cluster Foundation 安装 LSF","keywords":"","body":"2.3 使用 IBM Spectrum Cluster Foundation 安装 LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/Install LSF Suites 10.1.1 with IBM Spectrum Cluster Foundation.html":{"url":"chapter2/section3/Install LSF Suites 10.1.1 with IBM Spectrum Cluster Foundation.html","title":"使用 IBM Spectrum Cluster Foundation安装 LSF Suites 10.1.1","keywords":"","body":"使用 IBM Spectrum Cluster Foundation安装 LSF Suites 10.1.1 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Installation.html":{"url":"chapter2/section3/subsection1/Installation.html","title":"安装","keywords":"","body":"安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Installation planning.html":{"url":"chapter2/section3/subsection1/Installation planning.html","title":"安装规划","keywords":"","body":"安装规划 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Preinstallation roadmap.html":{"url":"chapter2/section3/subsection1/Preinstallation roadmap.html","title":"预安装路线图","keywords":"","body":"预安装路线图 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Installation roadmap.html":{"url":"chapter2/section3/subsection1/Installation roadmap.html","title":"安装路线图","keywords":"","body":"安装路线图 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Preparing to install IBM Spectrum LSF Suite for Workgroups, or IBM Spectrum LSF Suite for HPC.html":{"url":"chapter2/section3/subsection1/Preparing to install IBM Spectrum LSF Suite for Workgroups, or IBM Spectrum LSF Suite for HPC.html","title":"准备安装适用于工作组的IBM Spectrum LSF Suite或适用于 HPC 的 IBM Spectrum LSF Suite","keywords":"","body":"准备安装适用于工作组的IBM Spectrum LSF Suite或适用于HPC的IBM Spectrum LSF Suite © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Requirements.html":{"url":"chapter2/section3/subsection1/Requirements.html","title":"要求","keywords":"","body":"要求 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Configure and test switches.html":{"url":"chapter2/section3/subsection1/Configure and test switches.html","title":"配置和测试开关","keywords":"","body":"配置和测试开关 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Plan your network configuration.html":{"url":"chapter2/section3/subsection1/Plan your network configuration.html","title":"规划您的网络配置","keywords":"","body":"规划您的网络配置 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Installing and verifying the operating system on the management node.html":{"url":"chapter2/section3/subsection1/Installing and verifying the operating system on the management node.html","title":"在管理节点上安装和验证操作系统","keywords":"","body":"在管理节点上安装和验证操作系统 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Performing an installation.html":{"url":"chapter2/section3/subsection1/Performing an installation.html","title":"执行安装","keywords":"","body":"执行安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Comparing installation methods.html":{"url":"chapter2/section3/subsection1/Comparing installation methods.html","title":"安装方法的对比","keywords":"","body":"安装方法的对比 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Interactive installation roadmaps.html":{"url":"chapter2/section3/subsection1/Interactive installation roadmaps.html","title":"交互式安装路线图","keywords":"","body":"交互式安装路线图 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Quick installation roadmap.html":{"url":"chapter2/section3/subsection1/Quick installation roadmap.html","title":"快速安装路线图","keywords":"","body":"快速安装路线图 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Custom installation.html":{"url":"chapter2/section3/subsection1/Custom installation.html","title":"自定义安装","keywords":"","body":"自定义安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Performing an interactive installation using the installer.html":{"url":"chapter2/section3/subsection1/Performing an interactive installation using the installer.html","title":"使用安装程序执行交互式安装","keywords":"","body":"使用安装程序执行交互式安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Performing a silent installation.html":{"url":"chapter2/section3/subsection1/Performing a silent installation.html","title":"执行静默安装","keywords":"","body":"执行静默安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Verifying the installation.html":{"url":"chapter2/section3/subsection1/Verifying the installation.html","title":"验证安装","keywords":"","body":"验证安装 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Taking the first steps after installation.html":{"url":"chapter2/section3/subsection1/Taking the first steps after installation.html","title":"安装后的第一步","keywords":"","body":"安装后的第一步 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Adding compute nodes after installation.html":{"url":"chapter2/section3/subsection1/Adding compute nodes after installation.html","title":"安装后添加计算节点","keywords":"","body":"安装后添加计算节点 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Import compute nodes.html":{"url":"chapter2/section3/subsection1/Import compute nodes.html","title":"导入计算节点","keywords":"","body":"导入计算节点 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Discover compute nodes.html":{"url":"chapter2/section3/subsection1/Discover compute nodes.html","title":"发现计算节点","keywords":"","body":"发现计算节点 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection1/Troubleshooting installation problems.html":{"url":"chapter2/section3/subsection1/Troubleshooting installation problems.html","title":"解决安装问题","keywords":"","body":"解决安装问题 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection2/Cluster deployment.html":{"url":"chapter2/section3/subsection2/Cluster deployment.html","title":"集群部署","keywords":"","body":"集群部署 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection2/Creating an LSF Workgroups cluster after installation.html":{"url":"chapter2/section3/subsection2/Creating an LSF Workgroups cluster after installation.html","title":"安装后创建 LSF 工作组集群","keywords":"","body":"安装后创建 LSF 工作组集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection2/Creating an LSF HPC cluster after installation.html":{"url":"chapter2/section3/subsection2/Creating an LSF HPC cluster after installation.html","title":"安装后创建 LSF HPC 集群","keywords":"","body":"安装后创建 LSF HPC 集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/Setting up a high availability environment.html":{"url":"chapter2/section3/subsection3/Setting up a high availability environment.html","title":"设置高可用性环境","keywords":"","body":"设置高可用性环境 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/High availability requirements.html":{"url":"chapter2/section3/subsection3/High availability requirements.html","title":"高可用性要求","keywords":"","body":"高可用性要求 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/Prepare a shared file system.html":{"url":"chapter2/section3/subsection3/Prepare a shared file system.html","title":"准备共享文件系统","keywords":"","body":"准备共享文件系统 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/Preparing high availability.html":{"url":"chapter2/section3/subsection3/Preparing high availability.html","title":"准备高可用性","keywords":"","body":"准备高可用性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/Enable a high availability environment.html":{"url":"chapter2/section3/subsection3/Enable a high availability environment.html","title":"启用高可用性环境","keywords":"","body":"启用高可用性环境 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/Verifying a high availability environment.html":{"url":"chapter2/section3/subsection3/Verifying a high availability environment.html","title":"验证高可用性环境","keywords":"","body":"验证高可用性环境 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection3/Troubleshooting a high availability environment enablement.html":{"url":"chapter2/section3/subsection3/Troubleshooting a high availability environment enablement.html","title":"对高可用性环境启用进行故障排除","keywords":"","body":"对高可用性环境启用进行故障排除 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/After installation.html":{"url":"chapter2/section3/subsection4/After installation.html","title":"安装之后","keywords":"","body":"安装之后 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Managing a cluster.html":{"url":"chapter2/section3/subsection4/Managing a cluster.html","title":"管理集群","keywords":"","body":"管理集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Add or remove servers.html":{"url":"chapter2/section3/subsection4/Add or remove servers.html","title":"添加或删除服务器","keywords":"","body":"添加或删除服务器 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Synchronizing a cluster.html":{"url":"chapter2/section3/subsection4/Synchronizing a cluster.html","title":"同步集群","keywords":"","body":"同步集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Deleting a cluster.html":{"url":"chapter2/section3/subsection4/Deleting a cluster.html","title":"删除集群","keywords":"","body":"删除集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Maintenance mode.html":{"url":"chapter2/section3/subsection4/Maintenance mode.html","title":"维护模式","keywords":"","body":"维护模式 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Health check.html":{"url":"chapter2/section3/subsection4/Health check.html","title":"健康检查","keywords":"","body":"健康检查 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Upgrading a cluster.html":{"url":"chapter2/section3/subsection4/Upgrading a cluster.html","title":"升级集群","keywords":"","body":"升级集群 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Performing a cluster upgrade.html":{"url":"chapter2/section3/subsection4/Performing a cluster upgrade.html","title":"执行集群升级","keywords":"","body":"执行集群升级 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection4/Rolling or bulk upgrade process overview.html":{"url":"chapter2/section3/subsection4/Rolling or bulk upgrade process overview.html","title":"滚动或批量升级过程概述","keywords":"","body":"滚动或批量升级过程概述 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section3/subsection5/Known issues and limitations.html":{"url":"chapter2/section3/subsection5/Known issues and limitations.html","title":"已知问题和局限性","keywords":"","body":"已知问题和局限性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section4/Upgrade and migrate.html":{"url":"chapter2/section4/Upgrade and migrate.html","title":"2.4 升级和迁移","keywords":"","body":"2.4 升级和迁移 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section4/subsection1/Upgrade on UNIX and Linux.html":{"url":"chapter2/section4/subsection1/Upgrade on UNIX and Linux.html","title":"在UNIX和Linux上升级","keywords":"","body":"在UNIX和Linux上升级 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section4/subsection1/Upgrade LSF on UNIX and Linux.html":{"url":"chapter2/section4/subsection1/Upgrade LSF on UNIX and Linux.html","title":"在 UNIX 和 Linux 上升级 LSF","keywords":"","body":"在 UNIX 和 Linux 上升级 LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section4/subsection1/Getting fixes from IBM Fix Central.html":{"url":"chapter2/section4/subsection1/Getting fixes from IBM Fix Central.html","title":"从IBM Fix Central获取修订","keywords":"","body":"从IBM Fix Central获取修订 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section4/subsection2/Migrate on Windows.html":{"url":"chapter2/section4/subsection2/Migrate on Windows.html","title":"在Windows上迁移","keywords":"","body":"在Windows上迁移 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter2/section4/subsection2/Migrate LSF on Windows.html":{"url":"chapter2/section4/subsection2/Migrate LSF on Windows.html","title":"在Windows上迁移LSF","keywords":"","body":"在Windows上迁移LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/user_fundations.html":{"url":"chapter3/user_fundations.html","title":"Chapter 3 用户操作基础","keywords":"","body":"Chapter 3 用户操作基础 概述 IBM Spectrum LSF 的工作负载管理概念和操作。 IBM Spectrum LSF 概述 了解 LSF 如何满足您的作业要求，并找到最佳资源来运行该作业的。 深入 LSF 集群内部 了解在 LSF 主机上运行的各种守护进程，LSF 集群通信路径，以及 LSF 如何容许集群中的主机故障。 深入工作负载管理 了解 LSF 的作业生命周期。 使用 bsub 命令将作业提交到队列，并指定作业的提交选项以修改默认作业行为。 提交的作业在队列中等待，直到将它们调度并调度到主机来执行。 在作业分发时，LSF 会检查哪些主机有资格运行该作业。 启用 EGO 的 LSF 启用具有企业网格协调器（enterprise grid orchestrator EGO）的 LSF 能够提供系统基础结构，来控制和管理集群资源。 资源是应用程序使用的物理和逻辑实体。 LSF资源按照 EGO资源分配计划中的定义进行共享。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section1/LSF_overview.html":{"url":"chapter3/section1/LSF_overview.html","title":"3.1 LSF 概览","keywords":"","body":"3.1 LSF 概览 了解 LSF 是如何满足您的作业要求，并找到最佳资源来运行该作业的。 IBM Spectrum LSF 介绍 IBM Spectrum LSF (\"LSF\", load sharing facility 的简称) 软件是行业领先的企业级软件。LSF 将工作分散在现有的各种 IT 资源中，以创建共享的，可扩展的和容错的基础架构，从而提供更快，更可靠的工作负载性能并降低成本。 LSF 平衡负载和分配资源，并提供对这些资源的访问。 LSF集群组件 LSF 集群管理资源，接受和调度工作负载以及监视所有事件。 用户和管理员可以通过命令行界面，API 或通过IBM Spectrum LSF Application Center (PAC) 访问 LSF。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section1/LSF_introduction.html":{"url":"chapter3/section1/LSF_introduction.html","title":"LSF 介绍","keywords":"","body":"LSF 介绍 IBM Spectrum LSF (\"LSF\", load sharing facility 的简称) 软件是行业领先的企业级软件。LSF 将工作分散在现有的各种 IT 资源中，以创建共享的，可扩展的和容错的基础架构，从而提供更快，更可靠的工作负载性能并降低成本。 LSF 平衡负载和分配资源，并提供对这些资源的访问。 LSF 提供了一个资源管理框架，可满足您的工作要求，找到最佳资源来运行该工作并监视其进度。 作业始终根据主机负载和站点策略运行。 Cluster（集群） 运行 LSF 的一组计算机（主机），它们作为一个单元一起工作，结合了计算能力，工作量和资源。 集群为计算资源网络提供单系统映像。 可以通过多种方式将主机分组到集群中。 集群可以包含： 单个管理组中的所有主机 子网中的所有主机 Hosts（主机） 集群中的主机执行不同的功能。 Master host （主节点） LSF 服务器主机，充当集群的整体协调器，负责所有作业的调度和分配。 Server host （服务主机） 提交并运行作业的主机。 Client host （客户主机） 仅提交作业和任务的主机。 Execution host （执行主机） 运行作业和任务的主机。 Submission host （提交主机） 从中提交作业和任务的主机。 Job（作业） 作业是在 LSF 系统中运行的工作单元。 它是一个提交给 LSF 来执行的命令。 LSF 则根据配置的策略，来调度，控制和跟踪作业。 作业可以是复杂的问题，模拟方案，大规模计算或任何需要计算力的事物。 Job slot（作业槽位） 作业槽是一个存储区，在 LSF 系统中将单个工作单元分配到该存储区中。 主机可以配置有多个作业槽，并且您可以从队列中分派作业，直到所有作业槽都被填满。 您可以将作业槽与集群中的 CPU 总数相关联。 Queue（队列） 集群范围内的作业容器。 所有作业都在队列中等待，直到将它们调度并分配到主机为止。 队列不对应单个主机； 每个队列都可以使用集群中的所有服务器主机，或服务器主机的已配置子集。 将作业提交到队列时，无需指定执行主机。 LSF 会将作业分派到集群中，最佳可用的执行主机来运行该作业。 队列执行不同的作业调度和控制策略。 Resources（资源） 资源是集群中可用于运行作业的对象。 例如，资源包括但不限于主机，CPU 槽和许可证。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section1/cluster_components.html":{"url":"chapter3/section1/cluster_components.html","title":"集群组件","keywords":"","body":"集群组件 LSF 集群管理资源，接受和调度工作负载，以及监视所有事件。 用户和管理员可以通过命令行界面，API 或通过IBM Spectrum LSF Application Center (PAC web 界面) 访问 LSF。 IBM Spectrum LSF LSF的核心包括守护程序和其他功能，用于调度和运行作业以及管理集群资源。 IBM Spectrum LSF License Scheduler 策略，控制组织中不同用户之间共享软件许可证的方式。 IBM Spectrum LSF License Scheduler 可与 FlexNet™ 和其他产品一起使用，以控制和监视许可证的使用情况。 LSF 文档 IBM Spectrum LSF documentation in IBM Knowledge Center IBM知识中心是 IBM 产品文档以及您访问所有IBM Spectrum LSF信息的访问点。 在 IBM Knowledge Center 中的所有内容中搜索您感兴趣的主题，或者在产品中搜索，或者将搜索范围限制为产品的一个版本。 使用您的 IBMid 登录以充分利用 IBM Knowledge Center 中提供的个性化功能。 同样，可以通过向主题添加评论来与同事和 IBM 进行交流。 在原始版本的IBM Spectrum LSF 10.1之后，会定期更新和重新生成，可通过IBM Knowledge Center获得的文档。 LSF 随附的 IBM Spectrum LSF Application Center（LSF Application Center）Basic Edition中，提供了该文档的脱机版本。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section2/Inside_an_LSF_cluster.html":{"url":"chapter3/section2/Inside_an_LSF_cluster.html","title":"3.2 LSF 细观","keywords":"","body":"3.2 LSF 细观 了解在 LSF 主机上运行的各种守护进程，LSF 集群通信路径，以及 LSF 如何容许集群中的主机故障。 LSF 守护程序和进程 集群中的每个主机上都运行多个 LSF 进程。 正在运行的进程的类型和数量，取决于主机是主节点还是计算节点。 LSF 集群通信路径 了解集群中 LSF daemon 之间的通信路径。 容错和自动主控主机故障转移 LSF 的强大体系结构在设计时考虑了容错能力。 系统中的每个组件，都有一个恢复操作。关键组件由另一个组件监视，并且可以自动从故障中恢复。 安全性 了解 LSF 安全模型，身份验证和用户角色。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section2/LSF_daemons_and_processes.html":{"url":"chapter3/section2/LSF_daemons_and_processes.html","title":"LSF 服务与进程","keywords":"","body":"LSF 服务与进程 集群中的每个主机上都运行多个 LSF 进程。 正在运行的进程的类型和数量，取决于主机是主节点还是计算节点。 主节点守护程序进程 LSF 主机根据它们在集群中的角色，运行各种守护进程。 守护程序 角色 mbatchd 作业请求与分配 mbschd 作业调度 sbatchd 作业执行 res 作业执行 lim 节点信息 pim 作业进程信息 elim 动态负荷指标 mbatchd 在主节点上运行的主批处理守护程序。 负责系统中作业的总体状态。 接收作业提交和信息学查询请求。管理队列中保留的作业。由 mbschd 确定将作业分配给主机。 mbschd 在主节点上运行的主批处理调度守护程序。 与 mbatchd 一起使用。 根据作业要求，策略和资源可用性制定调度决策。 将调度决策发送到 mbatchd sbatchd 在每个服务器主机（包括主主机）上运行的从属批处理守护程序。 从 mbatchd 接收运行作业的请求，并管理作业的本地执行。 负责执行本地策略并维护主机上的作业状态。 sbatchd 会为每个作业分出一个子sbatchd。 子 sbatchd 运行一个 res 实例，以创建作业在其中运行的执行环境。 作业完成后，子sbatchd 退出。 res 在每个服务器主机上运行的远程执行服务器（RES）。 接受远程执行请求，以提供清晰，安全的作业和任务的远程执行。 lim 在每个服务器主机上运行的负载信息管理器（LIM）。 收集主机负载和配置信息，并将其转发到在主节点上运行的主LIM。报告由 lsload 和 lshosts 显示的信息。 当 LIM 启动或 CPU（ncpus）数量更改时，将报告静态索引。 Master LIM 在主节点上运行的 LIM。从集群中的节点上运行的 LIM，接收负载信息。 将负载信息转发到 mbatchd，后者将信息转发到 mbschd 以支持调度决策。如果主 LIM 不可用，则候选主节点上的 LIM 将自动接管。 PIM 在每个服务器主机上运行的进程信息管理器（PIM）。 由 LIM 启动，它会定期检查 PIM 并在 PIM 挂掉后重新启动。 收集有关主机上运行的作业进程的信息，例如作业使用的 CPU 和内存，并将该信息报告给 sbatchd。 ELIM 外部LIM（ELIM）是一个可在站点定义的可执行文件，用于收集和跟踪自定义动态负载索引。 ELIM 可以是 Shell 脚本或编译的二进制程序，它们返回您定义的动态资源的值。 ELIM 可执行文件必须命名为 elim.anything，并且位于 LSF_SERVERDIR 中。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section2/cluster_communication_paths.html":{"url":"chapter3/section2/cluster_communication_paths.html","title":"集群通信方式","keywords":"","body":"集群通信方式 了解集群中 LSF daemon 之间的通信路径。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section2/falut_tolerance.html":{"url":"chapter3/section2/falut_tolerance.html","title":"容错","keywords":"","body":"容错 LSF 的强大体系结构在设计时考虑了容错能力。 系统中的每个组件都具有恢复操作，因此，重要组件可以由另一个组件监视，并可以自动从故障中恢复。 即使集群中的某些主机不可用，LSF 也可以继续运行。 集群中的一个主机充当主节点，但是如果该主节点不可用，则由另一台主机候选节点接管。当集群中有一个主节点候选时，LSF 可用。 LSF 可以容许集群中，任何主机或主机组的故障。当某主机不可用时，在该主机上运行的所有作业，将会重新排队运行或丢失，具体取决于该作业是否被标记为可重新运行。其他挂起或正在运行的作业，则不会受到影响。 故障转移的工作原理 容错能力取决于事件日志文件 lsb.events，该文件保存在主文件服务器上。系统中的每个事件都记录在该文件中，包括所有作业提交以及作业和主机状态更改。如果主节点不可用，则从主节点候选列表中选择一个新的主节点，新的主节点上的 sbatchd 守护程序，将启动一个新的 mbatchd 守护程序。 新的 mbatchd 守护程序，会读取lsb.events 文件以恢复系统状态。 重复事件记录 对于不希望仅依靠中央文件服务器获取恢复信息的站点，可以将 LSF 配置为通过保留 lsb.events 文件的副本来维护重复的事件日志。 副本存储在文件服务器上，并且在主副本不可用时使用。 启用重复事件日志记录后，主事件日志将本地存储在第一个主主机上，并在主机恢复时与复制的副本重新同步。 主机故障转移 LSF 主节点是动态选择的。如果当前的主节点不可用，则另一台主机将自动接管。故障转移主机，是从 lsf.conf 文件（在安装时在 install.config 文件中指定）的 LSF_MASTER_LIST 参数中定义的列表中选择的。列表中的第一个可用节点充当主机。 正在运行的作业由每个服务器主机上的 sbatchd 守护程序管理。 当新的 mbatchd 守护程序启动时，它将轮询每个主机上的 sbatchd 守护程序，并找到其作业状态。如果 sbatchd 守护程序失效，但主机仍在运行，则主机上正在运行的作业不会丢失。 重新启动 sbatchd 守护程序后，它将重新获得对主机上正在运行的所有作业的控制。 作业故障转移 作业可以通过可重新运行的方式来提交，如此一来，它们可以从头开始自动运行，也可以通过可检查点的形式提交，如此一来，如果由于主机故障而挂掉，则可以从另一个主机上的检查点重新开始。 如果集群中的所有主机都关闭，则所有正在运行的作业都将丢失。 当主节点的候选节点，恢复并接管为主节点时，它将读取 lsb.events 文件，以获取所有批处理作业的状态。 除非系统将其标记为可重新运行，否则系统关闭时，正在运行的作业将被认为已退出，并且电子邮件将发送给提交用户。等待的作业则保留在队列中，并在主机可用时，进行调度。 分区集群 如果集群因网络故障而分区，则 master LIM 会接管分区的每一侧，而候选主节点则在分区的每一侧都可用。 当每个主机仍然可以访问 LSF 可执行文件时，交互式负载共享仍然可用。. 分区网络 如果对网络进行了分区，则只有一个分区可以访问 lsb.events 文件，因此 LSF 服务仅在分区的一侧可用。一个锁定文件，用于确保集群中仅运行一个 mbatchd 守护程序。 作业异常处理 您可以配置主机和队列，以便 LSF 在作业运行时检测到异常情况，并自动采取适当的措施。 您可以自定义检测到哪些异常以及相应的操作。 例如，您可以将 LSF 设置为在作业退出并显示特定错误代码时，自动重新启动。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section2/security.html":{"url":"chapter3/section2/security.html","title":"安全","keywords":"","body":"安全 了解 LSF 安全模型，身份验证和用户角色。 LSF 安全模型 默认情况下，LSF 安全模型在内部跟踪用户帐户。 LSF 中定义的用户帐户，包括用于提供身份验证的密码和用于提供授权的已分配角色，例如管理员。 LSF 用户角色 没有启用EGO的 LSF 支持以下用户角色： LSF 用户 有权将作业提交到 LSF 集群，并查看作业和群集的状态。 LSF 主要管理员 有权执行集群范围的操作，更改配置文件，重新配置集群以及控制所有用户提交的作业。lsb.params 和lsb.hosts 等配置文件，可配置 LSF 的各个方面。 LSF 管理员 有权执行影响其他LSF用户的操作。集群管理员可以对集群中的所有作业和队列，执行管理操作。可能没有更改 LSF 配置文件的权限。 队列管理员的管理权限仅限于指定的队列。 主机组的管理员管理权限仅限于指定的主机组。 用户组的管理员管理权限仅限于指定的用户组。 启用 EGO 的 LSF 用户角色 启用 EGO 的 LSF，支持以下角色： 集群管理员 可以管理集群中的任何对象和工作负载。 消费者管理员 可以管理他们有权访问的使用者中的任何对象和工作负载。 消费者用户 可以在他们有权访问的使用者中运行工作负载。 用户帐户是在 EGO 中创建和管理的。 EGO 从其用户数据库授权用户。 LSF 用户组 在可以指定 LSF 用户组的任何位置上，指定 UNIX 或 Linux 用户组，以此来直接使用任何现有的 UNIX 和 Linux 用户组。 外部认证 LSF 为倾向于使用外部或第三方安全机制（例如 Kerberos，LDAP 或 ActiveDirectory）的站点，提供了一个安全插件。 您可以创建自定义的 eauth 可执行文件，以提供用户，主机和守护程序的外部身份验证。 凭证是从外部安全系统传递的。还可以通过自定义的 eauth 可执行文件，来从操作系统或从诸如 Kerberos 的身份验证协议获取凭据。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section3/inside_workload_management.html":{"url":"chapter3/section3/inside_workload_management.html","title":"3.3 作业负载管理","keywords":"","body":"3.3 作业负载管理 了解 LSF 作业生命周期。 使用 bsub 将作业提交到队列，并指定作业提交选项以修改默认作业行为。 提交的作业在队列中等待，直到将它们调度并分配到主机上来执行。 在作业分发时，LSF 会检查哪些主机有资格运行该作业。 作业生命周期 LSF 作业会经历几种状态，从作业提交开始，到分发，执行和最终返回作业结果。 作业提交 使用 bsub 命令在命令行上提交作业。 您可以使用 bsub 命令指定许多选项来修改默认行为。 作业必须提交到队列中。 作业调度和分配 提交的作业在队列中等待，直到将它们调度并分配到主机上来执行。 主机选择 每次 LSF 尝试分派作业时，它都会检查哪些主机有资格运行该作业。 作业执行环境 当 LSF 运行作业时，它将环境从提交主机，复制到执行主机。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section3/job_lifecycle.html":{"url":"chapter3/section3/job_lifecycle.html","title":"作业生命周期","keywords":"","body":"作业生命周期 LSF 作业经历几种状态，从作业提交开始，到调度，执行和最终返回作业结果。 1. 提交一个作业 您可以使用 bsub 命令从 LSF 客户端或服务器提交作业。 如果在提交作业时未指定队列，则该作业将提交到默认队列。 作业在等待调度时被排在队列中。 等待的作业处于 PEND 状态。 如果在配置文件lsb.params中定义了 MAX_INFO_DIRS 参数，则该作业将保存在 LSF_SHAREDIR/cluster_name/logdir/info/ 目录中的作业文件中，或者在其子目录之一中。 作业 ID 提交作业时，LSF 为每个作业分配唯一的作业 ID。 作业名 您还可以使用 bsub的 -J 选项为作业分配一个任意名称。 与作业 ID 不同，作业名称不一定是唯一的。 2. 调度该作业 1 主批处理守护程序（mbatchd）查看队列中的作业，然后将要调度的作业，发送到主批处理调度程序守护程序（mbschd）。 作业以预设的时间间隔进行调度（由配置文件 lsb.params 中的参数 JOB_SCHEDULING_INTERVAL 定义）。 2 mbschd 根据以下内容评估作业并制定计划决策： 作业优先级 调度策略 可用资源 3 mbschd 选择作业可以运行的最佳主机，并将其决策发送回 mbatchd。资源信息由主负载信息管理器（LIM）以预设的时间间隔，从服务器主机上的 LIM 上收集。 主 LIM 将此信息传达给 mbatchd，后者又将其传达给 mbschd 以支持调度决策。 3. 分配该作业 mbatchd 收到调度决策后，便立即将作业分配给主机。 4. 运行该作业 从属批处理守护程序（sbatchd）具有以下功能： 1 从 mbatchd 接收请求。 2 为作业创建一个子 sbatchd 。 3 创建执行环境。 4 通过使用远程执行服务器（res）启动作业。 LSF 将执行环境从提交主机复制到执行主机： 作业所需的环境变量 作业开始运行的工作目录 其他与系统有关的环境设置 在 UNIX 和 Linux 上，资源限制和 umask 在 Windows，桌面和 Windows 根目录 作业在提交该作业的用户帐户下运行，并且状态为 RUN。 5. 返回结果 作业完成后，如果作业完成没有任何问题，则会被分配为 \"DONE\" 状态。 如果错误导致作业无法完成，则会为该作业分配 \"EXIT\" 状态。 sbatchd 传递作业信息，例如错误信息，并输出到 mbatchd。 6. 向客户发送 email mbatchd 通过电子邮件将作业输出，作业错误和作业信息返回给提交主机。 使用 bsub 的 -o 和 -e 选项将作业输出和错误发送到文件。 作业报告通过电子邮件发送给 LSF 客户端，其中包括以下信息： 作业信息: CPU 使用 内存使用 提交作业的帐户名称 作业输出 错误 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section3/job_submission.html":{"url":"chapter3/section3/job_submission.html","title":"作业提交","keywords":"","body":"作业提交 使用 bsub 命令在命令行上提交作业。 您可以使用 bsub 命令指定许多选项来修改默认行为。 而作业必须提交到队列中。 队列 队列代表一组挂起等待的作业，它们按定义的顺序排列，并等待其使用资源的机会。 队列执行不同的作业调度和控制策略。 队列有以下特征: 优先级 名称 队列限制 (对主机、作业数、用户、组、或者处理器的限制) 标准 UNIX 和 Linux 限制（内存，交换，进程，CPU） 调度策略 管理员 运行条件 负载共享阈值条件 UNIX nice 值 (设置 UNIX 和 Linux 调度程序优先级) 队列优先级 定义搜索队列，以确定要处理的作业的顺序。 LSF 管理员为队列分配了优先级，其中数值越高，优先级越高。 LSF按从高到低的优先级为队列提供服务。 如果多个队列具有相同的优先级，则 LSF 按照先来先服务的顺序，调度这些队列中的所有作业。 自动队列选择 提交作业时，LSF 会考虑作业要求，并自动从候选默认队列列表中，选择合适的队列。 LSF 根据以下约束条件选择合适的队列： 用户访问限制 不允许该用户提交作业的队列，不会予以考虑。 节点限制 如果作业明确指定了可以在其上运行作业的主机节点列表，则必须将所选的队列，配置为作业发送到列表中的主机。 队列状态 不考虑关闭的队列。 排他执行限制 如果作业需要排他执行，则未配置为接受排他作业的队列，将不予考虑。 作业所需的资源 作业请求的资源，必须在所选队列的资源分配限制内。 如果多个队列满足上述要求，则选择满足要求的候选队列中，列出的第一个队列。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section3/job_scheduling_and_dispatch.html":{"url":"chapter3/section3/job_scheduling_and_dispatch.html","title":"作业调度","keywords":"","body":"作业调度与分配 提交的作业在队列中等待，直到将它们调度并分配到主机以执行。 将作业提交给 LSF 时，许多因素控制着作业开始的时间和地点: 队列或主机的活动时间窗口 作业的资源需求 合格主机的可用性 各种作业槽位限制 作业依赖条件 Fairshare 限制（已配置的用户共享策略） 负载条件 调度策略 为了解决各种问题，LSF 允许在同一集群中，使用多种调度策略。 LSF 有几种队列调度策略，例如排他，抢占，公平共享和分层公平共享。 先来先服务（FCFS）调度: 默认情况下，队列中的作业按 FCFS 顺序分派。 这意味着作业将根据其在队列中的顺序进行调度。 服务水平协议（SLA）调度: LSF 中的 SLA 是 “及时” 的调度策略，用于调度 LSF 管理员和 LSF 用户之间约定的服务。 SLA 调度策略定义应从每个 SLA 运行多少作业，以达到配置的目标。 公平共享调度: 如果您为队列指定一个公平共享调度策略，或者如果已配置主机分区，则 LSF 会根据分配的用户份额，资源使用，或其他因素在用户之间调度作业。 抢占式调度: 您可以指定所需的行为，以便当两个或多个作业竞争同一资源时，一个作业优先于另一个作业。 抢占不仅适用于作业槽位，而且还适用于提前预订（为特定作业保留主机节点）和许可证（使用 IBM Platform License Scheduler）。 回填式调度： 允许小型作业在为其他作业保留的作业槽位上运行，前提是，回填作业在保留时间到期，且资源使用到期之前完成。 调度与分配 定期安排作业（默认为5秒）。 一旦安排了作业，就可以立即将其分配给主机。 为了防止任何节点过载，默认情况下，LSF 在将作业分发到同一节点之间，会等待一小段时间。 分配顺序 作业不一定按提交顺序分派。 定义队列时，每个队列都有一个由 LSF 管理员设置的优先级编号。LSF 会尝试首先从优先级最高的队列中，启动作业。 LSF 按以下顺序考虑要分派的作业： 对于每个队列，从最高优先级到最低优先级。 如果多个队列具有相同的优先级，则 LSF 会按照先来先服务（FCFS）的顺序，调度这些队列中的所有作业。 对于队列中的每个作业，根据 FCFS 顺序。 如果有任何主机有资格运行此作业，将在最合格的主机上启动该作业，并标记该主机不具备启动任何其他作业的资格，直到经过 JOB_ACCEPT_INTERVAL 参数所指定的时间段为止。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section3/host_selection.html":{"url":"chapter3/section3/host_selection.html","title":"节点选择","keywords":"","body":"节点选择 每次 LSF 尝试分配作业时，它都会检查哪些主机有资格运行该作业。 许多条件决定了节点是否符合条件: 主机调度窗口 作业的资源需求 队列的资源需求 队列中的节点列表 节点负载水平 节点的作业槽位限制 用户配额与用户限制 只有满足所有条件，主机节点才有资格运行作业。 如果队列中已有一个作业，并且该作业的合格主机可用，则该作业将放置在该主机上。 如果有多个主机符合条件，则根据作业和队列资源要求，在最佳主机上启动作业。 主机负载级别 如果主机的负载索引（例如r1m，pg，mem）的值，在配置的调度阈值之内，则该主机可用。 您可以配置两种调度阈值：主机和队列。 如果主机上的任何负载索引，超过相应的主机阈值或队列阈值，则该主机不符合运行任何作业的条件。 合格的主机 当 LSF 尝试放置作业时，它将获取所有主机的当前负载信息。 将每个主机上的负载级别，与在 lsb.hosts 的 Host 部分中，为该主机配置的调度阈值，以及在 lsb.queues 中配置的按队列调度阈值进行比较。 如果任何负载索引，超过其按队列或按主机调度的阈值，则不会在该主机上启动任何新作业。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section3/job_execution_environment.html":{"url":"chapter3/section3/job_execution_environment.html","title":"作业运行环境","keywords":"","body":"作业运行环境 当 LSF 运行作业时，它将环境，从提交主机复制到执行主机。 执行环境包括以下信息: 作业所需的环境变量 作业开始运行的工作目录 其他与系统有关的环境设置，例如资源使用限制 共享的用户目录 为了提供透明的远程执行，LSF 命令确定用户的当前工作目录，并在远程主机上使用该目录。 可执行文件和 PATH 环境变量 可执行文件的搜索路径（PATH环境变量），未更改地传递到远程执行主机。 注意 在混合集群中，当用户二进制目录，在不同主机类型上具有相同路径名时，LSF 效果最佳。使用相同的路径名，可使 PATH 变量在所有主机上均有效。 为了便于管理，LSF 配置文件存储在共享目录中。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section4/LSF_with_EGO_enabled.html":{"url":"chapter3/section4/LSF_with_EGO_enabled.html","title":"3.4 启用 EGO 的 LSF","keywords":"","body":"3.4 启用 EGO 的 LSF 具有 LSF 的企业网格协调器（enterprise grid orchestrator EGO）能够提供系统基础结构，来控制和管理集群资源。 资源是应用程序使用的物理和逻辑实体。 LSF 资源按照 EGO资源分配计划中的定义进行共享。 EGO组件概述 可以使用 LSF 启用 EGO，以提供系统基础结构来控制和管理群集资源。 资源 资源是应用程序用来运行的物理和逻辑实体。 资源是一个通用术语，可以包含低级内容，例如共享内存段或信号灯。 在 LSF 中，EGO 管理CPU 槽位。. LSF 如何通过 EGO 共享资源 可以通过定义 EGO 资源分配计划，来共享 LSF 资源。 LSF 向 EGO 资源管理器请求资源。 根据资源分配计划中指定的值，资源管理器返回可用槽位数（m）和该槽位所在的主机的名称。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section4/EGO_component_overview.html":{"url":"chapter3/section4/EGO_component_overview.html","title":"EGO 组件概览","keywords":"","body":"EGO 组件概览 可以使用 LSF 启用 EGO，以提供系统基础结构来控制和管理群集资源。 就像在一台计算机上运行的操作系统聚合并虚拟化物理资源，并将其分配给应用程序一样，EGO 可以在分布式环境中执行类似的功能。 EGO 管理逻辑和物理资源，并支持所有形式的应用程序。 EGO 管理资源的供应，使资源可供应用程序使用。 主机可以分为两类：管理节点和计算节点。 管理节点为集群提供专门服务，而计算节点运行用户作业负载。 管理节点 管理节点在集群内提供集群和工作负载管理服务，并且不应为用户运行工作负载。 主节点，所有候选主节点和会话管理器主机必须是管理节点。 其他管理节点，包括运行数据加载器的主机，和用于报告功能的数据清除器。所有管理节点，都在同一操作系统上运行：所有 Windows，所有 UNIX 或所有 Linux。 主节点 主节点是集群中安装的第一台主机。 集群的资源管理器（vemkd）驻留在此主机上。 主节点控制集群中其余的主机，并且是集群客户端的接口。 候选主节点 一次只有一台主机。 如果主节点发生故障，则另一台主机将自动接管主节点角色。 可以充当主节点的主机称为候选主节点。 会话管理节点 一个或多个管理节点运行会话管理器。 管理节点上每个可用插槽都有一个会话管理器。 每个应用程序有一个会话管理器。 计算节点 计算节点，是集群中为消费者提供计算资源的那些主机。 集群可以包含任意数量的计算节点，但必须至少具有一个计算节点。 CPU 槽位 CPU 槽位是用于测量计算资源的单位。 一个 CPU 槽位可以在计算节点上运行一个服务实例，也可以在管理节点上运行一个会话管理器。 守护进程 vemkd 在主节点上运行的 VEM 内核守护程序。 它启动其他守护程序并响应分配请求。 egosc v 服务控制器从 vemkd 守护程序，请求适当的资源并控制服务实例。 pem 进程执行管理器，可用于 vemkd 守护程序，启动，控制和监视活动，以及收集和发送运行时资源使用情况。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section4/resources.html":{"url":"chapter3/section4/resources.html","title":"资源","keywords":"","body":"资源 资源是应用程序用来运行的物理和逻辑实体。 资源是一个通用术语，可以包含低级内容，例如共享内存段，或信号灯。 在 LSF 中，EGO 管理 CPU 槽位。 特定类型的资源具有属性。 例如，计算主机具有内存，CPU 利用率，操作系统类型的属性。 资源组 可以将资源分组为逻辑组，以简化标识，资源分配或用于管理和监视目的。 这些资源组，用于为使用者提供一组类似的主机来运行工作负载。 资源组中的任何主机都可以运行相同的工作负载。 下图显示了两个资源组： 管理节点 计算节点 如果所有主机都相同，则这些资源组可能就足够了。 如果您的应用程序需要特定类型的主机（例如，以最低的处理器速度运行），并且并非所有主机都满足此条件，则可能需要创建资源组以将类似的主机分组在一起。 例如，对资源进行分组的一种简单方法，可能是按操作系统类型对主机进行分组。 EGO 提供了一种通用的资源分组机制。 资源可能来自系统，或经过系统，因此 EGO 支持资源组中的动态成员资格。 可以将主机显式放置到各个资源组中，也可以根据特定条件，使用动态成员资格来定义资源组。 此条件包括操作系统类型，CPU 速度，总内存或交换配置或自定义属性。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter3/section4/LSF_resource_sharing.html":{"url":"chapter3/section4/LSF_resource_sharing.html","title":"LSF 资源共享","keywords":"","body":"LSF 资源共享 可以通过定义 EGO 资源分配计划来共享 LSF 资源。 LSF 向 EGO 资源管理器请求资源。 根据资源分配计划中指定的值，资源管理器返回可用槽数（m）和该槽位所在的主机的名称。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/administrator_fundations.html":{"url":"chapter4/administrator_fundations.html","title":"Chapter 4 管理员操作基础","keywords":"","body":"Chapter 4 管理员操作基础 本章内容是 IBM Spectrum LSF 的管理员概述，掌握本章，可以了解如何管理各种类型的工作负载和集群操作。 LSF 集群概览 概述您的集群以及重要的 LSF 目录和配置文件的位置。 使用 LSF 调度 启动和停止 LSF 守护程序，重新配置集群属性。 检查 LSF 状态并提交 LSF 作业。 解决 LSF 问题 解决常见的 LSF 问题并了解 LSF 错误信息。 如果在这里找不到解决问题的方法，请联系 IBM 支持。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section1/cluster_overview.html":{"url":"chapter4/section1/cluster_overview.html","title":"4.1 集群概览","keywords":"","body":"4.1 集群概览 概述您的集群以及重要的 LSF 目录和配置文件的位置。 LSF术语和概念 首次使用 LSF 之前，应先阅读 LSF Foundations Guide，以基本了解作业负载管理和作业提交，以及 Administrator Foundations Guide，以概述集群管理和操作。 集群特征 在安装后查询集群的名称，集群管理员以及定义主机的位置。 文件系统，目录，文件 LSF 设计用于所有主机都具有共享文件系统，且所有主机上文件都同名的网络。 重要目录和配置文件 通过多个配置文件管理 LSF 配置，您可以使用这些文件来修改集群的行为。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section1/terms_and_concepts.html":{"url":"chapter4/section1/terms_and_concepts.html","title":"术语与概念","keywords":"","body":"术语与概念 首次使用 LSF 之前，应先阅读 LSF Foundations Guide，以基本了解作业负载管理和作业提交，以及 Administrator Foundations Guide，以概述集群管理和操作。 作业状态 IBM Spectrum LSF 作业有几种状态。 PEND 在队列中等待调度和分配。 RUN 分派给主机并运行。 DONE 正常完成，退出状态码为 0。 EXIT 以非 0 退出值结束。 PSUSP 作业等待时挂起。 USUSP 被用户暂停。 SSUSP 被 LSF 系统暂停。 POST_DONE 后处理完成，没有错误。 POST_ERR 后处理完成，但有错误。 UNKWN mbatchd 守护程序，与作业运行所在主机上的 sbatchd 守护程序失去联系。 WAIT 对于提交到块作业队列的作业，是等待运行的块作业成员。 ZOMBI 如果杀死不可重新运行的作业，或将可重新运行的作业重新排队时，如果执行主机不可访问，则该作业将变为ZOMBI。 主机节点 LSF 主机是集群中的单个计算机。 每个主机可能有多个处理器。 多处理器主机用于运行并行作业。 具有单个进程队列的多处理器主机被视为一台计算机。 装满处理器的盒子，每个处理器都有自己的处理队列，被视为一组单独的计算机。 Tip 主机的名称应唯一。 它们不能与集群名称或为集群定义的任何队列相同。 作业 LSF 作业是在 LSF 系统中运行的工作单元。 作业是使用 bsub 命令提交给 LSF 以便执行的命令。 LSF 根据配置的策略计划，控制和跟踪作业。 作业可能是复杂的问题，模拟方案，大量的计算，以及任何需要计算能力的事物。 作业文件 将作业提交到队列后，LSF 会将其保存在作业文件中，直到适合运行条件为止。 然后，使用作业文件运行作业。 在 UNIX 上，作业文件是在执行时运行的 Bourne Shell 脚本。 在 Windows 上，作业文件是在执行时处理的批处理文件。 交互式批处理作业 交互式批处理作业，是一个批处理作业，它使您可以与应用程序进行交互，并且仍然利用 LSF 调度策略和容错能力。 所有输入和输出都是通过用于键入作业提交命令的终端进行的。 提交交互式作业时，在作业等待调度时会显示一条消息。 在交互式作业完成或终止之前，无法提交新作业。 交互式任务 交互式任务是没有提交给批处理队列，并由 LSF 调度但立即分派的命令。 LSF 查找任务所需的资源，并在具有所需资源且负载较轻的候选主机中选择最佳主机。 每个命令可以是单个进程，也可以是一组协作进程。 在不使用 LSF 的批处理功能的情况下运行任务，但仍具有资源需求和根据负载选择最佳主机来运行任务的优势。 本地任务 本地任务是没有意义的应用程序或命令，不能远程运行。 例如，UNIX上的 ls 命令。 远程任务 远程任务，是可以在集群中的另一台计算机上运行的应用程序或命令。 主机类型与主机型号 LSF中的主机，由主机类型和主机模型表征。 以下示例是类型为 X86_64 的主机，其主机型号为 Opteron240，Opteron840，Intel_EM64T 等。 主机类型 LSF 主机类型，是操作系统和主机 CPU 体系结构的组合。 在相同计算机体系结构上，运行相同操作系统的所有计算机都属于同一类型。 这些主机彼此二进制兼容。 每种主机类型，通常需要一组不同的 LSF 二进制文件。 主机型号 LSF 主机模型是计算机的主机类型，它确定在负载和放置计算中应用的 CPU 速度缩放因子。 调度作业时，要考虑 CPU 因素。 资源 LSF 资源是 LSF 系统资源中的对象，LSF 使用这些对象，来跟踪作业要求并根据它们在各个主机上的可用性来调度作业。 资源使用 LSF 系统使用内置的和配置的资源，来跟踪资源的可用性和使用情况。 根据各个主机上可用的资源，来调度作业。 通过 LSF 系统提交的作业，在运行时将受到使用的资源的监视。 此信息用于强制执行资源限制，和负载阈值以及公平分配策略。 LSF收集以下类型的信息： 作业中，所有进程消耗的总 CPU 时间 作业中，所有当前正在运行的进程的总常驻内存使用量（KB 为单位） 作业中，所有当前正在运行的进程的总虚拟内存使用量（KB 为单位） 作业中，当前活动的进程组 ID 作业中，当前活跃的进程 在 UNIX 和 Linux 上，通过 PIM 收集作业级资源使用情况。 负载指数 负载指数，衡量集群中主机上动态非共享资源的可用性。 内置在 LIM 中的负载指数，会按固定的时间间隔进行更新。 外部负载指数 由 LSF 管理员定义和配置，并由外部负载信息管理器（ELIM）程序收集。 当收到新值时，ELIM 也会更新 LIM。 静态资源 表示不会随时间变化的主机信息的内置资源，例如，可用于用户进程的最大 RAM 或计算机中的处理器数量。 大多数静态资源由 LIM 在启动时确定。 静态资源，可用于根据二进制体系结构，相对 CPU 速度和系统配置，为特定作业选择适当的主机。 负载阈值 您的 LSF 管理员，可以配置两种类型的负载阈值来调度队列中的作业。 每个负载阈值指定一个负载索引值： loadSched 负载阈值，确定用于调度待处理作业的负载条件。 如果主机的负载超出任何定义的 loadSched，则无法在主机上启动作业。 此阈值还用作恢复挂起作业的条件。 loadStop 负载阈值，确定何时可以暂停正在运行的作业。 要在主机上调度作业，该主机上的负载级别，必须同时满足为该主机配置的阈值，和要从中调度作业的队列的阈值。 负载索引的值，可能随负载而增加或减少，具体取决于特定负载索引的含义。 因此，将主机负载条件与阈值进行比较时，需要根据负载指数使用大于（>）或小于（ 运行时资源使用限制 在作业运行时限制资源的使用。 发出消耗超过指定资源量的作业的信号。 硬性限制和软性限制 在队列级别指定的资源限制是硬限制，而在作业提交中指定的资源限制是软限制。 有关硬限制和软限制的信息，请参见 setrlimit 手册页。 资源分配限制 限制在作业调度期间是必须启动的，用于启动不同类别的作业的资源量，以及限制适用于哪些资源使用者。 如果消耗了所有资源，则在释放某些资源之前，无法启动更多作业。 资源需求 (bsub -R) bsub -R 选项指定作业的资源要求。 资源要求限制了可以在其上运行作业的主机。 符合资源要求的主机，是候选主机。 当 LSF 调度作业时，它将收集所有候选主机的负载索引值，并将它们与调度条件进行比较。 仅当所有负载值均在调度阈值之内时，作业才会调度到主机。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section1/cluster_characteristics.html":{"url":"chapter4/section1/cluster_characteristics.html","title":"集群特征","keywords":"","body":"集群特征 在安装后找到集群的名称，集群管理员以及定义主机节点的位置。 集群名称与管理员 根据 lsfinstall -f install.config 命令指定的安装选项，以及在 install.config 文件中选择的选项安装集群。您在安装时指定的集群名称，是 LSF_CONFDIR/lsf.cluster.cluster_name 文件的名称的一部分。 /usr/share/lsf/lsf_10/conf/lsf.cluster.lsf_10 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 ClusterAdmins 部分中列出了集群管理员。 LSF 主机节点 在 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 Hosts 部分中，列出了集群中安装的主机类型。 LSF 主节点，是在 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 Hosts 部分中配置的第一台主机。 集群中定义的 LSF 服务器主机，在 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 Hosts 部分的 “Server” 列中用 1 表示。 在集群中定义的 LSF 仅客户端主机，在 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 Hosts 部分的 “Server” 列中用 0 表示。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section1/filesystems_directories_and_files.html":{"url":"chapter4/section1/filesystems_directories_and_files.html","title":"文件系统、目录和文件","keywords":"","body":"文件系统、目录和文件 LSF 是为所有主机都具有共享文件系统，且所有主机上的文件具有相同名称的网络而设计的。 LSF 支持在批处理作业运行之前将用户数据复制到执行主机，并在作业运行之后将结果复制回来。 在不共享文件系统的网络中，此支持，可用于使远程作业访问本地数据。 支持的文件系统 UNIX 在 UNIX 系统上，LSF 支持以下共享文件系统： Network File System (NFS) NFS 文件系统可以永久安装，也可以使用 automount 命令按需安装。 Andrew File System (AFS) 在 9.1.2 集成的参数下，以及一些已发布的配置参数下按需支持。 支持访问 AFS，AFS 上的JOB_SPOOL_DIR ，以及 AFS 上的作业输出和错误文件的顺序和并行用户作业。 Distributed File System (DCE/DFS) 按需支持。 Windows 在 Windows 上，可以从 Windows 服务器计算机的主机之间，共享包含 LSF 文件的目录。 非共享目录和文件 LSF 用于具有共享文件空间的网络。 当共享文件空间不可用时，LSF 可以在作业运行之前，将所需文件复制到执行主机，并在作业完成后，将结果文件复制回提交主机。 某些网络不在主机之间共享文件。 LSF 仍可以在这些网络上使用，但容错能力却有所降低。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section1/Example directory structures.html":{"url":"chapter4/section1/Example directory structures.html","title":"示例目录结构","keywords":"","body":"示例目录结构 下图显示了在 UNIX 和 Linux 或 Microsoft Windows 上新安装的典型目录结构。 根据您安装的产品和选择的平台，目录结构可能会有所不同。 UNIX and Linux 下图显示了使用 lsfinstall 命令的新 UNIX 或 Linux 安装的典型目录结构。 Microsoft Windows 下图显示了新的 Windows 安装的典型目录结构。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section1/important_directories_and_configuration_files.html":{"url":"chapter4/section1/important_directories_and_configuration_files.html","title":"重要的文件目录与配置文件","keywords":"","body":"重要的文件目录与配置文件 LSF 配置，通过几个配置文件进行管理，您可以使用这些文件来修改集群的行为。 四个重要的 LSF 配置文件 以下是您最常使用的四个最重要的文件： LSF_CONFDIR/lsf.conf LSF_CONFDIR/lsf.cluster.cluster_name LSF_CONFDIR/lsf.shared LSB_CONFDIR/cluster_name/configdir/lsb.queues 这些文件，是在产品安装期间根据您在 install.config 文件中指定的选项创建的。 安装后，您可以更改这些文件中的配置参数，以适合您的站点的需求。 谁拥有这些文件 除了由 root 拥有的 LSF_CONFDIR/lsf.conf 外，所有这些文件均由 LSF 主管理员拥有，并且所有集群用户均可读取。 lsf.conf LSF 中最重要的文件。 它包含配置目录，日志目录，库和其他全局配置信息的路径。 lsf.conf 文件的位置由LSF_ENVDIR 变量定义。 如果 LSF 找不到此文件，则它将无法正常启动。默认情况下，LSF会检查 LSF_ENVDIR 参数定义的目录中的 lsf.conf 文件的位置。 如果 lsf.conf 文件不在 LSF_ENVDIR 中，则 LSF 在 /etc 目录中查找它。 lsf.cluster.cluster_name 定义集群中所有主机的主机名，型号和类型。 它还定义了 LSF 管理员的用户名，以及一个集群的不同共享资源的位置。 lsf.shared 该文件就像一个字典，定义了集群使用的所有关键字。 您可以添加自己的关键字，来指定资源或主机类型的名称。 lsb.queues 为一个集群定义作业负载队列及其参数。 LSF 目录 以下目录归 LSF 主管理员所有，并且所有集群用户均可读取： 目录 描述 示例 LSF_CONFDIR LSF 配置目录 /usr/share/lsf/cluster1/conf/ LSB_CONFDIR 批处理系统配置目录 /usr/share/lsf/cluster1/conf/lsbatch/ LSB_SHAREDIR 作业历史目录 /usr/share/lsf/cluster1/work/ LSF_LOGDIR 服务器守护程序错误日志，每个守护程序一个 /usr/share/lsf/cluster1/log/ 以下目录归 root 拥有，并且所有集群用户均可读取： 目录 描述 示例 LSF_BINDIR LSF 用户命令，由相同类型的所有主机共享 /usr/share/lsf/cluster1/10.1/sparc-sol10/bin/ LSF_INCLUDEDIR 头文件 lsf/lsf.h 和 lsf/lsbatch.h /usr/share/lsf/cluster1/10.1/include/ LSF_LIBDIR LSF 库，由相同类型的所有主机共享 /usr/share/lsf/cluster1/10.1/sparc-sol10/lib/ LSF_MANDIR LSF manual 手册 /usr/share/lsf/cluster1/10.1/man/ LSF_MISC 示例和其他杂项文件 /usr/share/lsf/cluster1/10.1/misc/ LSF_SERVERDIR 服务器守护程序二进制文件，脚本和其他实用程序，由相同类型的所有主机共享 /usr/share/lsf/cluster1/10.1/sparc-sol10/etc/ LSF_TOP 顶级安装目录 /usr/share/lsf/cluster1/ 可以在 LSF_CONFDIR/lsf.conf 文件中指定其他配置目录。 LSF 集群配置文件 以下文件归 LSF 主管理员所有，并且所有集群用户均可读取： 文件描述 示例 全局配置文件，描述集群的配置和操作 /usr/share/lsf/cluster1/conf/ego/cluster1/kernel/ego.conf /usr/share/lsf/cluster1/conf/lsf.conf 所有集群共享的关键字定义文件。 定义集群名称，主机类型，主机模型和特定于站点的资源 /usr/share/lsf/cluster1/conf/lsf.shared 群集配置文件，用于定义主机，管理员和站点定义的共享资源的位置 /usr/share/lsf/cluster1/conf/lsf.cluster.cluster1 任务名称及其默认资源要求的映射文件 /usr/share/lsf/cluster1/conf/lsf.task /usr/share/lsf/cluster1/conf/lsf.task.cluster1 LSF 批处理作业负载系统配置文件 以下文件归 LSF 主管理员所有，并且所有集群用户均可读取： 文件描述 示例 服务器主机及其属性，例如计划负载阈值，调度窗口和作业槽位限制。 如果在此文件中未定义任何主机，则假定LSF_CONFDIR/lsf.cluster.cluster_name中列出的所有 LSF 服务器主机都是 LSF 批处理服务器主机。 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.hosts LSF 调度程序和资源代理插件模块。 如果未配置任何调度程序，或资源代理模块，则 LSF 使用默认调度程序插件模块schmod_default。 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.modules LSF 批处理系统参数文件 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.params 作业队列定义 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.queues 资源分配限制，导出和资源使用限制。 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.resources LSF 用户组，用户和用户组的分层公平共享，以及用户和用户组的作业位置限制。 还用于为 LSF 多集群功能配置帐户映射。 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.users 应用程序概要文件，其中包含相同类型作业的通用参数，包括应用程序的执行要求，所需的资源以及运行和管理方式。 该文件是可选的。 使用 lsb.params 文件中的DEFAULT_APPLICATION 参数，为所有作业指定默认的应用程序配置文件。 LSF 不会自动分配默认的应用程序配置文件。 /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.applicatons LSF 批处理日志文件 文件描述 示例 批处理事件日志 /usr/share/lsf/cluster1/work/ cluster1/logdir/lsb.events 批处理记帐日志 /usr/share/lsf/cluster1/work/ cluster1/logdir/lsb.acct 守护程序日志文件 LSF 服务器守护程序日志文件，存储在 LSF_CONFDIR/lsf.conf 中由 LSF_LOGDIR 指定的目录中。 文件 示例 Load information manager (lim) /usr/share/lsf/cluster1/log/lim.log.hosta Remote execution server (res) /usr/share/lsf/cluster1/log/res.log.hosta Master batch daemon (mbatchd) /usr/share/lsf/cluster1/log/ mbatchd.log.hosta Master scheduler daemon (mbschd) /usr/share/lsf/cluster1/log/mbschd.log.hosta Slave batch daemon (sbatchd) /usr/share/lsf/cluster1/log/sbatchd.log.hosta Process information manager (pim) /usr/share/lsf/cluster1/log/ pim.log.hosta 注意： 谁拥有，谁应该写给LSF_LOGDIR 确保 LSF 主管理员拥有 LSF 日志目录（LSF_LOGDIR参数），并且 root 可以写入该目录。 如果 LSF 服务器无法写入 LSF_LOGDIR 参数，则在 /tmp 中创建错误日志。. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/work_with_LSF.html":{"url":"chapter4/section2/work_with_LSF.html","title":"4.2 使用 LSF","keywords":"","body":"4.2 使用 LSF 启动和停止 LSF 守护程序，然后重新配置集群属性。 检查 LSF 状态并提交 LSF 作业。 启动，停止和重新配置LSF 使用 LSF 管理命令 lsadmin 和 badmin 启动和停止 LSF 守护程序，并重新配置集群属性。 检查 LSF 状态 使用LSF管理命令检查集群配置，查看集群状态，以及 LSF 批处理作业负载系统配置和状态。 运行 LSF 作业 使用 bsub 和 lsrun 命令通过 LSF 运行作业。 使用 bjobs 命令查看您的作业状态。 使用 bstop，bresume 和 bkill 命令控制作业执行。 管理用户，主机和队列 使用 cshrc.lsf 和 profile.lsf 使集群对用户可用。 从集群中添加或删除主机和队列。 配置 LSF 启动 使用 lsf.sudoers 文件，以便 LSF 管理员可以启动和停止 LSF 守护程序。 将 LSF 设置为自动启动。 管理软件许可证和其他共享资源 设置 LSF 外部 LIM（ELIM），以将软件许可证作为动态共享资源进行监视。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter4/section2/subsection1/start_stop_and_reconfigure_LSF.html":{"url":"chapter4/section2/subsection1/start_stop_and_reconfigure_LSF.html","title":"开启、结束与重配置 LSF","keywords":"","body":"开启、结束与重配置 LSF 使用LSF管理命令 lsadmin 和 badmin 启动和停止 LSF 守护程序，并重新配置集群属性。 两个 LSF 管理命令（lsadmin 和 badmin） 注意 只有 LSF 管理员或 root 用户可以运行这些命令。 要启动和停止 LSF，以及在更改任何配置文件后，重新配置 LSF，请使用以下命令： lsadmin 命令控制 lim 和 res 守护程序的操作。 badmin 命令控制 mbatchd 和 sbatchd 守护程序的操作。 如果您以 非root 用户身份安装 LSF 默认情况下，只有 root 可以启动 LSF 守护程序。 如果 lsfinstall 命令检测到您以 非root 用户身份安装，则选择配置多用户集群还是单用户集群： 多用户配置 只有 root 可以启动 LSF 守护程序。 任何用户都可以将作业提交到您的集群。有关更改 lsadmin 和 badmin命令的所有权和权限的信息，请参阅 Troubleshooting LSF problems. 要允许 LSF 管理员启动和停止 LSF 守护程序，请按照 Configure LSF Startup 中的说明设置 /etc/lsf.sudoers 文件。 单用户 您的用户帐户必须是 LSF 主要管理员。 您可以启动 LSF 守护程序，但是只有您的用户帐户,才能将作业提交到集群。 您的用户帐户必须能够读取系统内核信息，例如 /dev/kmem。 使用 cshrc.lsf 和 profile.lsf 设置 LSF 环境 使用 LSF 之前，必须使用 cshrc.lsf 或 profile.lsf 文件设置 LSF 执行环境。 启动集群 使用 lsadmin 和 badmin 命令启动 LSF 守护程序。 停止集群 使用 lsadmin 和 badmin 命令停止 LSF 守护程序。 使用 lsadmin 和 badmin 重新配置群集 更改任何配置文件后，请使用 lsadmin 和 badmin 命令重新配置 LSF。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection1/Setting up the LSF environment.html":{"url":"chapter4/section2/subsection1/Setting up the LSF environment.html","title":"设置 LSF 环境","keywords":"","body":"设置 LSF 环境 使用 LSF 之前，必须使用 cshrc.lsf 或 profile.lsf 文件设置 LSF 执行环境。 步骤 登录到 LSF 主机后，使用以下 shell 程序环境文件之一，来设置您的 LSF 环境。 在 csh 或 tcsh shell 中，运行 source 命令： % source /conf/cshrc.lsf 在 sh , ksh, 或 bash shell 中运行以下命令: $ . /conf/profile.lsf 文件 cshrc.lsf 和 profile.lsf 是在安装过程中通过 lsfinstall 命令创建的，以设置 LSF 操作环境。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection1/Starting your cluster.html":{"url":"chapter4/section2/subsection1/Starting your cluster.html","title":"启动集群","keywords":"","body":"启动集群 使用 lsadmin 和 badmin 命令启动 LSF 守护程序。 步骤 以 root 用户身份登录到每个 LSF 服务器主机。 如果您以 非root 用户身份安装了单用户集群，请以 LSF 主管理员身份登录。 从 LSF 主节点开始，然后在所有 LSF 主机上重复这些步骤。 使用以下命令启动LSF集群： # lsadmin limstartup # lsadmin resstartup # badmin hstartup 在使用任何 LSF 命令之前，请等待几分钟，让 lim 守护程序所有主机执行以下操作： 互相联系 选择主节点 交换初始化信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection1/Stopping your cluster.html":{"url":"chapter4/section2/subsection1/Stopping your cluster.html","title":"停止集群","keywords":"","body":"停止集群 使用 lsadmin 和 badmin 命令停止 LSF 守护程序。 步骤 以 root 用户身份登录到每个 LSF 服务器主机。 如果您以 非root 用户身份安装了单用户集群，请以 LSF 主管理员身份登录。 使用以下命令停止 LSF 集群：: # badmin hshutdown all # lsadmin resshutdown all # lsadmin limshutdown all © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection1/Reconfiguring your cluster.html":{"url":"chapter4/section2/subsection1/Reconfiguring your cluster.html","title":"重新配置集群","keywords":"","body":"重新配置集群 更改任何配置文件后，请使用 lsadmin 和 badmin 重新配置群集 步骤 以 root 用户身份登录到每个 LSF 服务器主机。 如果您以 非root 用户身份安装了单用户集群，请以 LSF 主管理员身份登录。 使用以下命令重新配置 LSF 集群： # 重新加载修改的 LSF 配置文件并重新启动 lim： # lsadmin reconfig # 重新加载修改后的 LSF 批处理配置文件： # badmin reconfig # 重新加载修改后的 LSF 批处理配置文件，然后重新启动 mbatchd：: # badmin mbdrestart 此命令还会读取 LSF_LOGDIR/lsb.events文件，因此如果正在运行许多作业，则可能需要一些时间才能完成。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection2/check_LSF_status.html":{"url":"chapter4/section2/subsection2/check_LSF_status.html","title":"检查 LSF 状态","keywords":"","body":"检查 LSF 状态 使用 LSF 管理命令检查集群配置，查看集群状态，以及 LSF 批处理作业负载系统配置和状态。 命令输出示例 本节中显示的 LSF 命令显示了典型输出的示例。 您看到的输出，可能会根据您的配置而有所不同。 这些命令都是简要描述，以便您可以轻松地使用它们，来验证您的 LSF 安装。 有关完整的用法和命令选项，请参见 LSF Command Reference 或 LSF 手册页。 您可以在任何 LSF 主机上使用这些命令。 如果您从这些命令获得正确的输出，则可以使用集群了。 如果您的输出有错误，请参阅 Troubleshooting LSF problems 来获得帮助。 使用 lsadmin 命令检查集群配置 lsadmin 命令控制 LSF 集群的操作，并管理 LSF 守护程序 lim 和 res。 使用 lsid 和 lsload 命令检查集群状态 lsid 命令告诉您是否正确设置了 LSF 环境。 lsload 命令显示集群的当前负载级别。 使用 badmin 检查 LSF 批处理系统配置 badmin 命令控制和监视 LSF 批处理作业负载系统的操作。 使用 bhosts 和 bqueues 查找批处理系统状态 使用 bhosts 命令查看 LSF 批处理作业负载系统是否正常运行。 bqueues 命令显示可用队列的状态及其配置参数。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection2/Check cluster configuration.html":{"url":"chapter4/section2/subsection2/Check cluster configuration.html","title":"检查集群配置","keywords":"","body":"检查集群配置 lsadmin 命令控制 LSF 集群的操作，并管理 LSF 守护程序 lim 和 res。 使用 lsadmin ckconfig 命令检查 LSF 配置文件。 -v 选项显示有关 LSF 配置的详细信息： 以下输出中显示的消息是 lsadmin ckconfig -v 的典型消息。其他消息可能表明您的 LSF 配置有问题。 % lsadmin ckconfig -v Checking configuration files ... EGO 3.6.0 build 800000, Jul 25 2017 Copyright International Business Machines Corp. 1992, 2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. binary type: linux2.6-glibc2.3-x86_64 Reading configuration from /opt/lsf/conf/lsf.conf Aug 3 13:45:27 2017 20884 6 3.6.0 Lim starting... Aug 3 13:45:27 2017 20884 6 3.6.0 LIM is running in advanced workload execution mode. Aug 3 13:45:27 2017 20884 6 3.6.0 Master LIM is not running in EGO_DISABLE_UNRESOLVABLE_HOST mode. Aug 3 13:45:27 2017 20884 5 3.6.0 /opt/lsf/10.1/linux2.6-glibc2.3-x86_64/etc/lim -C Aug 3 13:45:27 2017 20884 7 3.6.0 Could not construct product entitlement version array Aug 3 13:45:27 2017 20884 Last message repeated 1 time(s). Aug 3 13:45:27 2017 20884 6 3.6.0 initEntitlement: EGO_AUDIT_MAX_SIZE was not set. Default value will be used. Aug 3 13:45:27 2017 20884 6 3.6.0 initEntitlement: EGO_AUDIT_MAX_ROTATE was not set. Default value will be used. Aug 3 13:45:27 2017 20884 6 3.6.0 LIM is running as IBM Spectrum LSF Standard Edition. Aug 3 13:45:27 2017 20884 6 3.6.0 reCheckClass: numhosts 1 so reset exchIntvl to 15.00 Aug 3 13:45:27 2017 20884 6 3.6.0 Checking Done. --------------------------------------------------------- No errors found. 参阅 Troubleshooting LSF problems 或者 LSF Command Reference 以获取一些常见配置错误的帮助。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection2/Check cluster status.html":{"url":"chapter4/section2/subsection2/Check cluster status.html","title":"检查集群状态","keywords":"","body":"检查集群状态 lsid 命令告诉您是否正确设置了 LSF 环境。 lsload 命令显示集群的当前负载级别。 lsid 命令 lsid 命令显示集群的当前 LSF 版本号，集群名称和当前 LSF 主主机的主机名。 lsid 命令显示的 LSF 主名称可以有所不同，但通常是 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 Hosts 部分中配置的第一台主机。 % lsid IBM Spectrum LSF Standard 10.1.0.0, Apr 04 2016 Copyright International Business Machines Corp, 1992-2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. My cluster name is cluster1 My master name is hosta 如果看到如下消息 Cannot open lsf.conf file LSF_ENVDIR 环境变量可能未正确设置。 使用 cshrc.lsf 或 profile.lsf 文件来设置您的环境。 有关更多帮助，请参见 Troubleshooting LSF problems 来寻求帮助。 lsload 命令 lsload命令的输出为集群中的每个主机包含一行。 集群中所有主机的正常状态是 ok。 % lsload HOST_NAME status r15s r1m r15m ut pg ls it tmp swp mem hosta ok 0.0 0.0 0.1 1% 0.0 1 224 43G 67G 3G hostc -ok 0.0 0.0 0.0 3% 0.0 3 0 38G 40G 7G hostf busy *6.2 6.9 9.5 85% 1.1 30 0 5G 400G 385G hosth busy 0.1 0.1 0.3 7% *17 6 0 9G 23G 28G hostv unavail 对于任何负载指数超过其配置阈值的主机，将显示繁忙状态。 星号（*）标记了超出其阈值的负载索引，从而导致主机状态为繁忙。 值 o k前面的减号（-）表示 res 未在该主机上运行。 如果在启动或重新配置 LSF 之后看到以下消息之一，请等待几秒钟，然后再次尝试 lsload 命令，以使所有主机上的 lim 守护程序有时间初始化。 lsid: getentitlementinfo() failed: LIM is down; try later 或者 LSF daemon (LIM) not responding ... still trying 如果问题仍然存在，请参阅 Troubleshooting LSF problems 。 其他有用的命令 bparams 命令显示有关 LSF 批处理系统配置参数的信息。 bhist 命令显示有关作业的历史信息。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection2/Check LSF batch system configuration.html":{"url":"chapter4/section2/subsection2/Check LSF batch system configuration.html","title":"检查LSF批处理系统配置","keywords":"","body":"检查 LSF 批处理系统配置 badmin 命令控制和监视 LSF 批处理工作负载系统的操作。 使用 badmin ckconfig 命令检查 LSF 批处理系统配置文件。 -v 选项显示有关配置的详细信息： 以下输出中的消息是 badmin ckconfig -v 的典型消息。 其他消息可能表明您的 LSF 批处理工作负载系统配置存在问题。 % badmin ckconfig -v Checking configuration files ... Dec 20 12:22:55 2015 20246 9 9.1.3 minit: Trying to call LIM to get cluster name ... Dec 20 12:22:55 2015 20246 9 9.1.3 Batch is enabled Dec 20 12:22:55 2015 4433 9 9.1.3 Checking Done --------------------------------------------------------- No errors found. 请参阅 Troubleshooting LSF problems 或者 LSF Command Reference 以获取一些常见配置错误的帮助。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection2/Find out batch system status.html":{"url":"chapter4/section2/subsection2/Find out batch system status.html","title":"找出批处理系统状态","keywords":"","body":"查看批处理系统状态 使用 bhosts 命令查看 LSF 批处理作业负载系统是否正常运行。 bqueues 命令显示可用队列的状态及其配置参数。 要使用 LSF 批处理命令，集群必须已启动并正在运行。 有关启动 LSF 守护程序的信息，请参见 Starting your cluster 。 bhosts 命令 bhosts 命令显示集群中 LSF 批处理服务器主机的状态，以及有关批处理主机的其他详细信息： 单个用户允许的最大作业槽位数 系统中的作业总数，正在运行的作业，用户暂停的作业以及系统暂停的作业 预留作业槽位总数 集群中所有主机的正常状态都显示正常。 % bhosts HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV hosta ok - - 0 0 0 0 0 hostb ok - - 0 0 0 0 0 hostc ok - - 0 0 0 0 0 hostd ok - - 0 0 0 0 0 如果在启动或重新配置 LSF 时看到以下消息，请等待几秒钟，然后再次尝试 bhosts 命令，以使 mbatchd 守护程序有时间进行初始化。 batch system daemon not responding ... still trying 如果问题仍然存在，请参阅 Solving common LSF problems 以寻求帮助。 bqueues 命令 LS F队列组织具有不同优先级，和不同调度策略的作业。 bqueues 命令显示可用队列的状态及其配置参数。 要使队列接受和调度作业，状态必须为 Open:Active。 % bqueues QUEUE_NAME PRIO STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SUSP owners 43 Open:Active - - - - 0 0 0 0 priority 43 Open:Active - - - - 0 0 0 0 night 40 Open:Inact - - - - 0 0 0 0 chkpnt_rerun_qu 40 Open:Active - - - - 0 0 0 0 short 35 Open:Active - - - - 0 0 0 0 license 33 Open:Active - - - - 0 0 0 0 normal 30 Open:Active - - - - 0 0 0 0 idle 20 Open:Active - - - - 0 0 0 0 要查看更多详细的队列信息，请使用 bqueues -l 命令： % bqueues -l normal QUEUE: normal -- For normal low priority jobs, running only if hosts are lightly loaded. This is the default queue. PARAMETERS/STATISTICS PRIO NICE STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SSUSP USUSP RSV 30 20 Open:Active - - - - 0 0 0 0 0 0 Interval for a host to accept two jobs is 0 seconds SCHEDULING PARAMETERS r15s r1m r15m ut pg io ls it tmp swp mem loadSched - - - - - - - - - - - loadStop - - - - - - - - - - - SCHEDULING POLICIES: FAIRSHARE NO_INTERACTIVE USER_SHARES: [default, 1] USERS: all HOSTS: all bqueues -l 命令显示有关队列的以下信息： 什么类型的作业要在队列上运行 资源使用限制 能够使用队列的主机和用户 调度阈值： loadSched 是 LSF 停止自动分派作业的阈值 loadStop 是 LSF 自动挂起作业的阈值 其他有用的命令 bparams 命令显示有关 LSF 批处理系统配置参数的信息。 bhist 命令显示有关作业的历史信息。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection3/run_jobs.html":{"url":"chapter4/section2/subsection3/run_jobs.html","title":"运行作业","keywords":"","body":"运行作业 使用 bsub 和 lsrun 命令通过 LSF 运行作业。 使用 bjobs 命令查看您的作业状态。 使用 bstop，bresume 和 bkill 命令控制作业执行。 使用 bsub 和 lsrun 运行 LSF 作业 使用两个基本命令通过 LSF 运行作业： bsub 将作业提交给 LSF 批处理调度程序。 LSF 根据您在 LSF 队列中配置的调度策略，将作业调度并调度到最佳可用主机。 lsrun 命令根据 lim 守护程序，收集的当前系统负载信息在最佳主机上运行交互式任务。 对于大多数作业，您需要做的就是在通常使用的作业命令之前添加 lsrun 或 bsub 命令。 通常，您不需要修改可执行应用程序或执行脚本。 使用bsub提交批处理作业 bsub 命令将作业提交到 LSF 批处理调度队列。 通过bjobs显示工作状态 使用 bjobs 命令查看作业 ID 和有关您的作业的其他信息。 使用 bstop，bresume 和 bkill 控制作业执行 使用 LSF 命令来暂停（bstop），恢复（bresume）和杀死（bkill）作业。 使用 lsrun 和 lsgrun 运行交互式任务 如果可以找到所需的资源和适当的主机类型，则 lsrun 命令可以在当前本地主机或最佳可用主机上远程运行任务。 lsgrun 命令与 lsrun 命令类似，但是它在一组主机上运行任务。 将您的应用程序与LSF集成 通过将应用程序与 LSF 集成，可以确保用户可以使用正确而完整的作业提交选项来提交和运行其作业，而无需使他们学习 LSF 命令。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection3/Submit batch jobs.html":{"url":"chapter4/section2/subsection3/Submit batch jobs.html","title":"提交批处理作业","keywords":"","body":"提交批处理作业 bsub 命令将作业提交到 LSF 批处理调度队列。 以下命令将 sleep 作业提交到默认队列（normal）： % bsub sleep 60 Job is submitted to default queue . 当作业提交给 LSF 时，将为其分配唯一的作业 ID，在这种情况下为 3616。 您可以在 bsub 命令上指定各种作业选项。 例如，您可以指定一个队列，作业命令 sleep 60 是最后一个选项： % bsub -q short sleep 60 Job is submitted to queue . LSF 对作业输出的作用 默认情况下，当作业完成时，LSF将电子邮件和作业报告以及所有输出和错误消息发送到提交作业的用户帐户。 您可以选择使用 -o 和 -e 选项将标准输出和标准错误保存到文件中。 以下命令，将作业的标准输出和标准错误附加到 user1 家目录下的 job子目录中的 output.3640 和 errors.3640文件中。 % bsub -q short -o /home/user1/job/output.%J -e /home/user1/job/errors.%J ls -l Job is submitted to queue . 创建文件时，％J 变量将替换为作业 ID。 使用 ％J 可以帮助您在运行大量作业时找到作业输出。 使用 bsub -I 的交互式批处理作业 要通过 LSF 提交交互式作业，请使用 -I 选项： 以下命令提交一个批处理交互式作业，该作业显示 ls 命令的输出： % bsub -I ls 要使用伪终端 (pseudo-terminal) 提交批处理交互式作业，请使用 bsub -Ip 选项。 要提交批处理交互式作业,并创建具有 Shell 模式支持的伪终端，请使用 bsub -Is 选项。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection3/Display job status.html":{"url":"chapter4/section2/subsection3/Display job status.html","title":"显示作业状态","keywords":"","body":"显示作业状态 使用 bjobs 命令查看作业 ID 和有关您的作业的其他信息。 每个 LSF 作业的状态都会定期更新。 % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 RUN normal hosta hostb sleep 60 Jun 5 17:39:58 名为 sleep 60 的作业将运行60秒。 作业完成时，LSF 发送电子邮件以报告作业完成情况。 您可以使用作业 ID 监视特定作业的状态。 如果所有主机都忙，则不会立即启动作业，并且 STAT 列显示 PEND。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection3/Control job execution.html":{"url":"chapter4/section2/subsection3/Control job execution.html","title":"控制作业执行","keywords":"","body":"控制作业执行 使用 LSF 命令来暂停（bstop），恢复（bresume）和杀死（bkill）作业。 bstop 命令 要暂停正在运行的作业，请使用 bstop 命令并指定作业 ID： % bstop 1266 Job is being stopped 如果作业在停止时正在运行，则 bjobs 命令显示作业 1266 的 USUSP 状态： % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 USUSP normal hosta hostb sleep 60 Jun 5 17:39:58 作业提交者只能暂停自己的工作。 LSF 管理员可以暂停任何作业。 bresume 命令 要恢复暂停的作业，请使用 bresume 命令。 % bresume 1266 Job is being resumed 如果作业立即恢复，则 bjobs 命令显示作业 1266 的运行状态： % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 RUN normal hosta hostb sleep 60 Jun 5 17:39:58 作业提交者只能恢复自己的工作。 LSF 管理员可以恢复任何作业。. bkill 命令 要取消作业，请使用 bkill 命令，该命令会向指定的作业发送信号。 例如，如果作业所有者或 LSF 管理员运行以下命令，作业 1266 将被杀死： % bkill 1266 Job is being terminated © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection3/Run interactive tasks.html":{"url":"chapter4/section2/subsection3/Run interactive tasks.html","title":"运行交互式任务","keywords":"","body":"运行交互式作业 如果可以找到所需的资源和适当的主机类型，则 lsrun 命令可以在当前本地主机或最佳可用主机上远程运行任务。 lsgrun 命令与 lsrun 命令类似，但是它在一组主机上运行任务。 以下命令运行 ls 命令。 在这种情况下，该命令在本地主机上通过 LSF 运行： % lsrun ls -l /usr/share/lsf/cluster1/conf/ total 742 -rw-r--r-- 1 root lsf 11372 Jul 16 16:23 cshrc.lsf -rw-r--r-- 1 root lsf 365 Oct 25 10:55 hosts drwxr-xr-x 3 lsfadmin lsf 512 Jul 16 15:53 lsbatch -rw-r--r-- 1 lsfadmin lsf 1776 Nov 23 15:13 lsf.conf -rw-r--r-- 1 lsfadmin lsf 8453 Nov 16 17:46 lsf.shared -rw-r--r-- 1 lsfadmin lsf 578 Jul 16 15:53 lsf.task -rw-r--r-- 1 root lsf 10485 Jul 16 17:08 profile.lsf 您还可以指定要在其中运行命令的主机。 例如，以下命令在远程主机 hosta 上运行 hostname 命令： % lsrun -v -m hosta hostname > hosta 以下命令在三个远程主机上运行 hostname 命令： % lsgrun -v -m \"hosta hostb hostc\" hostname > hosta > hostb > hostc © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection3/Integrate your applications with LSF.html":{"url":"chapter4/section2/subsection3/Integrate your applications with LSF.html","title":"将应用程序与 LSF 集成","keywords":"","body":"将您的应用程序与 LSF 集成 通过将应用程序与 LSF 集成，可以确保用户可以使用正确而完整的作业提交选项，来提交和运行其作业，而无需学习 LSF 命令。 通过三种方式将应用程序与 LSF 集成： 包装 shell 脚本 包装二进制可执行文件 修改现有的应用程序源代码和接口 包装 shell 脚本 最简单的集成方法是将 bsub 命令放入可执行文件中，例如 Shell 脚本。 包装脚本是用于通过 LSF 启动应用程序的可执行文件。 它为用户提供了一个简单的界面来运行其作业，该界面易于部署和维护。 例如，如果您的应用程序名为 abc，则将 abc 重命名为 abc_real ，并创建一个名为 abc 的包装脚本： #! /bin/sh bsub -R \"rusage[abc_license=1:duration=1]\" abc_real 用户运行 abc 时，实际上是在运行脚本，以使用 1 个名为 abc_license 的共享资源向 LSF 提交作业 abc_real。 有关使用 bsub 命令的 -R 选项上的资源需求（使用率）字符串，指定共享资源的更多信息，请参阅 Manage software licenses and other shared resources. 通过向脚本添加适当的选项，可以增强集成： 根据许可证的可用性重新排队 在执行主机上的本地目录中，复制输入和输出文件 计算和估算资源需求 包装二进制程序 包装二进制文件类似于已编译二进制可执行文件形式的包装 shell 脚本。 编译的包装文件通常比 shell 脚本运行得更快，更高效，并且它们还可以访问 LSF API（LSLIB 和 LSBLIB）。 二进制代码也更加安全，因为用户无法在没有源代码和适当的库的情况下对其进行修改，但是开发包装器二进制程序比包装 shell 脚本要花费更多的时间。 修改现有的应用程序源代码和接口 LSF 已经与许多常用软件产品紧密集成。 IBM 和其他软件应用程序供应商提供了用于更紧密地集成 LSF 和其他应用程序的设施和服务。 通过修改现有的应用程序用户界面，您可以轻松完成作业提交，许可证最大化，并行执行以及其他高级 LSF 功能。 在某些情况下，您可以直接从应用程序用户界面运行 LSF 作业。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection4/manage_users_hosts_and_queues.html":{"url":"chapter4/section2/subsection4/manage_users_hosts_and_queues.html","title":"管理用户、节点与队列","keywords":"","body":"管理用户、节点与队列 通过 cshrc.lsf 和 profile.lsf 使集群对用户可用。 从集群中添加或删除主机和队列。 使用 cshrc.lsf 和 profile.lsf 为用户提供集群 确保所有 LSF 用户在其自己的 .cshrc 或 .profile 文件末尾都包含 cshrc.lsf 或 profile.lsf 文件，或者在使用 LSF 之前运行这两个文件之一。 将主机添加到集群中 使用 LSF 安装脚本 lsfinstall 将新主机和主机类型添加到您的集群中。 从集群中删除主机 从 LSF 中删除主机，包括关闭主机以防止主机上运行任何其他作业，以及从 lsf.cluster.cluster_name 文件和其他配置文件中删除对该主机的引用。 添加队列 编辑 lsb.queues 文件以添加新的队列定义。 添加队列不会影响挂起或正在运行的作业。 删除队列 编辑 lsb.queues 以除去队列定义。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection4/Making your cluster available to users.html":{"url":"chapter4/section2/subsection4/Making your cluster available to users.html","title":"使您的集群可供用户使用","keywords":"","body":"使您的集群可供用户使用 确保所有 LSF 用户在其自己的 .cshrc 或 .profile 文件末尾都包含 cshrc.lsf 或 profile.lsf 文件，或者在使用 LSF 之前运行这两个文件之一。 任务说明 要为您的用户设置 LSF 环境，请使用以下两个 Shell 文件： LSF_CONFDIR/cshrc.lsf 将此文件用于 csh 或 tcsh shell。 LSF_CONFDIR/profile.lsf 将此文件用于 sh, ksh, 或 bash shell. 步骤 对于 csh 或 tcsh shell： 为所有用户将 cshrc.lsf 文件添加到 .cshrc 文件的末尾： 将 cshrc.lsf 文件的内容复制到 .cshrc 文件中。 在 .cshrc 文件的末尾添加带有 source 命令的行： 例如，如果集群的 LSF_TOP 目录为 /usr/share/lsf/conf，则将以下行添加到 .cshrc 文件中： source /usr/share/lsf/conf/cshrc.lsf 对于 sh, ksh, 或 bash shell: 为所有用户将 profile.lsf 文件添加到 .profile 文件的末尾： 将 profile.lsf 文件的内容复制到 .profile 文件中。 例如，如果集群的 LSF_TOP 目录为 /usr/share/lsf/conf，则在 .profile 文件的末尾添加类似于以下内容的行： . /usr/share/lsf/conf/profile.lsf © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection4/Adding a host to your cluster.html":{"url":"chapter4/section2/subsection4/Adding a host to your cluster.html","title":"将主机节点添加到集群","keywords":"","body":"将主机添加到集群 使用 LSF 安装脚本 lsfinstall 将新主机和主机类型添加到您的集群中。 开始之前 确保您具有要添加的主机类型的 LSF distribution 文件。 例如，要向集群添加运行 x86-64 内核 2.6 和 3.x 的 Linux系统，请获取文件 lsf10.1_linux2.6-glibc2.3-x86_64.tar.Z。 可通过 IBM Passport Advantage 下载所有受支持的 LSF 版本的分发软件包。 有关受支持的操作系统的完整列表，请参阅 IBM developerWorks 上的 LSF System Requirements 。 以下视频提供了有关通过IBM Passport Advantage下载LSF的更多帮助： YouTube IBM Education Assistant 任务说明 将主机添加到集群，有以下主要步骤： 安装主机类型的LSF二进制文件。 将主机信息添加到 lsf.cluster.cluster_name 文件。 设置新主机。 步骤 为新的主机类型安装二进制文件。 使用 lsfinstall 命令将新的主机类型添加到您的集群中。 如果您已经具有要添加的主机类型的发布文件，则可以跳过这些步骤。 以 root 用户身份登录到可以访问 LSF 安装脚本目录的任何主机。 转到安装脚本目录。 # cd /usr/share/lsf/cluster1/10.1/install 编辑 install.config 文件以指定要用于新主机类型的选项。 有关 install.config 文件的更多信息，请参见 IBM Spectrum LSF Configuration Reference. 有关 lsfinstall 命令的信息, 请参阅 Installing IBM Spectrum LSF on UNIX and Linux 以及 IBM Spectrum LSF Command Reference. 运行 ./lsfinstall -f install.config 命令。 请遵循 Installing IBM Spectrum LSF on UNIX and Linux 中 After Installing LSF 中的主机设置步骤。(或在由 lsfinstall 脚本生成的 lsf_getting_started.html 文件中) 设置新主机。 将主机信息添加到 lsf.cluster.cluster_name 文件。 以主要 LSF 管理员身份登录到 LSF 主节点。 编辑 LSF_CONFDIR/lsf.cluster.cluster_name 文件，并将新主机的主机信息添加到 “Host” 部分。 添加主机名。 添加型号或类型。 如果在 “model” 和 “type” 列中输入关键字 ！，主机上的 lim 会自动检测到主机模型。 您可能现在想使用该主机类型的默认值，并在以后有更多经验或更多信息时更改它们。 在服务器列中指定 LSF 服务器或客户端： 1（one）表示 LSF 服务器主机。 0 (zero) 指示仅 LSF 客户端主机。 默认情况下，所有主机都被视为 LSF 服务器主机。 HOSTNAME model type server r1m mem RESOURCES REXPRI hosta ! SUNSOL 1 1.0 4 () 0 hostb ! LINUX 0 1.0 4 () 0 hostc ! HPPA 1 1.0 4 () 0 End Host 将更改保存到 LSF_CONFDIR/lsf.cluster.cluster_name。 重新配置 lim 以启用集群中的新主机。 % lsadmin reconfig Checking configuration files ... No errors found. Do you really want to restart LIMs on all hosts? [y/n] y Restart LIM on ...... done Restart LIM on ...... done Restart LIM on ...... done lsadmin reconfig 命令检查配置错误。 如果未找到无法恢复的错误，将要求您确认要在所有主机上重新启动lim，并重新配置 lim。 如果发现不可恢复的错误，则重新配置退出。 重新配置 mbatchd. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated badmin reconfig 命令检查配置错误。 如果未找到不可恢复的错误，则要求您确认重新配置。 如果发现不可恢复的错误，则重新配置退出。 （可选）使用 hostsetup 命令设置新主机。 以 root 用户身份登录到可以访问 LSF 安装脚本目录的任何主机。 转到安装脚本目录。 # cd /usr/share/lsf/cluster1/10.1/install 运行 hostsetup 命令以设置新主机。 # ./hostsetup --top=\"/usr/share/lsf/lsf_62\" --boot=\"y\" 有关 hostsetup 命令的信息，请参阅 [Installing IBM Spectrum LSF on UNIX and Linux 以及 IBM Spectrum LSF Command Reference. 在新主机上启动 LSF。 运行以下命令： # lsadmin limstartup # lsadmin resstartup # badmin hstartup 运行 bhosts 和 lshosts 命令以验证您的更改。 如果任何主机类型或主机模型是UNKNOWN或DEFAULT，请参阅 IBM Spectrum LSF Cluster Management and Operations 中的 Working with hosts 来解决此问题。 结论 使用动态主机配置将主机添加到集群，而无需手动更改 LSF 配置。 有关动态添加主机的更多信息，请参阅 IBM Spectrum LSF Cluster Management and Operations。 如果遇到错误，请参见 Troubleshooting LSF problems ，以获取有关一些常见配置错误的帮助。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection4/Removing a host from your cluster.html":{"url":"chapter4/section2/subsection4/Removing a host from your cluster.html","title":"从集群中移除主机节点","keywords":"","body":"从集群中删除主机 从 LSF 中删除主机，包括关闭主机以防止主机上运行任何其他作业，以及从 lsf.cluster.cluster_name 文件和其他配置文件中删除对该主机的引用。 任务说明 注意 切勿从 LSF 中删除 master 主控节点。 如果要更改当前的默认主节点，请更改 lsf.cluster.cluster_name 文件以分配其他默认主控主机。 然后删除以前是主控节点的主机。 步骤 以 root 用户身份登录到 LSF 主机。 运行 badmin hclose 关闭主机。 关闭主机可防止将作业分派到主机，并允许正在运行的作业完成。 手动停止所有正在运行的守护程序。 在 LSF_CONFDIR/lsf.cluster.cluster_name 文件的 “Host” 部分中删除对主机的所有引用。 从以下配置文件中，删除对主机的任何其他引用（如果适用）： LSF_CONFDIR/lsf.shared LSB_CONFDIR/cluster_name/configdir/lsb.hosts LSB_CONFDIR/cluster_name/configdir/lsb.queues LSB_CONFDIR/cluster_name/configdir/lsb.resources 注销要删除的主机，然后以 root 或 LSF 主管理员身份，登录到集群中的任何其他主机。 运行 lsadmin reconfig 命令以重新配置 LIM。 % lsadmin reconfig Checking configuration files ... No errors found. Do you really want to restart LIMs on all hosts? [y/n] y Restart LIM on ...... done Restart LIM on ...... done lsadmin reconfig 命令检查配置错误。 如果未找到错误，将要求您确认要在所有主机上重新启动 lim，并且已重新配置 lim。 如果发现不可恢复的错误，则重新配置退出。 运行 badmin mbdrestart 命令来重启 mbatchd. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated badmin mbdrestart 命令检查配置错误。 如果未找到不可恢复的错误，则要求您确认重新配置。 如果发现不可恢复的错误，则重新配置退出。 如果您将 LSF 守护程序，配置为在系统启动时自动启动，请从主机的系统启动文件中删除 LSF 部分。 有关自动 LSF 守护程序启动的更多信息, 请参阅 Setting up automatic LSF startup 如果主机的任何用户使用 lstcsh shell 作为其登录 shell，请将其登录 shell 更改为 tcsh 或 csh。 从 /etc/shells 文件中删除 lstcsh。 结论 使用动态主机配置将主机从集群中删除，而无需手动更改 LSF 配置。 有关动态删除主机的更多信息, 请参阅 IBM Platform LSF Cluster Management and Operations. 如果遇到错误，请参阅 Troubleshooting LSF problems 帮助解决一些常见的配置错误。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection4/Adding a queue.html":{"url":"chapter4/section2/subsection4/Adding a queue.html","title":"添加队列","keywords":"","body":"添加队列 编辑 lsb.queues 文件以添加新的队列定义。 添加队列不会影响挂起或正在运行的作业。 步骤 以管理员身份，登录到集群中的任何主机上。 编辑 LSB_CONFDIR/cluster_name/configdir/lsb.queues 文件以添加新的队列定义。 您可以从该文件复制另一个队列定义作为起点。 切记更改已复制队列的 QUEUE_NAME 参数。 将更改保存到 lsb.queues 文件。 准备好配置文件后，运行 badmin ckconfig 命令以检查新的队列定义。 如果报告任何错误，请解决此问题，然后再次检查配置。 运行 badmin reconfig 命令以重新配置集群。 % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated badmin reconfig 命令还检查配置错误。 如果未找到不可恢复的错误，则要求您确认重新配置。 如果发现不可恢复的错误，则重新配置退出。 结论 如果遇到错误，请参阅 Troubleshooting LSF problems 以获取有关一些常见配置错误的帮助。 有关 lsb.queues 文件的更多信息，请参见 Configuration Reference。 有关 badmin reconfig 命令的更多信息, 请参见 Command Reference. 示例 Begin Queue QUEUE_NAME = normal PRIORITY = 30 STACKLIMIT= 2048 DESCRIPTION = For normal low priority jobs, running only if hosts are lightly loaded. QJOB_LIMIT = 60 # job limit of the queue PJOB_LIMIT = 2 # job limit per processor ut = 0.2 io = 50/240 USERS = all HOSTS = all NICE = 20 End Queue © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection4/Removing a queue.html":{"url":"chapter4/section2/subsection4/Removing a queue.html","title":"移除队列","keywords":"","body":"移除队列 编辑 lsb.queues 以移除队列定义。 开始之前 重要 删除队列之前，请确保该队列中没有正在运行的作业。 使用 bqueues 命令，可查看现有队列列表以及在这些队列中运行的作业。如果作业在要删除的队列中，则必须将挂起的，和正在运行的作业切换到另一个队列，然后删除该队列。 如果删除其中有待处理作业的队列，则作业将暂时移至 lost_and_found 队列。 作业状态不变。 正在运行的作业将继续，原始队列中暂挂的作业将在 lost_and_found 队列中暂挂。 作业保持待处理状态，直到用户或队列管理员使用 bswitch 命令，将作业切换为常规队列为止。 其他队列中的作业不受影响。 步骤 以主要管理员身份登录到集群中的任何主机上。 关闭队列以防止提交任何新作业。 badmin qclose night Queue night is closed 将所有暂挂和正在运行的作业切换到另一个队列。 例如，bswitch -q night idle 0 命令从 night 队列到 idle 队列中选择作业。 作业ID号 0 切换所有作业。 bjobs -u all -q night JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 5308 user5 RUN night hostA hostD job5 Nov 21 18:16 5310 user5 PEND night hostA hostC job10 Nov 21 18:17 bswitch -q night idle 0 Job is switched to queue Job is switched to queue 编辑 LSB_CONFDIR/cluster_name/configdir/lsb.queues 文件，然后删除或注释掉要删除的队列的定义。 将更改保存到 lsb.queues 文件。 运行 badmin reconfig 命令以重新配置集群。 % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated badmin reconfig 命令检查配置错误。 如果未找到不可恢复的错误，则要求您确认重新配置。 如果发现不可恢复的错误，则重新配置退出。 结论 如果遇到错误，请参见 Troubleshooting LSF problems 以获取有关一些常见配置错误的帮助。 有关 lsb.queues 文件的更多信息，请参见 Configuration Reference. 有关 badmin reconfig 命令的更多信息，请参见 Command Reference. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection5/configure_LSF_startup.html":{"url":"chapter4/section2/subsection5/configure_LSF_startup.html","title":"配置 LSF 启动","keywords":"","body":"配置 LSF 启动 使用 lsf.sudoers 文件，以便 LSF 管理员可以启动和停止 LSF 守护程序。 将 LSF 设置为自动启动。 允许 LSF 管理员使用 lsf.sudoers 启动 LSF 守护程序 要允许 LSF 管理员启动和停止 LSF 守护程序，请配置 /etc/lsf.sudoers 文件。 如果 lsf.sudoers 文件不存在，则只有 root 可以启动和停止 LSF 守护程序。 设置 LSF 自动启动 将 LSF 守护程序，配置为在集群中的每个 LSF 服务器主机上自动启动。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter4/section2/subsection5/Allowing LSF administrators to start LSF daemons.html":{"url":"chapter4/section2/subsection5/Allowing LSF administrators to start LSF daemons.html","title":"允许 LSF 管理员启动 LSF 守护程序","keywords":"","body":"允许 LSF 管理员启动 LSF 守护程序 要允许 LSF 管理员启动和停止 LSF 守护程序，请配置 /etc/lsf.sudoers 文件。 如果 lsf.sudoers 文件不存在，则只有 root 可以启动和停止 LSF 守护程序。 任务说明 使用 lsf.sudoers 文件要求您启用 setuid bit。 因为这允许 LSF 管理命令以 root 特权运行，所以如果您不希望这些命令以 root 特权运行，请不要继续。 步骤 以 root 用户身份登录到每个 LSF 服务器主机。 从 LSF 主节点开始，然后在所有 LSF 主机上重复这些步骤。 在每个 LSF 主机上创建一个 /etc/lsf.sudoers 文件，并指定 LSF_STARTUP_USERS 和 LSF_STARTUP_PATH 参数。 LSF_STARTUP_USERS=\"lsfadmin user1\" LSF_STARTUP_PATH=/usr/share/lsf/cluster1/10.1/sparc-sol2/etc LSF_STARTUP_PATH 通常是 LSF_SERVERDIR 目录的路径，其中有 LSF 服务器二进制文件（lim，res，sbatchd，mbatchd，mbschd ，等等），如 LSF_CONFDIR/lsf.conf 文件中所定义。 lsf.sudoers 文件必须具有文件许可权模式 -rw -------（600），并且只能由 root 读取和写入： # ls -la /etc/lsf.sudoers -rw------- 1 root lsf 95 Nov 22 13:57 lsf.sudoers 运行 lsfrestart 命令以重新启动集群： # lsfrestart © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection5/Setting up automatic LSF startup.html":{"url":"chapter4/section2/subsection5/Setting up automatic LSF startup.html","title":"设置 LSF 自动启动","keywords":"","body":"设置 LSF 自动启动 将 LSF 守护程序，配置为在集群中的每个 LSF 服务器主机上自动启动。 步骤 使用 hostsetup 命令的 boot=y 选项。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter4/section2/subsection6/manage_software_licenses_and_other_resources.html":{"url":"chapter4/section2/subsection6/manage_software_licenses_and_other_resources.html","title":"管理软件许可证及其他共享资源","keywords":"","body":"管理软件许可证及其他共享资源 设置LSF外部 LIM（ELIM），以将软件许可证，作为动态共享资源进行监视。 LSF 如何使用动态共享资源 LSF 识别两种主要类型的资源： 基于主机的资源在集群中的所有主机上均可用，例如，主机类型和型号或节点锁定的软件许可证。 共享资源作为动态负载索引进行管理，可用于集群中的一组主机，例如，网络浮动软件许可证，共享文件系统。 共享的资源由一组 LSF 主机共享。 LSF 管理用于主机选择，以及批处理或交互式作业执行的共享资源。 这些资源是动态资源，因为系统上的负载随资源的可用性而变化。 软件许可作为共享资源 共享资源最常见的应用程序，是管理软件应用程序许可证。 您提交需要这些许可证的作业，并且在许可证可用时，LSF 会根据其优先级运行作业。 如果许可证不可用，则 LSF 将作业排队，然后在许可证免费时将其分派。 将应用程序许可证配置为共享资源，可确保最佳地使用昂贵的关键资源。 在 ELIM 中定义动态共享资源 为了使 LSF 使用共享资源（如软件许可证），必须在 lsf.shared 文件的 “Resource” 部分中定义资源。 您可以定义资源的类型，以及 LSF 刷新资源值的频率。 为了使 LSF 能够随着时间正确跟踪资源，必须将它们定义为外部负载索引。 LSF 使用称为外部负载信息管理器（ELIM）的程序定期更新负载索引。 ELIM 可以是 Shell 脚本或编译的二进制程序，它们返回您定义的共享资源的值。 ELIM 必须命名为 elim，并且位于 LSF_SERVERDIR 目录中： /usr/share/lsf/lsf/cluser1/10.1/sparc-sol2/etc/elim 您可以在 misc/examples 目录中找到示例 ELIM 的示例。 共享许可证示例 在 lsf.shared 文件中，为软件许可证定义两个动态共享资源，名为 license1 和 license2： Begin Resource RESOURCENAME TYPE INTERVAL INCREASING RELEASE DESCRIPTION # Keywords license1 Numeric 30 N Y (license1 resource) license2 Numeric 30 N Y (license2 resource) End Resource 共享资源的 TYPE 参数可以是以下类型之一： Numeric Boolean String 在这种情况下，资源是(数字型）Numeric. INTERVAL 参数指定您希望刷新值的频率。 在此示例中，ELIM 每 30 秒更新一次共享资源 license1 和 license2 的值。 INCREASING 列中的 N 表示许可证资源正在减少； 也就是说，随着更多许可证的可用，负载会降低。 RELEASE 列中的 Y 表示当使用许可证的作业被挂起时，许可证资源被释放。 将动态共享资源映射到主机 要使 LSF 知道所定义的动态共享资源 license1 和 license2 的位置，请将它们映射到它们所在的主机。 在 LSF_CONFDIR/lsf.cluster.cluster_name 文件中，配置 ResourceMap 部分以指定您在 LSF_CONFDIR/lsf.shared 文件中定义的共享资源 license1 和 license2 之间的映射，以及您要将它们映射到的主机： Begin ResourceMap RESOURCENAME LOCATION license1 [all] license1 [all] End ResourceMap 在此资源映射中，LOCATION 参数下的 [all] 属性意味着 RESOURCENAME 参数下的资源 license1 和 license2 在集群中的所有主机上均可用。主节点上仅需要运行一个 ELIM，因为这两个资源对于所有主机而言都是相同的。 如果资源在不同主机上的位置不同，则必须在每个主机上运行不同的 ELIM。 监控动态共享资源 为了使 LSF 正确接收外部负载索引，ELIM 必须以以下格式，将可用资源的计数发送到标准输出： number_indexes [index_name index_value] ... 本示例中的字段包含以下信息： 2 license1 3 license2 2 外部负载指数总数 (2) 第一个外部负载索引的名称 (license1) 第一个负荷指数的值 (3) 第二个外部负载索引的名称 (license2) 第二个负荷指数 (2) 编写 ELIM 程序 ELIM 必须是位于 LSF_SERVERDIR 目录中的名为 elim 的可执行程序。 启动或重新启动 lim 守护程序时，它将在同一主机上运行 elim 程序，并获取 elim 程序发送的外部负载索引的标准输出。 通常，您可以将任何可量化资源，定义为外部负载索引，编写 ELIM 报告其值，然后将其用作 LSF 资源。 以下示例 ELIM 程序使用 license1 和 license2，并假定 FLEXlm 许可证服务器控制它们： #!/bin/sh NUMLIC=2 # number of dynamic shared resources while true do TMPLICS='/usr/share/lsf/cluster1/10.1/sparc-sol2/etc/lic -c /usr/share/lsf/cluster1/conf/license.dat -f license1, license2' LICS='echo $TMPLICS | sed -e s/-/_/g' echo $NUMLIC $LICS # $NUMLIC is number of dynamic shared resource sleep 30 # Resource done 在脚本中，sed 命令将许可证功能名称中的减号（-）更改为下划线（_），因为 LSF 使用减号进行计算，并且资源名称中不允许使用该减号。 lic 实用程序可从 IBM 支持获得。 您也可以使用 FLEXlm 命令 lmstat 来制作自己的 ELIM。 使用动态共享资源 要在集群中启用新的共享资源，请使用以下命令重新启动 LSF： lsadmin reconfig badmin reconfig 如果未发现错误，请使用 lsload -l 命令来验证动态共享资源的值： HOST_NAME status r15s r1m r15m ut pg io ls it tmp swp mem license1 license2 hosta ok 0.1 0.3 0.4 8% 0.2 50 73 0 62M 700M 425M 3 0 hostb ok 0.1 0.1 0.4 4% 5.7 3 3 0 79M 204M 64M 3 0 提交使用共享资源的作业 要提交使用一个 license1 资源的批处理作业，请使用以下命令： % bsub -R 'rusage[license1=1:duration=1]' myjob 在资源需求（使用率）字符串中，duration=1 表示将 license1 保留 1 分钟，以使 LSF 有时间从 FLEXlm 中检出它。 您还可以在队列的 RES_REQ 参数中，在队列级别指定资源需求字符串。 在 LSB_CONFDIR/cluster_name/configdir/lsb.queues 文件中，指定以下资源需求字符串： Begin Queue QUEUE_NAME = license1 RES_REQ=rusage[license1=1:duration=1] ... End Queue 然后，使用以下命令提交使用一个 license1 资源的批处理作业： % bsub -q license1 myjob 当许可证可用时，LSF 会立即运行您的工作；当所有许可证都被使用时，LSF 会将您的作业排入队列，并在许可证可用时将其分派。 这样，您的所有许可证都将得到最大利用。 更多信息 有关 lsf.shared 和 lsf.cluster.cluster_name 文件以及用于配置共享资源的参数的更多信息, 请看 Configuration Reference. 有关向集群添加外部资源以及配置 ELIM 以自定义资源的更多信息，请参阅 Administering IBM Spectrum LSF 中的 External load indices 。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter4/section3/troubleshooting_LSF_problems.html":{"url":"chapter4/section3/troubleshooting_LSF_problems.html","title":"4.3 LSF 排错","keywords":"","body":"4.3 LSF 排错 解决常见的 LSF 问题并了解 LSF 错误消息。 如果在这里找不到解决问题的方法，请联系 IBM 支持。 解决常见的 LSF 问题 大多数问题是由于错误的安装或配置引起的。 在开始对 LSF 问题进行故障排除之前，请始终先检查错误日志文件。 日志消息通常直接指出问题所在。 LSF 错误消息 以下错误消息由 LSF 守护程序记录，或由 lsadmin ckconfig 和 badmin ckconfig 命令显示。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter4/section3/solving_common_LSF_problems.html":{"url":"chapter4/section3/solving_common_LSF_problems.html","title":"常见 LSF 问题","keywords":"","body":"常见 LSF 问题 大多数问题是由于错误的安装或配置引起的。 在开始对 LSF 问题进行故障排除之前，请始终先检查错误日志文件。 日志消息通常直接指出问题所在。 查找 LSF 错误日志 When something goes wrong, LSF server daemons log error messages in the LSF log directory (specified by the LSF_LOGDIR parameter in the lsf.conf file). 步骤 Make sure that the primary LSF administrator owns LSF_LOGDIR, and that root can write to this directory. If an LSF server is unable to write to LSF_LOGDIR, then the error logs are created in /tmp. LSF logs errors to the following files: lim.log.host_name res.log.host_name pim.log.host_name mbatchd.log.master_host mbschd.log.master_host sbatchd.log.host_name vemkd.log.master_host If these log files contain any error messages that you do not understand, contact IBM Support. 诊断和修复大多数 LSF 问题 General troubleshooting steps for most LSF problems. 步骤 Run the lsadmin ckconfig -v command and note any errors that are shown in the command output. Look for the error in one of the problems described in this section. If none of these troubleshooting steps applies to your situation, contact IBM Support. Use the following commands to restart the LSF cluster: # lsadmin limrestart all # lsadmin resrestart all # badmin hrestart all Run the ps -ef command to see whether the LSF daemons are running. Look for the processes similar to the following command output: root 17426 1 0 13:30:40 ? 0:00 /opt/lsf/cluster1/10.1/sparc-sol10/etc/lim root 17436 1 0 13:31:11 ? 0:00 /opt/lsf/cluster1/10.1/sparc-sol10/etc/sbatchd root 17429 1 0 13:30:56 ? 0:00 /opt/lsf/cluster1/10.1/sparc-sol10/etc/res Check the LSF error logs on the first few hosts that are listed in the Host section of the LSF_CONFDIR/lsf.cluster.cluster_name file. If the LSF_MASTER_LIST parameter is defined in the LSF_CONFDIR/lsf.conf file, check the error logs on the hosts that are listed in this parameter instead. 无法打开 lsf.conf 文件 You might see this message when you run the lsid file. The message usually means that the LSF_CONFDIR/lsf.conf file is not accessible to LSF. 任务说明 By default, LSF checks the directory that is defined by the LSF_ENVDIR parameter for the lsf.conf file. If the lsf.conf file is not in LSF_ENVDIR, LSF looks for it in the /etc directory. For more information, see Setting up the LSF environment with cshrc.lsf and profile.lsf. 步骤 Make sure that a symbolic link exists from /etc/lsf.conf to lsf.conf Use the csrhc.lsf or profile.lsf script to set up your LSF environment. Make sure that the cshrc.lsf or profile.lsf script is available for users to set the LSF environment variables. LIM 无响应地挂掉 When the LSF LIM daemon exits unexpectedly, check for errors in the LIM configuration files. 步骤 Run the following commands: lsadmin ckconfig -v This command displays most configuration errors. If the command does not report any errors, check in the LIM error log. LIM 通信超时 Sometimes the LIM is up, but running the lsload command prints the following error message:Communication time out. 任务说明 If the LIM just started, LIM needs time to get initialized by reading configuration files and contacting other LIMs. If the LIM does not become available within one or two minutes, check the LIM error log for the host you are working on. To prevent communication timeouts when the local LIM is starting or restarting, define the parameter LSF_SERVER_HOSTS in the lsf.conf file. The client contacts the LIM on one of the LSF_SERVER_HOSTS and runs the command. At least one of the hosts that are defined in the list must have a LIM that is up and running. When the local LIM is running but the cluster has no master, LSF applications display the following message: Cannot locate master LIM now, try later. 步骤 Check the LIM error logs on the first few hosts that are listed in the Host section of the lsf.cluster.cluster_name file. If the LSF_MASTER_LIST parameter is defined in the lsf.conf file, check the LIM error logs on the hosts that are listed in this parameter instead. 主 LIM 挂掉 Sometimes the master LIM is up, but running the lsload or lshosts command displays the following error message: Master LIM is down; try later. 任务说明 If the /etc/hosts file on the host where the master LIM is running is configured with the host name that is assigned to the loopback IP address (127.0.0.1), LSF client LIMs cannot contact the master LIM. When the master LIM starts up, it sets its official host name and IP address to the loopback address. Any client requests get the master LIM address as 127.0.0.1, and try to connect to it, and in fact tries to access itself. 步骤 Check the IP configuration of your master LIM in /etc/hosts. The following example incorrectly sets the master LIM IP address to the loopback address: 127.0.0.1 localhost myhostname The following example correctly sets the master LIM IP address: 127.0.0.1 localhost 192.168.123.123 myhostname For a master LIM running on a host that uses an IPv6 address, the loopback address is ::1 The following example correctly sets the master LIM IP address by using an IPv6 address: ::1 localhost ipv6-localhost ipv6-loopback fe00::0 ipv6-localnet ff00::0 ipv6-mcastprefix ff02::1 ipv6-allnodes ff02::2 ipv6-allrouters ff02::3 ipv6-allhosts 用户权限被拒绝 If the remote host cannot securely determine the user ID of the user that is requesting remote execution, remote execution fails with the following error message: User permission denied.. 步骤 Check the RES error log on the remote host for more detailed error message. If you do not want to configure an identification daemon (LSF_AUTH in lsf.conf), all applications that do remote executions must be owned by root with the setuid bit set. Run the following command: chmod 4755 filename If the application binary files are on an NFS-mounted file system, make sure that the file system is not mounted with the nosuid flag. If you are using an identification daemon (the LSF_AUTH parameter in the lsf.conf file), the inetd daemon must be configured. The identification daemon must not be run directly. Inconsistent host names in a name server with /etc/hosts and /etc/hosts.equiv can also cause this problem. If the LSF_USE_HOSTEQUIV parameter is defined in the lsf.conf file, check that the /etc/hosts.equiv file or the HOME/.rhosts file on the destination host has the client host name in it. For Windows hosts, users must register and update their Windows passwords by using the lspasswd command. Passwords must be 3 characters or longer, and 31 characters or less. For Windows password authentication in a non-shared file system environment, you must define the parameter LSF_MASTER_LIST in the lsf.conf file so that jobs run with correct permissions. If you do not define this parameter, LSF assumes that the cluster uses a shared file system environment. 由于文件名空间不一致，远程执行失败 A non-uniform file name space might cause a command to fail with the following error message: chdir(...) failed: no such file or directory. 任务说明 You are trying to run a command remotely, but either your current working directory does not exist on the remote host, or your current working directory is mapped to a different name on the remote host. If your current working directory does not exist on a remote host, do not run commands remotely on that host. 步骤 If the directory exists, but is mapped to a different name on the remote host, you must create symbolic links to make them consistent. LSF can resolve most, but not all, problems by using automount. The automount maps must be managed through NIS. Contact IBM Support if you are running automount and LSF is not able to locate directories on remote hosts. 批处理守护程序无响应挂掉 When the LSF batch daemons sbatchd and mbatchd exit unexpectedly, check for errors in the configuration files. 任务说明 If the mbatchd daemon is running but the sbatchd daemon dies on some hosts, it might be because mbatchd is not configured to use those hosts. 步骤 Check the sbatchd and mbatchd daemon error logs. Run the badmin ckconfig command to check the configuration. Check for email in the LSF administrator mailbox. sbatchd 启动，但是 mbatchd 没有启动 When the sbatchd daemon starts but the mbatchd daemon is not running, it is possible that mbatchd is temporarily unavailable because the master LIM is temporarily unknown. The following error message is displayed: sbatchd: unknown service. 步骤 Run the lsid command to check whether LIM is running. If LIM is not running properly, follow the steps in the following topics to fix LIM problems: LIM dies quietly LIM communication times out Master LIM is down Check whether services are registered properly. 避免孤立的作业流程 LSF uses process groups to track all the processes of a job. However, if the application forks a child, the child becomes a new process group. The parent dies immediately, and the child process group is orphaned from the parent process, and cannot be tracked. 任务说明 For more information about process tracking with Linux cgroups, see Memory and swap limit enforcement based on Linux cgroup memory subsystem. 步骤 When a job is started, the application runs under the job RES or root process group. If an application creates a new process group, and its parent process ID (PPID) still belongs to the job, PIM can track this new process group as part of the job. The only reliable way to not lose track of a process is to prevent it from using a new process group. Any process that daemonizes itself is lost when child processes are orphaned from the parent process group because it changes its process group right after it is detached. LSF 未使用主机 The mbatchd daemon allows the sbatchd daemon to run only on the hosts that are listed in the Host section of the lsb.hosts file. If you configure an unknown host in the following configurations, mbatchd logs an error message: HostGroup or HostPartition sections of the lsb.hosts file, or as a HOSTS definition for a queue in the lsb.queues file. 任务说明 If you try to configure a host that is not listed in the Host section of the lsb.hosts file, the mbatchd daemon logs the following message. mbatchd on host: LSB_CONFDIR/cluster1/configdir/file(line #): Host hostname is not used by lsbatch; ignored If you start the sbatchd daemon on a host that is not known by the mbatchd daemon, mbatchd rejects the sbatchd. The sbatchd daemon logs the following message and exits. This host is not used by lsbatch system. 步骤 Add the unknown host to the list of hosts in the Host section of the lsb.hosts file. Start the LSF daemons on the new host. Run the following commands to reconfigure the cluster: lsadmin reconfig badmin reconfig 未知的主机类型或型号 A model or type UNKNOWN indicates that the host is down or the LIM on the host is down. You need to take immediate action to restart LIM on the UNKNOWN host. 步骤 Start the host. Run the lshosts command to see which host has the UNKNOWN host type or model. lshosts HOST_NAME type model cpuf ncpus maxmem maxswp server RESOURCES hostA UNKNOWN Ultra2 20.2 2 256M 710M Yes () Run the lsadmin limstartup command to start LIM on the host. lsadmin limstartup hostA Starting up LIM on .... done If EGO is enabled in the LSF cluster, you can run the following command instead: egosh ego start lim hostA Starting up LIM on .... done You can specify more than one host name to start LIM on multiple hosts. If you do not specify a host name, LIM is started on the host from which the command is submitted. To start LIM remotely on UNIX or Linux, you must be root or listed in the lsf.sudoers file (or the ego.sudoers file if EGO is enabled in the LSF cluster). You must be able to run the rsh command across all hosts without entering a password. Wait a few seconds, then run the lshosts command again. The lshosts command displays a specific model or type for the host or DEFAULT. If you see DEFAULT, it means that automatic detection of host type or model failed, and the host type that is configured in the lsf.shared file cannot be found. LSF works on the host, but a DEFAULT model might be inefficient because of incorrect CPU factors. A DEFAULT type might also cause binary incompatibility because a job from a DEFAULT host type can be migrated to another DEFAULT host type. 默认主机类型或型号 If you see DEFAULT in lim -t, it means that automatic detection of host type or model failed, and the host type that is configured in the lsf.shared file cannot be found. LSF works on the host, but a DEFAULT model might be inefficient because of incorrect CPU factors. A DEFAULT type might also cause binary incompatibility because a job from a DEFAULT host type can be migrated to another DEFAULT host type. 步骤 Run the lshosts command to see which host has the DEFAULT host model or type. lshosts HOST_NAME type model cpuf ncpus maxmem maxswp server RESOURCES hostA DEFAULT DEFAULT 1 2 256M 710M Yes () If Model or Type are displayed as DEFAULT when you use the lshosts command and automatic host model and type detection is enabled, you can leave it as is or change it. If the host model is DEFAULT, LSF works correctly but the host has a CPU factor of 1, which might not make efficient use of the host model. If the host type is DEFAULT, there might be binary incompatibility. For example, if one host is Linux and another is AIX, but both hosts are set to type DEFAULT, jobs that are running on the Linux host might be migrated to the AIX host and vice versa, which might cause the job to file. Run lim -t on the host whose type is DEFAULT: lim -t Host Type : NTX64 Host Architecture : EM64T_1596 Total NUMA Nodes : 1 Total Processors : 2 Total Cores : 4 Total Threads : 2 Matched Type : NTX64 Matched Architecture : EM64T_3000 Matched Model : Intel_EM64T CPU Factor : 60.0 NoteThe value of HostType and Host Architecture. Edit the lsf.shared file to configure the host type and host model for the host. In the HostType section, enter a new host type. Use the host type name that is detected with the lim -t command. Begin HostType TYPENAME DEFAULT CRAYJ NTX64 ... End HostType In the HostModel section, enter the new host model with architecture and CPU factor. Use the architecture that is detected with the lim -t commmand. Add the host model to the end of the host model list. The limit for host model entries is 127. Lines commented out with # are not counted in the 127-line limit. Begin HostModel MODELNAME CPUFACTOR ARCHITECTURE # keyword Intel_EM64T 20 EM64T_1596 End HostModel Save changes to the lsf.shared file. Run the lsadmin reconfig command to reconfigure LIM. Wait a few seconds, and run the lim -t command again to check the type and model of the host. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter4/section3/LSF_error_messages.html":{"url":"chapter4/section3/LSF_error_messages.html","title":"LSF 错误信息","keywords":"","body":"LSF 错误信息 以下错误消息由 LSF 守护程序记录，或由 lsadmin ckconfig 和 badmin ckconfig 命令显示。 一般性错误 The following messages can be generated by any LSF daemon: can’t open file: error The daemon might not open the named file for the reason that is given by error. This error is usually caused by incorrect file permissions or missing files. All directories in the path to the configuration files must have execute (x) permission for the LSF administrator, and the actual files must have read (r) permission. Missing files might be caused by the following errors: Incorrect path names in the lsf.conf file Running LSF daemons on a host where the configuration files are not installed Having a symbolic link that points to a file or directory that does not exist file(line): malloc failed Memory allocation failed. Either the host does not have enough available memory or swap space, or there is an internal error in the daemon. Check the program load and available swap space on the host. If the swap space is full, you must add more swap space or run fewer (or smaller) programs on that host. auth_user: getservbyname(ident/tcp) failed: error; ident must be registered in services The LSF_AUTH=ident parameter is defined in the lsf.conf file, but the ident/tcp service is not defined in the services database. Add ident/tcp to the services database, or remove the LSF_AUTH=ident parameter from the lsf.conf file and use the setuid root command on the LSF files that require authentication. auth_user: operation(/) failed: error The LSF_AUTH=ident parameter is defined in the lsf.conf file, but the LSF daemon failed to contact the identd daemon on the host. Check that identd is defined in inetd.conf and the identd daemon is running on host. auth_user: Authentication data format error (rbuf=) from / auth_user: Authentication port mismatch (...) from / The LSF_AUTH=ident parameter is defined in the lsf.conf file, but there is a protocol error between LSF and the ident daemon on host. Make sure that the identd daemon on the host is configured correctly. userok: Request from bad port (), denied The LSF_AUTH=ident parameter is not defined, and the LSF daemon received a request that originates from a non-privileged port. The request is not serviced. Set the LSF binary files to be owned by root with the setuid bit set, or define the LSF_AUTH=ident parameter and set up an ident server on all hosts in the cluster. If the files are on an NFS-mounted file system, make sure that the file system is not mounted with the nosuid flag. userok: Forged username suspected from /: / The service request claimed to come from user claimed_user but ident authentication returned that the user was actual_user. The request was not serviced. userok: ruserok(,) failed The LSF_USE_HOSTEQUIV parameter is defined in the lsf.conf file, but host is not set up as an equivalent host in /etc/host.equiv, and user uid is not set up in a .rhosts file. init_AcceptSock: RES service(res) not registered, exiting init_AcceptSock: res/tcp: unknown service, exiting initSock: LIM service not registered. initSock: Service lim/udp is unknown. Read LSF Guide for help get_ports: service not registered The LSF services are not registered. init_AcceptSock: Can’t bind daemon socket to port : error, exiting init_ServSock: Could not bind socket to port : error These error messages can occur if you try to start a second LSF daemon (for example, RES is already running, and you run RES again). If so, and you want to start the new daemon, kill the running daemon or use the lsadmin or badmin commands to shut down or restart the daemon. 配置错误 The following messages are caused by problems in the LSF configuration files. General errors are listed first, and then errors from specific files. file(line): Section name expected after Begin; ignoring section file(line): Invalid section name name; ignoring section The keyword Begin at the specified line is not followed by a section name, or is followed by an unrecognized section name. file(line): section section: Premature EOF The end of file was reached before reading the End section line for the named section. file(line): keyword line format error for section section; Ignore this section The first line of the section must contain a list of keywords. This error is logged when the keyword line is incorrect or contains an unrecognized keyword. file(line): values do not match keys for section section; Ignoring line The number of fields on a line in a configuration section does not match the number of keywords. This error can be caused by not putting () in a column to represent the default value. file: HostModel section missing or invalid file: Resource section missing or invalid file: HostType section missing or invalid The HostModel, Resource, or HostType section in the lsf.shared file is either missing or contains an unrecoverable error. file(line): Name name reserved or previously defined. Ignoring index The name that is assigned to an external load index must not be the same as any built-in or previously defined resource or load index. file(line): Duplicate clustername name in section cluster. Ignoring current line A cluster name is defined twice in the same lsf.shared file. The second definition is ignored. file(line): Bad cpuFactor for host model model. Ignoring line The CPU factor declared for the named host model in the lsf.shared file is not a valid number. file(line): Too many host models, ignoring model name You can declare a maximum of 127 host models in the lsf.shared file. file(line): Resource name name too long in section resource. Should be less than 40 characters. Ignoring line The maximum length of a resource name is 39 characters. Choose a shorter name for the resource. file(line): Resource name name reserved or previously defined. Ignoring line. You attempted to define a resource name that is reserved by LSF or already defined in the lsf.shared file. Choose another name for the resource. file(line): illegal character in resource name: name, section resource. Line ignored. Resource names must begin with a letter in the set [a-zA-Z], followed by letters, digits, or underscores [a-zA-Z0-9_]. LIM 信息 The following messages are logged by the LIM: findHostbyAddr/: Host / is unknown by function: Gethostbyaddr_(/) failed: error main: Request from unknown host /: error function: Received request from non-LSF host / The daemon does not recognize host. The request is not serviced. These messages can occur if host was added to the configuration files, but not all the daemons were reconfigured to read the new information. If the problem still occurs after reconfiguring all the daemons, check whether the host is a multi-addressed host. rcvLoadVector: Sender (/) may have different config? MasterRegister: Sender (host) may have different config? LIM detected inconsistent configuration information with the sending LIM. Run the following command so that all the LIMs have the same configuration information. lsadmin reconfig Note any hosts that failed to be contacted. rcvLoadVector: Got load from client-only host /. Kill LIM on / A LIM is running on a client host. Run the following command, or go to the client host and kill the LIM daemon. lsadmin limshutdown host saveIndx: Unknown index name from ELIM LIM received an external load index name that is not defined in the lsf.shared file. If name is defined in lsf.shared, reconfigure the LIM. Otherwise, add name to the lsf.shared file and reconfigure all the LIMs. saveIndx: ELIM over-riding value of index This warning message is logged when the ELIM sent a value for one of the built-in index names. LIM uses the value from ELIM in place of the value that is obtained from the kernel. getusr: Protocol error numIndx not read (cc=num): error getusr: Protocol error on index number (cc=num): error Protocol error between ELIM and LIM. RES 信息 The following messages are logged by the RES: doacceptconn: getpwnam(@/) failed: error doacceptconn: User has uid on client host /, uid on RES host; assume bad user authRequest: username/uid /@/ does not exist authRequest: Submitter’s name @ is different from name on this host RES assumes that a user has the same user ID and user name on all the LSF hosts. These messages occur if this assumption is violated. If the user is allowed to use LSF for interactive remote execution, make sure the user’s account has the same user ID and user name on all LSF hosts. doacceptconn: root remote execution permission denied authRequest: root job submission rejected Root tried to run or submit a job but LSF_ROOT_REX is not defined in the lsf.conf file. resControl: operation permission denied, uid = The user with user ID uid is not allowed to make RES control requests. Only the LSF administrator can make RES control requests. If the LSF_ROOT_REX parameter is defined in the lsf.conffile, can also make RES control requests. resControl: access(respath, X_OK): error The RES received a restart request, but failed to find the file respath to re-execute itself. Make sure respath contains the RES binary, and it has execution permission. mbatchd 和 sbatchd 信息 The following messages are logged by the mbatchd and sbatchd daemons: renewJob: Job : rename(,) failed: error mbatchd failed in trying to resubmit a rerunnable job. Check that the file from exists and that the LSF administrator can rename the file. If from is in an AFS directory, check that the LSF administrator’s token processing is properly setup. logJobInfo_: fopen() failed: error logJobInfo_: write failed: error logJobInfo_: seek failed: error logJobInfo_: write xdrpos failed: error logJobInfo_: write xdr buf len failed: error logJobInfo_: close() failed: error rmLogJobInfo: Job : can’t unlink(): error rmLogJobInfo_: Job : can’t stat(): error readLogJobInfo: Job can’t open(): error start_job: Job : readLogJobInfo failed: error readLogJobInfo: Job : can’t read() size size: error initLog: mkdir() failed: error : fopen( failed: error getElogLock: Can’t open existing lock file : error getElogLock: Error in opening lock file : error releaseElogLock: unlink() failed: error touchElogLock: Failed to open lock file : error touchElogLock: close failed: error mbatchd failed to create, remove, read, or write the log directory or a file in the log directory, for the reason that is given in error. Check that the LSF administrator has read, write, and execute permissions on the logdir directory. replay_newjob: File at line : Queue not found, saving to queue replay_switchjob: File at line : Destination queue not found, switching to queue When the mbatchd daemon was reconfigured, jobs were found in queue but that queue is no longer in the configuration. replay_startjob: JobId : exec host not found, saving to host When the mbatchd daemon was reconfigured, the event log contained jobs that are dispatched to host, but that host is no longer configured to be used by LSF. do_restartReq: Failed to get hData of host / mbatchd received a request from sbatchd on host host_name, but that host is not known to mbatchd. Either the configuration file has changed but mbatchd was not reconfigured to pick up the new configuration, or host_name is a client host but the sbatchd daemon is running on that host. Run the following command to reconfigure the mbatchd daemon or kill the sbatchd daemon on host_name. badmin reconfig LSF 命令信息 LSF daemon (LIM) not responding ... still trying During LIM restart, LSF commands might fail and display this error message. User programs that are linked to the LIM API also fail for the same reason. This message is displayed when LIM running on the master host list or server host list is restarted after configuration changes, such as adding new resources, or binary upgrade. Use the LSF_LIM_API_NTRIES parameter in the lsf.conf file or as an environment variable to define how many times LSF commands retry to communicate with the LIM API while LIM is not available. The LSF_LIM_API_NTRIES parameter is ignored by LSF and EGO daemons and all EGO commands. When the LSB_API_VERBOSE=Y parameter is set in the lsf.conf file, LSF batch commands display the not responding retry error message to stderr when LIM is not available. When the LSB_API_VERBOSE=N parameter is set in the lsf.conf file, LSF batch commands do not display the retry error message when LIM is not available. Batch 命令客户端信息 LSF displays error messages when a batch command cannot communicate with the mbatchd daemon. The following table provides a list of possible error reasons and the associated error message output. Point of failure Possible reason Error message output Establishing a connection with the mbatchd daemon The mbatchd daemon is too busy to accept new connections. The connect() system call times out. LSF is processing your request. Please wait… The mbatchd daemon is down or no process is listening at either the LSB_MBD_PORT or the LSB_QUERY_PORT LSF is down. Please wait… The mbatchd daemon is down and the LSB_QUERY_PORT is busy bhosts displays LSF is down. Please wait. . .bjobs displays Cannot connect to LSF. Please wait… Socket error on the client side Cannot connect to LSF. Please wait… connect() system call fails Cannot connect to LSF. Please wait… Internal library error Cannot connect to LSF. Please wait… Send/receive handshake message to/from the mbatchd daemon The mbatchd daemon is busy. Client times out when LSF is waiting to receive a message from mbatchd. LSF is processing your request. Please wait… Socket read()/write() fails Cannot connect to LSF. Please wait… Internal library error Cannot connect to LSF. Please wait… EGO 命令信息 You cannot run the egosh command because the administrator has chosen not to enable EGO in lsf.conf: LSF_ENABLE_EGO=N. If EGO is not enabled, the egosh command cannot find the ego.conf file or cannot contact the vemkd daemon (likely because it is not started). © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/run_jobs.html":{"url":"chapter5/run_jobs.html","title":"Chapter 5 作业调度管理","keywords":"","body":"Chapter 5 作业调度管理 运行，监视和控制提交给 LSF 的作业。 关于IBM Spectrum LSF 作业运行 作业监控 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/about_IBM_Spectrum_LSF.html":{"url":"chapter5/section1/about_IBM_Spectrum_LSF.html","title":"5.1 关于 IBM Spectrum LSF","keywords":"","body":"5.1 关于 IBM Spectrum LSF Clusters, jobs, and queues The IBM Spectrum LSF (\"LSF\", short for load sharing facility) software is industry-leading enterprise-class software that distributes work across existing heterogeneous IT resources to create a shared, scalable, and fault-tolerant infrastructure, that delivers faster, balanced, more reliable workload performance and reduces cost. Hosts LSF daemons Batch jobs and tasks Host types and host models Users and administrators Resources Job lifecycle © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/LSF_clusters_jobs_and_queues.html":{"url":"chapter5/section1/LSF_clusters_jobs_and_queues.html","title":"LSF 集群，作业与队列","keywords":"","body":"LSF 集群，作业与队列 The IBM Spectrum LSF (\"LSF\", short for load sharing facility) software is industry-leading enterprise-class software that distributes work across existing heterogeneous IT resources to create a shared, scalable, and fault-tolerant infrastructure, that delivers faster, balanced, more reliable workload performance and reduces cost. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/hosts.html":{"url":"chapter5/section1/hosts.html","title":"节点","keywords":"","body":"节点 A host is an individual computer in the cluster. Each host may have more than one processor. Multiprocessor hosts are used to run parallel jobs. A multiprocessor host with a single process queue is considered a single machine, while a box full of processors that each have their own process queue is treated as a group of separate machines. Commands lsload — View load on hosts lshosts — View configuration information about hosts in the cluster including number of CPUS, model, type, and whether the host is a client or server bhosts — View batch server hosts in the cluster TipThe names of your hosts should be unique. They should not be the same as the cluster name or any queue defined for the cluster. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/LSF_daemons.html":{"url":"chapter5/section1/LSF_daemons.html","title":"LSF 守护程序","keywords":"","body":"LSF daemons LSF daemon Role mbatchd Job requests and dispatch mbschd Job scheduling sbatchd**res** Job execution Parent topic: About IBM Spectrum LSF mbatchd Master Batch Daemon running on the master host. Started by sbatchd. Responsible for the overall state of jobs in the system. Receives job submission, and information query requests. Manages jobs that are held in queues. Dispatches jobs to hosts as determined by mbschd. Configuration Port number is defined in lsf.conf. mbschd Master Batch Scheduler Daemon running on the master host. Works with mbatchd. Started by mbatchd. Makes scheduling decisions based on job requirements, and policies, and resource availability. Sends scheduling decisions to mbatchd. sbatchd Slave Batch Daemon running on each server host. Receives the request to run the job from mbatchd and manages local execution of the job. Responsible for enforcing local policies and maintaining the state of jobs on the host. The sbatchd forks a child sbatchd for every job. The child sbatchd runs an instance of res to create the execution environment in which the job runs. The child sbatchd exits when the job is complete. Commands badmin hstartup — Starts sbatchd badmin hshutdown — Shuts down sbatchd badmin hrestart — Restarts sbatchd Configuration Port number is defined in lsf.conf res Remote Execution Server (res) running on each server host. Accepts remote execution requests to provide transparent and secure remote execution of jobs and tasks. Commands lsadmin resstartup — Starts res lsadmin resshutdown — Shuts down res lsadmin resrestart — Restarts res Configuration Port number is defined in lsf.conf. lim Load Information Manager (LIM) running on each server host. Collects host load and configuration information and forwards it to the master LIM running on the master host. Reports the information that is displayed by lsload and lshosts. Static indices are reported when the LIM starts up or when the number of CPUs (ncpus) change. Static indices are: Number of CPUs (ncpus) Number of disks (ndisks) Total available memory (maxmem) Total available swap (maxswp) Total available temp (maxtmp) Dynamic indices for host load collected at regular intervals are: Hosts status (status) 15 second, 1 minute, and 15 minute run queue lengths (r15s, r1m, and r15m) CPU utilization (ut) Paging rate (pg) Number of login sessions (ls) Interactive idle time (it) Available swap space (swp) Available memory (mem) Available temp space (tmp) Disk IO rate (io) Commands lsadmin limstartup — Starts LIM lsadmin limshutdown — Shuts down LIM lsadmin limrestart — Restarts LIM lsload — View dynamic load values lshosts — View static host load values Configuration Port number is defined in lsf.conf. Master LIM The LIM running on the master host. Receives load information from the LIMs running on hosts in the cluster. Forwards load information to mbatchd, which forwards this information to mbschd to support scheduling decisions. If the master LIM becomes unavailable, a LIM on another host automatically takes over. Commands lsadmin limstartup — Starts LIM lsadmin limshutdown — Shuts down LIM lsadmin limrestart — Restarts LIM lsload — View dynamic load values lshosts — View static host load values Configuration Port number is defined in lsf.conf. ELIM External LIM (ELIM) is a site-definable executable that collects and tracks custom dynamic load indices. An ELIM can be a shell script or a compiled binary program, which returns the values of the dynamic resources you define. The ELIM executable must be named elim and located in LSF_SERVERDIR. pim Process Information Manager (PIM) running on each server host. Started by LIM, which periodically checks on pim and restarts it if it dies. Collects information about job processes running on the host such as CPU and memory that is used by the job, and reports the information to sbatchd. Commands bjobs — View job information © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/Batch_jobs_and_tasks.html":{"url":"chapter5/section1/Batch_jobs_and_tasks.html","title":"批处理作业和任务","keywords":"","body":"Batch jobs and tasks You can either run jobs through the batch system where jobs are held in queues, or you can interactively run tasks without going through the batch system, such as tests. Parent topic: About IBM Spectrum LSF Job A unit of work that is run in the LSF system. A job is a command that is submitted to LSF for execution, using the bsub command. LSF schedules, controls, and tracks the job according to configured policies. Jobs can be complex problems, simulation scenarios, extensive calculations, anything that needs compute power. Commands bjobs — View jobs in the system bsub — Submit jobs Interactive batch job A batch job that allows you to interact with the application and still take advantage of LSF scheduling policies and fault tolerance. All input and output are through the terminal that you used to type the job submission command. When you submit an interactive job, a message is displayed while the job is awaiting scheduling. A new job cannot be submitted until the interactive job is completed or terminated. The bsub command stops display of output from the shell until the job completes, and no mail is sent to you by default. Use Ctrl-C at any time to terminate the job. Commands bsub -I — Submit an interactive job Interactive task A command that is not submitted to a batch queue and scheduled by LSF, but is dispatched immediately. LSF locates the resources that are needed by the task and chooses the best host among the candidate hosts that has the required resources and is lightly loaded. Each command can be a single process, or it can be a group of cooperating processes. Tasks are run without using the batch processing features of LSF but still with the advantage of resource requirements and selection of the best host to run the task based on load. Commands lsrun — Submit an interactive task lsgrun — Submit an interactive task to a group of hosts See also LSF utilities such as ch, lsacct, lsacctmrg, lslogin, lsplace, lsload, lsloadadj, lseligible, lsmon, lstcsh. Local task An application or command that does not make sense to run remotely. For example, the ls command on UNIX. Commands lsltasks — View and add tasks Configuration lsf.task— Configure system-wide resource requirements for tasks lsf.task.cluster — Configure cluster-wide resource requirements for tasks .lsftasks — Configure user-specific tasks Remote task An application or command that can be run on another machine in the cluster. Commands lsrtasks — View and add tasks Configuration lsf.task — Configure system-wide resource requirements for tasks lsf.task.cluster — Configure cluster-wide resource requirements for tasks .lsftasks — Configure user-specific tasks © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/Host_types_and_host_models.html":{"url":"chapter5/section1/Host_types_and_host_models.html","title":"主机类型和主机型号","keywords":"","body":"Host types and host models Hosts in LSF are characterized by host type and host model. The following example has a host type of X86_64. Host models are Opteron240, Opteron840, Intel_EM64T, Intel_IA64. Host type The combination of operating system version and host CPU architecture. All computers that run the same operating system on the same computer architecture are of the same type — in other words, binary-compatible with each other. Each host type usually requires a different set of LSF binary files. Commands: lsinfo -t — View all host types that are defined in lsf.shared Configuration: Defined in lsf.shared Mapped to hosts in lsf.cluster.cluster_name Host model The combination of host type and CPU speed (CPU factor) of the computer. All hosts of the same relative speed are assigned the same host model. The CPU factor is taken into consideration when jobs are being dispatched. Commands: lsinfo -m — View a list of currently running models lsinfo -M — View all models that are defined in lsf.shared Configuration: Defined in lsf.shared Mapped to hosts in lsf.cluster.cluster_name © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/Users_and_administrators.html":{"url":"chapter5/section1/Users_and_administrators.html","title":"用户和管理员","keywords":"","body":"Users and administrators LSF user A user account that has permission to submit jobs to the LSF cluster. LSF administrator In general, you must be an LSF administrator to perform operations that will affect other LSF users. Each cluster has one primary LSF administrator, specified during LSF installation. You can also configure additional administrators at the cluster level and at the queue level. Primary LSF administrator The first cluster administrator specified during installation and first administrator listed in lsf.cluster.cluster_name. The primary LSF administrator account owns the configuration and log files. The primary LSF administrator has permission to perform clusterwide operations, change configuration files, reconfigure the cluster, and control jobs submitted by all users. Cluster administrator May be specified during LSF installation or configured after installation. Cluster administrators can perform administrative operations on all jobs and queues in the cluster. Cluster administrators have the same cluster-wide operational privileges as the primary LSF administrator except that they do not necessarily have permission to change LSF configuration files. For example, a cluster administrator can create an LSF host group, submit a job to any queue, or terminate another user’s job. Queue administrator An LSF administrator user account that has administrative permissions limited to a specified queue. For example, an LSF queue administrator can perform administrative operations on the specified queue, or on jobs running in the specified queue, but cannot change LSF configuration or operate on LSF daemons. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/Resources.html":{"url":"chapter5/section1/Resources.html","title":"资源","keywords":"","body":"Resources Resource usage The LSF system uses built-in and configured resources to track resource availability and usage. Jobs are scheduled according to the resources available on individual hosts. Jobs that are submitted through the LSF system will have the resources that they use monitored while they are running. This information is used to enforce resource limits and load thresholds as well as fairshare scheduling. LSF collects information such as: Total CPU time consumed by all processes in the job Total resident memory usage in KB of all currently running processes in a job Total virtual memory usage in KB of all currently running processes in a job Currently active process group ID in a job Currently active processes in a job On UNIX, job-level resource usage is collected through PIM. Commands lsinfo — View the resources available in your cluster bjobs -l — View current resource usage of a job Configuration SBD_SLEEP_TIME in lsb.params — Configures how often resource usage information is sampled by PIM, collected by sbatchd, and sent to mbatchd Load indices Load indices measure the availability of dynamic, non-shared resources on hosts in the cluster. Load indices that are built into the LIM are updated at fixed time intervals. Commands lsload -l — View all load indices bhosts -l — View load levels on a host External load indices Defined and configured by the LSF administrator and collected by an External Load Information Manager (ELIM) program. The ELIM also updates LIM when new values are received. Commands lsinfo — View external load indices Static resources Built-in resources that represent host information that does not change over time, such as the maximum RAM available to user processes or the number of processors in a machine. Most static resources are determined by the LIM at startup. Static resources can be used to select appropriate hosts for particular jobs based on binary architecture, relative CPU speed, and system configuration. Load thresholds Two types of load thresholds can be configured by your LSF administrator to schedule jobs in queues. Each load threshold specifies a load index value: loadSched determines the load condition for dispatching pending jobs. If a host’s load is beyond any defined loadSched, a job will not be started on the host. This threshold is also used as the condition for resuming suspended jobs. loadStop determines when running jobs should be suspended. To schedule a job on a host, the load levels on that host must satisfy both the thresholds that are configured for that host and the thresholds for the queue from which the job is being dispatched. The value of a load index may either increase or decrease with load, depending on the meaning of the specific load index. Therefore, when comparing the host load conditions with the threshold values, you need to use either greater than (>) or less than ( Commands bhosts -l — View suspending conditions for hosts bqueues -l — View suspending conditions for queues bjobs -l — View suspending conditions for a particular job and the scheduling thresholds that control when a job is resumed Configuration lsb.hosts — Configure thresholds for hosts lsb.queues — Configure thresholds for queues Runtime resource usage limits Limit the use of resources while a job is running. Jobs that consume more than the specified amount of a resource are signaled or have their priority lowered. Configuration lsb.queues — Configure resource usage limits for queues Hard and soft limits Resource limits that are specified at the queue level are hard limits while those specified with job submission are soft limits. See setrlimit(2) man page for concepts of hard and soft limits. Resource allocation limits Restrict the amount of a given resource that must be available during job scheduling for different classes of jobs to start, and which resource consumers the limits apply to. If all of the resource has been consumed, no more jobs can be started until some of the resource is released. Configuration lsb.resources — Configure queue-level resource allocation limits for hosts, users, queues, and projects Resource requirements (bsub -R) Restrict which hosts the job can run on. Hosts that match the resource requirements are the candidate hosts. When LSF schedules a job, it collects the load index values of all the candidate hosts and compares them to the scheduling conditions. Jobs are only dispatched to a host if all load values are within the scheduling thresholds. Commands bsub -R — Specify resource requirement string for a job Configuration lsb.queues — Configure resource requirements for queues © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section1/Job_lifecycle.html":{"url":"chapter5/section1/Job_lifecycle.html","title":"作业生命周期","keywords":"","body":"Job lifecycle Parent topic: About IBM Spectrum LSF 1 Submit a job You submit a job from an LSF client or server with the bsub command. If you do not specify a queue when submitting the job, the job is submitted to the default queue. Jobs are held in a queue waiting to be scheduled and have the PEND state. The job is held in a job file in the LSF_SHAREDIR/cluster_name/logdir/info/ directory. Job ID LSF assigns each job a unique job ID when you submit the job. Job name You can also assign a name to the job with the -J option of bsub. Unlike the job ID, the job name is not necessarily unique. 2 Schedule job mbatchd looks at jobs in the queue and sends the jobs for scheduling to mbschd at a preset time interval (defined by the parameter JOB_SCHEDULING_INTERVAL in lsb.params). mbschd evaluates jobs and makes scheduling decisions based on the following: Job priority Scheduling policies Available resources mbschd selects the best hosts where the job can run and sends its decisions back to mbatchd. Resource information is collected at preset time intervals by the master LIM from LIMs on server hosts. The master LIM communicates this information to mbatchd, which in turn communicates it to mbschd to support scheduling decisions. 3 Dispatch job As soon as mbatchd receives scheduling decisions, it immediately dispatches the jobs to hosts. 4 Run job sbatchd handles job execution. It does the following: Receives the request from mbatchd Creates a child sbatchd for the job Creates the execution environment Starts the job using res The execution environment is copied from the submission host to the execution host and includes the following: Environment variables that are needed by the job Working directory where the job begins running Other system-dependent environment settings; for example: On UNIX and Linux, resource limits and umask On Windows, desktop and Windows root directory The job runs under the user account that submitted the job and has the status RUN. 5 Return output When a job is completed, it is assigned the DONE status if the job was completed without any problems. The job is assigned the EXIT status if errors prevented the job from completing. sbatchd communicates job information including errors and output to mbatchd. 6 Send email to client mbatchd returns the job output, job error, and job information to the submission host through email. Use the -o and -e options of bsub to send job output and errors to a file. Job report A job report is sent by email to the LSF client and includes the following information: Job information such as the following: CPU use Memory use Name of the account that submitted the job Job output Errors © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/working_with_jobs.html":{"url":"chapter5/section2/working_with_jobs.html","title":"5.2 作业运行","keywords":"","body":"5.2 作业运行 Submitting jobs (bsub) Modify pending jobs (bmod) Modify running jobs About controlling jobs LSF controls jobs dispatched to a host to enforce scheduling policies or in response to user requests. Using LSF with non-shared file space About resource reservation Set pending time limits You can specify pending time limits and eligible pending time limits for jobs to ensure that jobs do not remain pending in LSF for too long. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/submitting_jobs_using_bsub.html":{"url":"chapter5/section2/subsection1/submitting_jobs_using_bsub.html","title":"bsub 提交作业","keywords":"","body":"bsub 提交作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/About submitting a job to a specific queue.html":{"url":"chapter5/section2/subsection1/About submitting a job to a specific queue.html","title":"将作业提交到特定队列","keywords":"","body":"将作业提交到特定队列 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/View available queues.html":{"url":"chapter5/section2/subsection1/View available queues.html","title":"查看可用队列","keywords":"","body":"查看可用队列 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job to a queue.html":{"url":"chapter5/section2/subsection1/Submit a job to a queue.html","title":"将作业提交到队列","keywords":"","body":"将作业提交到队列 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job associated with a project.html":{"url":"chapter5/section2/subsection1/Submit a job associated with a project.html","title":"提交与项目关联的作业 (bsub -P)","keywords":"","body":"提交与项目关联的作业 (bsub -P) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job associated with a user group.html":{"url":"chapter5/section2/subsection1/Submit a job associated with a user group.html","title":"提交与用户组关联的作业 (bsub -G)","keywords":"","body":"提交与用户组关联的作业 (bsub -G) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job with a job name.html":{"url":"chapter5/section2/subsection1/Submit a job with a job name.html","title":"提交有作业名的作业 (bsub -J)","keywords":"","body":"提交有作业名的作业 (bsub -J) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job to a service class.html":{"url":"chapter5/section2/subsection1/Submit a job to a service class.html","title":"提交作业到服务类 (bsub -sla)","keywords":"","body":"提交作业到服务类 (bsub -sla) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job under a job group.html":{"url":"chapter5/section2/subsection1/Submit a job under a job group.html","title":"在作业组下提交作业 (bsub -g)","keywords":"","body":"在作业组下提交作业 (bsub -g) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job with a JSON file.html":{"url":"chapter5/section2/subsection1/Submit a job with a JSON file.html","title":"提交带有 JSON 文件的作业 (bsub -json)","keywords":"","body":"提交带有 JSON 文件的作业 (bsub -json) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job with a YAML file.html":{"url":"chapter5/section2/subsection1/Submit a job with a YAML file.html","title":"提交带有 YAML 文件的作业 (bsub -yaml)","keywords":"","body":"提交带有 YAML 文件的作业 (bsub -yaml) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection1/Submit a job with a JSDL file.html":{"url":"chapter5/section2/subsection1/Submit a job with a JSDL file.html","title":"提交带有 JSDL 文件的作业 (bsub -jsdl)","keywords":"","body":"提交带有 JSDL 文件的作业 (bsub -jsdl) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection2/Modify pending jobs.html":{"url":"chapter5/section2/subsection2/Modify pending jobs.html","title":"修改正在等待的作业 (bmod)","keywords":"","body":"修改正在等待的作业 (bmod) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection3/Modify running jobs.html":{"url":"chapter5/section2/subsection3/Modify running jobs.html","title":"修改正在运行的作业","keywords":"","body":"修改正在运行的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/About controlling jobs.html":{"url":"chapter5/section2/subsection4/About controlling jobs.html","title":"关于控制作业","keywords":"","body":"关于控制作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Kill a job.html":{"url":"chapter5/section2/subsection4/Kill a job.html","title":"杀掉作业 (bkill)","keywords":"","body":"杀掉作业 (bkill) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/About suspending and resuming jobs.html":{"url":"chapter5/section2/subsection4/About suspending and resuming jobs.html","title":"关于暂停和恢复作业 (bstop and bresume)","keywords":"","body":"关于暂停和恢复作业 (bstop and bresume) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Move a job to the bottom of a queue.html":{"url":"chapter5/section2/subsection4/Move a job to the bottom of a queue.html","title":"将作业移到队列底部 (bbot)","keywords":"","body":"将作业移到队列底部 (bbot) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Move a job to the top of a queue.html":{"url":"chapter5/section2/subsection4/Move a job to the top of a queue.html","title":"将作业移到队列顶部 (btop)","keywords":"","body":"将作业移到队列顶部 (btop) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Control jobs in job groups.html":{"url":"chapter5/section2/subsection4/Control jobs in job groups.html","title":"控制作业组中的作业","keywords":"","body":"控制作业组中的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Submit a job to specific hosts.html":{"url":"chapter5/section2/subsection4/Submit a job to specific hosts.html","title":"将作业提交给特定主机","keywords":"","body":"将作业提交给特定主机 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Submit a job with specific resources.html":{"url":"chapter5/section2/subsection4/Submit a job with specific resources.html","title":"提交具有特定资源的作业","keywords":"","body":"提交具有特定资源的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Queues and host preference.html":{"url":"chapter5/section2/subsection4/Queues and host preference.html","title":"队列和主机首选项","keywords":"","body":"队列和主机首选项 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Specify different levels of host preference.html":{"url":"chapter5/section2/subsection4/Specify different levels of host preference.html","title":"指定不同级别的主机首选项","keywords":"","body":"指定不同级别的主机首选项 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Submit a job with resource requirements.html":{"url":"chapter5/section2/subsection4/Submit a job with resource requirements.html","title":"提交具有资源需求的作业","keywords":"","body":"提交具有资源需求的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection4/Submit a job with SSH X11 forwarding.html":{"url":"chapter5/section2/subsection4/Submit a job with SSH X11 forwarding.html","title":"通过 SSH X11 转发提交作业","keywords":"","body":"通过 SSH X11 转发提交作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection5/Using LSF with non-shared file space.html":{"url":"chapter5/section2/subsection5/Using LSF with non-shared file space.html","title":"将 LSF 与非共享文件空间一起使用","keywords":"","body":"将 LSF 与非共享文件空间一起使用 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection5/Operator.html":{"url":"chapter5/section2/subsection5/Operator.html","title":"操作符","keywords":"","body":"操作符 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection6/About resource reservation.html":{"url":"chapter5/section2/subsection6/About resource reservation.html","title":"关于资源预约","keywords":"","body":"关于资源预约 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection6/View resource information.html":{"url":"chapter5/section2/subsection6/View resource information.html","title":"查看资源信息","keywords":"","body":"查看资源信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection6/Submit a job with resource requirements.html":{"url":"chapter5/section2/subsection6/Submit a job with resource requirements.html","title":"提交具有资源需求的作业","keywords":"","body":"提交具有资源需求的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection6/Submit a job with start or termination times.html":{"url":"chapter5/section2/subsection6/Submit a job with start or termination times.html","title":"提交有开始或终止时间的作业","keywords":"","body":"提交有开始或终止时间的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection6/Submit a job with compute unit resource requirements.html":{"url":"chapter5/section2/subsection6/Submit a job with compute unit resource requirements.html","title":"提交具有计算单元资源要求的作业","keywords":"","body":"提交具有计算单元资源要求的作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section2/subsection7/Set pending time limits.html":{"url":"chapter5/section2/subsection7/Set pending time limits.html","title":"设置等待时间限制","keywords":"","body":"设置等待时间限制 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/monitoring_jobs.html":{"url":"chapter5/section3/monitoring_jobs.html","title":"5.3 作业监控","keywords":"","body":"5.3 作业监控 View information about jobs Display resource allocation limits Use the blimits command to display resource allocation limits. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View information about jobs.html":{"url":"chapter5/section3/subsection1/View information about jobs.html","title":"查看有关作业的信息","keywords":"","body":"查看有关作业的信息 Procedure Use the bjobs and bhist commands to view information about jobs: bjobs reports the status of jobs and the various options allow you to display specific information. bhist reports the history of one or more jobs in the system. You can also find jobs on specific queues or hosts, find jobs that are submitted by specific projects, and check the status of specific jobs by using their job IDs or names. View unfinished jobs View summary information of unfinished jobs View all jobs View running jobs View pending reasons for jobs When you submit a job, it can be held in the queue before it starts running and it might be suspended while it is running. You can find out why jobs are pending or in suspension with the bjobs -p option. View job suspending reasons When you submit a job, it may be held in the queue before it starts running and it may be suspended while running. View detailed job information View job group information You can view information about jobs in job groups or view jobs by job group. Monitor SLA progress View job output View chronological history of jobs View history of jobs not listed in active event log View job history View the job submission environment Use the bjobs -env command option to view a job's environment variables or the bjobs -script command option to view the job script file. Update interval Job-level information © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View unfinished jobs.html":{"url":"chapter5/section3/subsection1/View unfinished jobs.html","title":"查看未完成的工作","keywords":"","body":"查看未完成的工作 Procedure Run bjobs to view the status of LSF jobs. When no options are specified, bjobs displays information about jobs in the PEND, RUN, USUSP, PSUSP, and SSUSP states for the current user. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View summary information of unfinished jobs.html":{"url":"chapter5/section3/subsection1/View summary information of unfinished jobs.html","title":"查看未完成的作业的摘要信息","keywords":"","body":"查看未完成的作业的摘要信息 Procedure Run bjobs -sum to view summary information on the status of LSF jobs. When no other options are specified, bjobs -sum displays the count of unfinished tasks for jobs in the following states: running (RUN), system suspended (SSUSP), user suspended (USUSP), pending (PEND), forwarded to remote clusters and pending (FWD_PEND), and UNKNOWN. Use bjobs -sum with other options (such as -m, -P, -q, and -u) to filter the results. For example, bjobs -sum -u user1 displays task counts just for user user1. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View all jobs.html":{"url":"chapter5/section3/subsection1/View all jobs.html","title":"查看所有作业","keywords":"","body":"查看所有作业 About this task You can display information about jobs that are both running and those recently finished (PEND, RUN, USUSP, PSUSP, SSUSP, DONE, and EXIT statuses). Procedure Run bjobs -a . All your jobs that are still in the system and jobs that have recently finished are displayed. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View running jobs.html":{"url":"chapter5/section3/subsection1/View running jobs.html","title":"查看正在运行的作业","keywords":"","body":"查看正在运行的作业 About this task You can display information about only jobs that are running (RUN status). Procedure Run bjobs -r. All your running jobs are displayed. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View pending reasons for jobs.html":{"url":"chapter5/section3/subsection1/View pending reasons for jobs.html","title":"查看在等待作业的原因","keywords":"","body":"查看在等待作业的原因 When you submit a job, it can be held in the queue before it starts running and it might be suspended while it is running. You can find out why jobs are pending or in suspension with the bjobs -p option. Procedure Run bjobs -p. Displays information for pending jobs (PEND state) and their reasons. There can be more than one reason why the job is pending. The pending reasons also display the number of hosts for each condition. To get specific host names along with pending reasons, run bjobs -lp. To view the pending reasons for all users, run bjobs -p -u all. Run bjobs -psum to display the summarized number of jobs, hosts, and occurrences for each pending reason. Run busers -w all to see the maximum pending job threshold for all users. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View job suspending reasons.html":{"url":"chapter5/section3/subsection1/View job suspending reasons.html","title":"查看作业暂停原因","keywords":"","body":"查看作业暂停原因 When you submit a job, it may be held in the queue before it starts running and it may be suspended while running. Procedure Run the bjobs -s command. Displays information for suspended jobs (SUSP state) and their reasons. There can be more than one reason why the job is suspended. The pending reasons also display the number of hosts for each condition. Run bjobs -ls to see detailed information about suspended jobs, including specific host names along with the suspend reason. The load threshold that caused LSF to suspend a job, together with the scheduling parameters, is displayed. NoteThe STOP_COND parameter affects the suspending reasons as displayed by the bjobs command. If theSTOP_COND parameter is specified in the queue and the loadStop thresholds are not specified, the suspending reasons for each individual load index are not displayed. To view the suspend reasons for all users, run bjobs -s -u all. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View detailed job information.html":{"url":"chapter5/section3/subsection1/View detailed job information.html","title":"查看详细的作业信息","keywords":"","body":"查看详细的作业信息 About this task The -l option of bjobs displays detailed information about job status and parameters, such as the job’s current working directory, parameters that are specified when the job was submitted, and the time when the job started running. bjobs -l with a job ID displays all the information about a job, including: Submission parameters Execution environment Resource usage Procedure Run bjobs -l. bjobs -l 7678 Job Id , User , Project , Status , Queue , Command Mon Oct 28 13:08:11 2009: Submitted from host ,CWD , Requested Resources 35>; PENDING REASONS: Queue’s resource requirements not satisfied:3 hosts; Unable to reach slave lsbatch server: 1 host; Not enough job slots: 1 host; SCHEDULING PARAMETERS: r15s r1m r15m ut pg io ls it tmp swp mem loadSched - 0.7 1.0 - 4.0 - - - - - - loadStop - 1.5 2.5 - 8.0 - - - - - - © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View job group information.html":{"url":"chapter5/section3/subsection1/View job group information.html","title":"查看作业组信息","keywords":"","body":"查看作业组信息 You can view information about jobs in job groups or view jobs by job group. Before you begin Procedure To see all job groups, run bjgroup. To see jobs by job group, run bjobs -g /group_name. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/Monitor SLA progress.html":{"url":"chapter5/section3/subsection1/Monitor SLA progress.html","title":"监测 SLA 进程","keywords":"","body":"监测 SLA 进程 About this task You can display the properties of service classes that are configured in lsb.serviceclasses and the dynamic state information for each service class. Procedure Run bsla Run bacct -sla to display historical performance of a service class. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View job output.html":{"url":"chapter5/section3/subsection1/View job output.html","title":"查看作业输出","keywords":"","body":"查看作业输出 Before you begin You must be logged on as the job owner. About this task The output from a job is normally not available until the job is finished. However, LSF provides the bpeek command for you to look at the output the job has produced so far. By default, bpeek shows the output from the most recently submitted job. You can also select the job by queue or execution host, or specify the job ID or job name on the command line. To save time, you can use this command to check whether your job is behaving as you expected and kill the job if it is running away or producing unusable results. Procedure Run bpeek job_id. Example For example: bpeek 1234 > Starting phase 1 Phase 1 done Calculating new parameters ... © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View chronological history of jobs.html":{"url":"chapter5/section3/subsection1/View chronological history of jobs.html","title":"查看作业的时间顺序历史","keywords":"","body":"查看作业的时间顺序历史 About this task By default, the bhist command displays information from the job event history file, lsb.events, on a per job basis. Procedure Use the -t option of bhist to display the events chronologically instead of grouping all events for each job. Use the -T option to select only those events within a given time range. Example For example, the following displays all events that occurred between 14:00 and 14:30 on a given day: bhist -t -T 14:00,14:30 Wed Oct 22 14:01:25 2009: Job done successfully; Wed Oct 22 14:03:09 2009: Job submitted from host to Queue , CWD , User , Project , Command , Requested Resources ; Wed Oct 22 14:03:18 2009: Job dispatched to ; Wed Oct 22 14:03:18 2009: Job starting (Pid 210); Wed Oct 22 14:03:18 2009: Job running with execution home , Execution CWD , Execution Pid ; Wed Oct 22 14:05:06 2009: Job submitted from host to Queue, CWD , User , Project , Command , Requested Resources ; Wed Oct 22 14:05:11 2009: Job dispatched to ; Wed Oct 22 14:05:11 2009: Job starting (Pid 429); Wed Oct 22 14:05:12 2009: Job running with execution home, Execution CWD , Execution Pid ; Wed Oct 22 14:08:26 2009: Job submitted from host to Queue, CWD , User , Project , Command; Wed Oct 22 14:10:55 2009: Job done successfully; Wed Oct 22 14:16:55 2009: Job exited; Wed Oct 22 14:17:04 2009: Job done successfully; © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View history of jobs not listed in active event log.html":{"url":"chapter5/section3/subsection1/View history of jobs not listed in active event log.html","title":"查看未在活动事件日志中列出的作业历史","keywords":"","body":"查看未在活动事件日志中列出的作业历史 About this task LSF periodically backs up and prunes the job history log. By default, bhist only displays job history from the current event log file. You can display the history for jobs that completed some time ago and are no longer listed in the active event log. The -n num_logfiles option tells the bhist command to search through the specified number of log files instead of only searching the current log file. Log files are searched in reverse time order. For example, the command bhist -n 3 searches the current event log file and then the two most recent backup files. Procedure Run bhist -n num_logfiles. Example bhist -n 1 ![复制代码](https://www.ibm.com/support/knowledgecenter/images/icons/copy.png) Searches the current event log file lsb.events bhist -n 2 ![复制代码](https://www.ibm.com/support/knowledgecenter/images/icons/copy.png) Searches lsb.events and lsb.events.1 bhist -n 3 ![复制代码](https://www.ibm.com/support/knowledgecenter/images/icons/copy.png) Searches lsb.events, lsb.events.1, lsb.events.2 bhist -n 0 ![复制代码](https://www.ibm.com/support/knowledgecenter/images/icons/copy.png) Searches all event log files in LSB_SHAREDIR © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View job history.html":{"url":"chapter5/section3/subsection1/View job history.html","title":"查看作业历史记录","keywords":"","body":"查看作业历史记录 About this task You can check on the status of your job since it was submitted. The bhist command displays a summary of the pending, suspended, and running time of jobs for the user who invoked the command. Procedure Run bhist. Run bhist -l to display the time information and a complete history of scheduling events for each job. Use bhist -u all to display a summary for all users in the cluster. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/View the job submission environment.html":{"url":"chapter5/section3/subsection1/View the job submission environment.html","title":"查看作业提交环境","keywords":"","body":"查看作业提交环境 Use the bjobs -env command option to view a job's environment variables or the bjobs -script command option to view the job script file. About this task You cannot specify the options with the-envor-scriptoptions. Procedure To view the environment variables for a specified job, run the bjobs -env command option. bjobs -env job_id You must specify a single job ID or job array element when using the -env command option. Multiple job IDs are not supported. To view the specified job's job script file, run the bjobs -script command option. bjobs -script job_id You must specify a single job ID or job array element when using the -script command option. Job arrays and multiple job IDs are not supported. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/Update interval.html":{"url":"chapter5/section3/subsection1/Update interval.html","title":"更新间隔","keywords":"","body":"更新间隔 The job-level resource usage information is updated at a maximum frequency of every SBD_SLEEP_TIME second. The update is done only if the value for the CPU time, resident memory usage, or virtual memory usage has changed by more than 10 percent from the previous update or if a new process or process group has been created. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection1/Job-level information.html":{"url":"chapter5/section3/subsection1/Job-level information.html","title":"作业级别信息","keywords":"","body":"作业级别信息 Job-level information includes: Total CPU time consumed by all processes of a job Total resident memory usage in KB of all currently running processes of a job Total virtual memory usage in KB of all currently running processes of a job Currently active process group ID of a job Currently active processes of a job © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection2/Display resource allocation limits.html":{"url":"chapter5/section3/subsection2/Display resource allocation limits.html","title":"显示资源分配限制","keywords":"","body":"显示资源分配限制 Use the blimits command to display resource allocation limits. The Configured policy name and information for limits that are being applied to running jobs. Configured policy name and information for all limits, even if they are not being applied to running jobs (-a option). Users (-u option) Queues (-q option) Hosts (-m option) Project names (-P option) All resource configurations in lsb.resources (-c option). This option is the same as bresources with no options. Resources that have no configured limits or no limit usage are indicated by a dash (-). Limits are displayed in a USED/LIMIT format. For example, if a limit of 10 slots is configured and 3 slots are in use, then blimits displays the limit for SLOTS as 3/10. If limits MEM, SWP, or TMP are configured as percentages, both the limit and the amount that is used are displayed in MB. For example, lshosts displays maximum memory (maxmem) of 249 MB, and MEM is limited to 10% of available memory. If 10 MB out of are used, blimits displays the limit for MEM as 10/25 (10 MB USED from a 25 MB LIMIT). Configured limits and resource usage for built-in resources (slots, mem, tmp, and swp load indices) are displayed as INTERNAL RESOURCE LIMITS separately from custom external resources, which are shown as EXTERNAL RESOURCE LIMITS. Limits are displayed for both the vertical tabular format and the horizontal format for Limit sections. If a vertical format Limit section has no name, blimits displays NONAMEnnn under the NAME column for these limits, where the unnamed limits are numbered in the order the vertical-format Limit sections appear in the lsb.resources file. If a resource consumer is configured as all, the limit usage for that consumer is indicated by a dash (-). PER_HOST slot limits are not displayed. The bhosts command displays these limits as MXJ limits. In MultiCluster, blimits returns the information about all limits in the local cluster. View information about resource allocation limits © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter5/section3/subsection2/View information about resource allocation limits.html":{"url":"chapter5/section3/subsection2/View information about resource allocation limits.html","title":"查看有关资源分配限制的信息","keywords":"","body":"查看有关资源分配限制的信息 About this task Your job may be pending because some configured resource allocation limit has been reached. You can display the dynamic counters of resource allocation limits configured in the Limit sections in lsb.resources. Procedure Run blimits to display the current resource usage, including any limits that may be blocking your job. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/Administer_LSF.html":{"url":"chapter6/Administer_LSF.html","title":"Chapter 6 LSF 集群维护管理","keywords":"","body":"Chapter 6 LSF 集群维护管理 了解如何管理 IBM Spectrum LSF 集群，如何控制守护程序，更改集群配置以及如何使用主机和队列。管理您的LSF 作业和作业调度策略。查看工作信息并控制工作。 了解如何为 LSF 作业配置和分配资源。 了解如何在 LSF 群集中提交，监视和控制高吞吐量和并行工作负载。 了解有关 LSF 错误和事件日志记录，以及 LSF 如何处理作业异常的信息。 调整 LSF 集群的性能和可伸缩性。 IBM Spectrum LSF 集群管理要点 了解如何管理 LSF 集群，控制守护程序，更改集群配置以及使用主机，队列和用户。 监视 IBM Spectrum LSF 集群操作和运行状况 了解如何监视集群性能，作业资源使用情况以及有关队列，作业和用户的其他信息。 管理 IBM Spectrum LSF 作业执行 了解如何管理 LSF 作业和作业调度策略。 查看作业信息，控制作业，并管理作业相关性，作业优先级，作业阵列，交互式作业，作业预处理和后处理，以及作业启动器。 配置和共享 IBM Spectrum LSF 作业资源 了解如何为 LSF 作业配置和分配资源。 在用户和项目之间公平地共享计算资源。 将资源分配限制应用于作业，管理主机和用户组，保留资源并指定作业的资源要求。 GPU 资源 了解如何为 LSF 作业配置和使用 GPU 资源。 使用 LSF 配置容器 为容器配置和使用 LSF 集成。 管理 IBM Spectrum LSF 的高吞吐量工作负载 了解如何在 LSF 集群中提交，监视和控制高吞吐量工作负载。 配置调度策略，以实现对短期作业的有效排队，调度和执行。 管理 IBM Spectrum LSF 并行工作负载 了解如何在 LSF 集群中提交，监视和控制并行工作负载。 配置保留资源的调度策略，以保证大型并行作业高效执行。 IBM Spectrum LSF 安全性 了解如何优化 LSF 集群的安全性。 IBM Spectrum LSF 高级配置 了解关于 LSF 错误和事件日志记录以及 LSF 如何处理作业异常的信息。 配置高级 LSF 功能。 IBM Spectrum LSF 性能调优 调整 LSF 集群的性能和可伸缩性。 IBM Spectrum LSF 能源感知调度 在大规模 LSF 安装中配置，管理和使用 IBM Spectrum LSF 能源感知调度功能，其中运行大型系统的能源需求，已成为这些系统总体成本的重要因素。 IBM Spectrum LSF 多集群功能 了解如何使用和管理 IBM Spectrum LSF 多集群功能，来实现跨 LSF 集群之间的资源共享。 IBM Spectrum LSF 高级版 配置和使用 IBM Spectrum LSF 高级版本（LSF Advanced Edition）。 学习使用专为具有高性能工作负载要求的大型集群，而设计的 LSF 的高级功能。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section1/Cluster management essentials.html":{"url":"chapter6/section1/Cluster management essentials.html","title":"6.1 集群管理要点","keywords":"","body":"6.1 Cluster management essentials © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section1/subsection1/Work with your cluster.html":{"url":"chapter6/section1/subsection1/Work with your cluster.html","title":"集群的使用","keywords":"","body":"集群的使用 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section1/subsection2/Working with hosts.html":{"url":"chapter6/section1/subsection2/Working with hosts.html","title":"主机节点的使用","keywords":"","body":"主机节点的使用 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section1/subsection3/Job directories and data.html":{"url":"chapter6/section1/subsection3/Job directories and data.html","title":"作业目录与数据","keywords":"","body":"作业目录与数据 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section1/subsection4/Job notification.html":{"url":"chapter6/section1/subsection4/Job notification.html","title":"作业通知","keywords":"","body":"作业通知 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/Monitoring cluster operations and health.html":{"url":"chapter6/section2/Monitoring cluster operations and health.html","title":"6.2 监视集群操作和运行状况","keywords":"","body":"6.2 Monitoring cluster operations and health © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/subsection1/Monitor cluster performance.html":{"url":"chapter6/section2/subsection1/Monitor cluster performance.html","title":"监控集群性能","keywords":"","body":"监控集群性能 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/subsection2/Monitor job information.html":{"url":"chapter6/section2/subsection2/Monitor job information.html","title":"监控作业信息","keywords":"","body":"监控作业信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/subsection3/Monitor applications by using external scripts.html":{"url":"chapter6/section2/subsection3/Monitor applications by using external scripts.html","title":"使用外部脚本监控应用","keywords":"","body":"使用外部脚本监控应用 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/subsection4/View resource information.html":{"url":"chapter6/section2/subsection4/View resource information.html","title":"查看资源信息","keywords":"","body":"查看资源信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/subsection5/View user and user group information.html":{"url":"chapter6/section2/subsection5/View user and user group information.html","title":"查看用户和用户组的信息","keywords":"","body":"查看用户和用户组的信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section2/subsection6/View queue information.html":{"url":"chapter6/section2/subsection6/View queue information.html","title":"查看队列信息","keywords":"","body":"查看队列信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/Managing job execution.html":{"url":"chapter6/section3/Managing job execution.html","title":"6.3 管理作业执行","keywords":"","body":"6.3 Managing job execution © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/subsection1/Managing job execution.html":{"url":"chapter6/section3/subsection1/Managing job execution.html","title":"管理作业执行","keywords":"","body":"管理作业执行 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/subsection2/Job file spooling.html":{"url":"chapter6/section3/subsection2/Job file spooling.html","title":"作业文件假脱机","keywords":"","body":"作业文件假脱机 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/subsection3/Job data management.html":{"url":"chapter6/section3/subsection3/Job data management.html","title":"作业数据管理","keywords":"","body":"作业数据管理 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/subsection4/Job scheduling and dispatch.html":{"url":"chapter6/section3/subsection4/Job scheduling and dispatch.html","title":"作业调度与分配","keywords":"","body":"作业调度与分配 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/subsection5/Control job execution.html":{"url":"chapter6/section3/subsection5/Control job execution.html","title":"控制作业执行","keywords":"","body":"控制作业执行 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section3/subsection6/Interactive jobs and remote tasks.html":{"url":"chapter6/section3/subsection6/Interactive jobs and remote tasks.html","title":"交互式作业和远程任务","keywords":"","body":"交互式作业和远程任务 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section4/Configuring and sharing job resources.html":{"url":"chapter6/section4/Configuring and sharing job resources.html","title":"6.4 配置和共享工作资源","keywords":"","body":"6.4 Configuring and sharing job resources © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section4/subsection1/About LSF resources.html":{"url":"chapter6/section4/subsection1/About LSF resources.html","title":"关于 LSF 资源","keywords":"","body":"关于 LSF 资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section4/subsection2/Representing job resources in LSF.html":{"url":"chapter6/section4/subsection2/Representing job resources in LSF.html","title":"在 LSF 中代表作业资源","keywords":"","body":"在 LSF 中代表作业资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section4/subsection3/Plan-based scheduling and reservations.html":{"url":"chapter6/section4/subsection3/Plan-based scheduling and reservations.html","title":"基于计划的调度与预留","keywords":"","body":"基于计划的调度与预留 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section4/subsection4/Distributing job resources to users in LSF.html":{"url":"chapter6/section4/subsection4/Distributing job resources to users in LSF.html","title":"在 LSF 中向用户分配作业资源","keywords":"","body":"在 LSF 中向用户分配作业资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section5/GPU resources.html":{"url":"chapter6/section5/GPU resources.html","title":"6.5 GPU 资源","keywords":"","body":"6.5 GPU resources © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section5/subsection1/Enabling GPU features.html":{"url":"chapter6/section5/subsection1/Enabling GPU features.html","title":"启用 GPU 资源","keywords":"","body":"启用 GPU 资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section5/subsection2/Monitoring GPU resources.html":{"url":"chapter6/section5/subsection2/Monitoring GPU resources.html","title":"监控 GPU 资源","keywords":"","body":"监控 GPU 资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section5/subsection3/Submitting and monitoring GPU jobs.html":{"url":"chapter6/section5/subsection3/Submitting and monitoring GPU jobs.html","title":"提交和监视 GPU 作业","keywords":"","body":"提交和监视 GPU 作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section5/subsection4/GPU features using ELIM.html":{"url":"chapter6/section5/subsection4/GPU features using ELIM.html","title":"使用 ELIM 的 GPU 功能","keywords":"","body":"使用 ELIM 的 GPU 功能 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section6/Configuring containers.html":{"url":"chapter6/section6/Configuring containers.html","title":"6.6 配置容器","keywords":"","body":"6.6 Configuring containers © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section6/subsection1/LSF with Docker.html":{"url":"chapter6/section6/subsection1/LSF with Docker.html","title":"LSF 与 Docker","keywords":"","body":"LSF 与 Docker © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section6/subsection2/LSF with Shifter.html":{"url":"chapter6/section6/subsection2/LSF with Shifter.html","title":"LSF 与 Shifter","keywords":"","body":"LSF 与 Shifter © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section6/subsection3/LSF with Singularity.html":{"url":"chapter6/section6/subsection3/LSF with Singularity.html","title":"LSF 与 Singularity","keywords":"","body":"LSF 与 Singularity © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section7/High throughput workload administration.html":{"url":"chapter6/section7/High throughput workload administration.html","title":"6.7 高吞吐量作业负载管理","keywords":"","body":"6.7 High throughput workload administration © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section7/subsection1/Job packs.html":{"url":"chapter6/section7/subsection1/Job packs.html","title":"作业包","keywords":"","body":"作业包 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section7/subsection2/Job arrays.html":{"url":"chapter6/section7/subsection2/Job arrays.html","title":"作业阵列","keywords":"","body":"作业阵列 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section7/subsection3/Fairshare scheduling.html":{"url":"chapter6/section7/subsection3/Fairshare scheduling.html","title":"公平共享调度","keywords":"","body":"公平共享调度 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section7/subsection4/Guaranteed resource pools.html":{"url":"chapter6/section7/subsection4/Guaranteed resource pools.html","title":"有保证的资源池","keywords":"","body":"有保证的资源池 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section7/subsection5/Reserving memory and license resources.html":{"url":"chapter6/section7/subsection5/Reserving memory and license resources.html","title":"保留内存和许可证资源","keywords":"","body":"保留内存和许可证资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/Parallel workload administration.html":{"url":"chapter6/section8/Parallel workload administration.html","title":"6.8 并行作业负载管理","keywords":"","body":"6.8 Parallel workload administration © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/subsection1/Running parallel jobs.html":{"url":"chapter6/section8/subsection1/Running parallel jobs.html","title":"运行并行作业","keywords":"","body":"运行并行作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/subsection2/Advance reservation.html":{"url":"chapter6/section8/subsection2/Advance reservation.html","title":"提前预定","keywords":"","body":"提前预定 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/subsection3/Fairshare scheduling.html":{"url":"chapter6/section8/subsection3/Fairshare scheduling.html","title":"公平共享调度","keywords":"","body":"公平共享调度 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/subsection4/Job checkpoint and restart.html":{"url":"chapter6/section8/subsection4/Job checkpoint and restart.html","title":"作业检查点与重启动","keywords":"","body":"作业检查点与重启动 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/subsection5/Job migration for checkpointable and rerunnable jobs.html":{"url":"chapter6/section8/subsection5/Job migration for checkpointable and rerunnable jobs.html","title":"可检查和可重新运行作业的作业迁移","keywords":"","body":"可检查和可重新运行作业的作业迁移 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section8/subsection6/Resizable jobs.html":{"url":"chapter6/section8/subsection6/Resizable jobs.html","title":"可调整作业","keywords":"","body":"可调整作业 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section9/Security in LSF.html":{"url":"chapter6/section9/Security in LSF.html","title":"6.9 LSF 中的安全性","keywords":"","body":"6.9 Security in LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section9/subsection1/Security considerations.html":{"url":"chapter6/section9/subsection1/Security considerations.html","title":"安全注意事项","keywords":"","body":"安全注意事项 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section9/subsection2/Secure your LSF cluster.html":{"url":"chapter6/section9/subsection2/Secure your LSF cluster.html","title":"保证 LSF 集群的安全","keywords":"","body":"保证 LSF 集群的安全 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/Advanced configuration.html":{"url":"chapter6/section10/Advanced configuration.html","title":"6.10 进阶设定","keywords":"","body":"6.10 Advanced configuration © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection1/Error and event logging.html":{"url":"chapter6/section10/subsection1/Error and event logging.html","title":"错误与事件记录","keywords":"","body":"错误与事件记录 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection2/Event generation.html":{"url":"chapter6/section10/subsection2/Event generation.html","title":"事件产生","keywords":"","body":"事件产生 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection3/Customize batch command messages.html":{"url":"chapter6/section10/subsection3/Customize batch command messages.html","title":"自定义批处理命令消息","keywords":"","body":"自定义批处理命令消息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection4/How LIM determines host models and types.html":{"url":"chapter6/section10/subsection4/How LIM determines host models and types.html","title":"LIM 如何确定主机型号与类型","keywords":"","body":"LIM 如何确定主机型号与类型 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection5/Shared file access.html":{"url":"chapter6/section10/subsection5/Shared file access.html","title":"共享文件访问","keywords":"","body":"共享文件访问 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection6/Shared configuration file content.html":{"url":"chapter6/section10/subsection6/Shared configuration file content.html","title":"共享的配置文件","keywords":"","body":"共享的配置文件 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection7/Authentication and authorization.html":{"url":"chapter6/section10/subsection7/Authentication and authorization.html","title":"认证与授权","keywords":"","body":"认证与授权 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection8/Handle job exceptions.html":{"url":"chapter6/section10/subsection8/Handle job exceptions.html","title":"处理作业异常","keywords":"","body":"处理作业异常 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection9/Tune CPU factors.html":{"url":"chapter6/section10/subsection9/Tune CPU factors.html","title":"调节 CPU 参数","keywords":"","body":"调节 CPU 参数 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection10/Set clean period for DONE jobs.html":{"url":"chapter6/section10/subsection10/Set clean period for DONE jobs.html","title":"为完成的作业设置清理周期","keywords":"","body":"为完成的作业设置清理周期 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection11/Enable host-based resources.html":{"url":"chapter6/section10/subsection11/Enable host-based resources.html","title":"启用基于主机的资源","keywords":"","body":"启用基于主机的资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection12/Global fairshare scheduling.html":{"url":"chapter6/section10/subsection12/Global fairshare scheduling.html","title":"全局公平共享调度","keywords":"","body":"全局公平共享调度 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection13/Manage LSF on EGO.html":{"url":"chapter6/section10/subsection13/Manage LSF on EGO.html","title":"在 EGO 中管理 LSF","keywords":"","body":"在 EGO 中管理 LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection14/Load sharing X applications.html":{"url":"chapter6/section10/subsection14/Load sharing X applications.html","title":"负载共享 X 应用","keywords":"","body":"负载共享 X 应用 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection15/Using LSF with the Etnus TotalView Debugger.html":{"url":"chapter6/section10/subsection15/Using LSF with the Etnus TotalView Debugger.html","title":"将 LSF 与 Etnus TotalView 调试器一起使用","keywords":"","body":"将 LSF 与 Etnus TotalView 调试器一起使用 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section10/subsection16/Register LSF host names and IP addresses to LSF servers.html":{"url":"chapter6/section10/subsection16/Register LSF host names and IP addresses to LSF servers.html","title":"将 LSF 主机名和 IP 地址注册到 LSF 服务器","keywords":"","body":"将 LSF 主机名和 IP 地址注册到 LSF 服务器 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/sectio11/Performance tuning.html":{"url":"chapter6/sectio11/Performance tuning.html","title":"6.11 性能调优","keywords":"","body":"6.11 Performance tuning © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section11/subsection1/Tune your cluster.html":{"url":"chapter6/section11/subsection1/Tune your cluster.html","title":"对集群进行调优","keywords":"","body":"对集群进行调优 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section11/subsection2/Achieve performance and scalability.html":{"url":"chapter6/section11/subsection2/Achieve performance and scalability.html","title":"实现性能和可扩展性","keywords":"","body":"实现性能和可扩展性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section12/Energy aware scheduling.html":{"url":"chapter6/section12/Energy aware scheduling.html","title":"6.12 能量感知调度","keywords":"","body":"6.12 Energy aware scheduling © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section12/subsection1/Managing host power states.html":{"url":"chapter6/section12/subsection1/Managing host power states.html","title":"管理主机电源状态","keywords":"","body":"管理主机电源状态 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section12/subsection2/CPU frequency management.html":{"url":"chapter6/section12/subsection2/CPU frequency management.html","title":"CPU 频率管理","keywords":"","body":"CPU 频率管理 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section12/subsection3/Automatic CPU frequency selection.html":{"url":"chapter6/section12/subsection3/Automatic CPU frequency selection.html","title":"自动 CPU 频率选择","keywords":"","body":"自动 CPU 频率选择 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section13/LSF multicluster capability.html":{"url":"chapter6/section13/LSF multicluster capability.html","title":"6.13 LSF 多集群功能","keywords":"","body":"6.13 LSF multicluster capability © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section13/subsection1/Overview of LSF multicluster capability.html":{"url":"chapter6/section13/subsection1/Overview of LSF multicluster capability.html","title":"LSF 多集群功能概述","keywords":"","body":"LSF 多集群功能概述 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section13/subsection2/Set up LSF multicluster capability.html":{"url":"chapter6/section13/subsection2/Set up LSF multicluster capability.html","title":"设置 LSF 多集群功能","keywords":"","body":"设置 LSF 多集群功能 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section13/subsection3/Job forwarding model.html":{"url":"chapter6/section13/subsection3/Job forwarding model.html","title":"作业转发模型","keywords":"","body":"作业转发模型 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section13/subsection4/Resource leasing model.html":{"url":"chapter6/section13/subsection4/Resource leasing model.html","title":"资源租赁模型","keywords":"","body":"资源租赁模型 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section14/LSF Advanced Edition.html":{"url":"chapter6/section14/LSF Advanced Edition.html","title":"6.14 LSF 高级版","keywords":"","body":"6.14 LSF Advanced Edition © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section14/subsection1/Overview of LSF Advanced Edition.html":{"url":"chapter6/section14/subsection1/Overview of LSF Advanced Edition.html","title":"LSF 高级版概述","keywords":"","body":"LSF 高级版概述 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section14/subsection2/Set up LSF Advanced Edition.html":{"url":"chapter6/section14/subsection2/Set up LSF Advanced Edition.html","title":"设置 LSF 高级版","keywords":"","body":"设置 LSF 高级版 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section14/subsection3/Configure LSF Advanced Edition features.html":{"url":"chapter6/section14/subsection3/Configure LSF Advanced Edition features.html","title":"配置 LSF Advanced Edition 功能","keywords":"","body":"配置 LSF Advanced Edition 功能 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section14/subsection4/Using LSF Advanced Edition.html":{"url":"chapter6/section14/subsection4/Using LSF Advanced Edition.html","title":"使用 LSF 高级版","keywords":"","body":"使用 LSF 高级版 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter6/section14/subsection5/Reference for LSF Advanced Edition.html":{"url":"chapter6/section14/subsection5/Reference for LSF Advanced Edition.html","title":"LSF 高级版参考","keywords":"","body":"LSF 高级版参考 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/Reference.html":{"url":"chapter7/Reference.html","title":"Chapter 7 参考文档","keywords":"","body":"Chapter 7 参考文档 LSF 命令和配置参数的参考信息。 IBM Spectrum LSF 命令参考 IBM Spectrum LSF命令的参考。 IBM Spectrum LSF 配置参考 了解 IBM Spectrum LSF 的相关功能，文件，事件和环境变量的配置参数。 IBM Spectrum LSF API 参考 请参阅对 IBM Spectrum LSF API 的全面参考。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/Command_reference.html":{"url":"chapter7/section1/Command_reference.html","title":"7.1 命令参考","keywords":"","body":"7.1 命令参考 IBM Spectrum LSF 命令的参考。 bacct Displays accounting statistics about finished jobs. badmin The badmin command is the administrative tool for LSF. bapp Displays information about application profile configuration. battach Runs a shell process to connect to an existing job execution host or container. battr Provides a set of subcommands to manage LSF host attributes for attribute affinity scheduling. bbot Moves a pending job to the bottom of the queue relative to the last job in the queue. bchkpnt Checkpoints one or more checkpointable jobs bclusters Displays information about IBM Spectrum LSF multicluster capability bconf Submits live reconfiguration requests, updating configuration settings in active memory without restarting daemons. bdata Provides a set of subcommands to query and manage IBM Spectrum LSF Data Manager. If no subcommands are supplied, bdata displays the command usage. bentags Queries or removes information about the energy policy tag from the mbatchd daemon, which is saved in the energy-aware scheduling database. Used with energy policy, or energy aware scheduling feature. bgadd Creates job groups bgdel Deletes job groups bgmod Modifies job groups bgpinfo Displays information about global fairshare. bhist Displays historical information about jobs bhosts Displays hosts and their static and dynamic resources bhpart Displays information about host partitions bimages Displays information on Docker container images bjdepinfo Displays job dependencies. bjgroup Displays information about job groups bjobs Displays and filters information about LSF jobs. Specify one or more job IDs (and, optionally, an array index list) to display information about specific jobs (and job arrays). bkill Sends signals to kill, suspend, or resume unfinished jobs bladmin Administrative tool for IBM Spectrum LSF License Scheduler. blaunch Launches parallel tasks on a set of hosts. blcollect License information collection daemon for LSF License Scheduler. The blcollect daemon collects license usage information. blcstat Displays dynamic update information from the blcollect daemon for LSF License Scheduler. blhosts Displays the names of all the hosts that are running the LSF License Scheduler daemon (bld). blimits Displays information about resource allocation limits of running jobs. blinfo Displays static LSF License Scheduler configuration information blkill Terminates an interactive (taskman) LSF License Scheduler task. blparams Displays information about configurable LSF License Scheduler parameters that are defined in the files lsf.licensescheduler and lsf.conf blstat Displays dynamic license information. bltasks Displays LSF License Scheduler interactive task information. blusers Displays license usage information for LSF License Scheduler. bmgroup Displays information about host groups and compute units. bmig Migrates checkpointable or rerunnable jobs. bmod Modifies job submission options of a job. bparams Displays information about configurable system parameters in the lsb.params file. bpeek Displays the stdout and stderr output of an unfinished job. bpost Sends external status messages and attaches data files to a job. bqueues Displays information about queues. bread Reads messages and attached data files from a job. brequeue Kills and requeues a job. bresize Decreases or increases tasks that are allocated to a running resizable job, or cancels pending job resize allocation requests. bresources Displays information about resource reservation, resource limits, and guaranteed resource policies. brestart Restarts checkpointed jobs. bresume Resumes one or more suspended jobs. brlainfo Displays host topology information. brsvadd Adds an advance reservation. brsvdel Deletes an advance reservation. brsvjob Shows information about jobs submitted with the brsvsub command to a specific advance reservation. brsvmod Modifies an advance reservation. brsvs Displays advance reservations. brsvsub Creates a dynamically scheduled reservation and submits a job to fill the advance reservation when the resources required by the job are available. brun Forces a job to run immediately. bsla Displays information about service classes. Service classes are used in guaranteed resource policies and service-level agreement (SLA) scheduling. bslots Displays slots available and backfill windows available for backfill jobs. bstage Stages data files for jobs with data requirements by copying files or creating symbolic links for them between the local staging cache and the job execution environment. You must run bstage only within the context of an LSF job (like blaunch). To access a file with the bstage command, you must have permission to read it. bstatus Gets current external job status or sets new job status. bstop Suspends unfinished jobs. bsub Submits a job to LSF by running the specified command and its arguments. bswitch Switches unfinished jobs from one queue to another. btop Moves a pending job relative to the first job in the queue. bugroup Displays information about user groups. busers Displays information about users and user groups. bwait Pauses and waits for the job query condition to be satisfied. ch Changes the host where subsequent commands run. gpolicyd Displays LSF global policy daemon information. lim Load information manager (LIM) daemon or service, monitoring host load. lsacct Displays accounting statistics on finished RES tasks in the LSF system. lsacctmrg Merges LSF RES task log files. lsadmin Administrative tool to control LIM and RES daemon operations in LSF. lsclusters Displays configuration information about LSF clusters. lseligible Displays whether a task is eligible for remote execution. lsfinstall The LSF installation and configuration script. lsfmon Install or uninstall LSF Monitor in an existing cluster. lsfrestart Restarts the LIM, RES, sbatchd, and mbatchd daemons on all hosts in the cluster lsfshutdown Shuts down the LIM, RES, sbatchd, and mbatchd daemons on all hosts in the cluster. lsfstartup Starts the LIM, RES, and sbatchd daemons on all hosts in the cluster. lsgrun Runs a task on a group of hosts. lshosts Displays hosts and their static resource information. lsid Displays the LSF version number, the cluster name, and the master host name. lsinfo Displays LSF configuration information. lsload Displays load information for hosts. lsloadadj Adjusts load indices on hosts. lslogin Remotely logs in to a lightly loaded host. lsltasks Displays or updates a local task list. lsmake Runs LSF make tasks in parallel. lsmon Displays load information for LSF hosts and periodically updates the display. lspasswd Registers Windows user passwords in LSF. Passwords must be 3 - 23 characters long. lsplace Displays hosts available to run tasks. lsportcheck Displays ports that LSF is currently using or the LSF ports that will be used before starting LSF. lsrcp Remotely copies files through LSF. lsreghost (UNIX) UNIX version of the lsreghost command registers UNIX LSF host names and IP addresses with LSF servers so that LSF servers can internally resolve these hosts without requiring a DNS server. lsreghost (Windows) Windows version of the lsreghost command registers Windows LSF host names and IP addresses with LSF servers so that LSF servers can internally resolve these hosts without requiring a DNS server. lsrtasks Displays or updates a remote task list. lsrun Runs an interactive task through LSF. lstcsh Load sharing tcsh for LSF pam Parallel Application Manager – job starter for MPI applications patchinstall UNIX only. Manage patches in LSF cluster. pversions (UNIX) UNIX version of the command. Displays the version information for IBM Spectrum LSF installed on UNIX hosts. pversions (Windows) Windows version of the command. Displays the version information for IBM Spectrum LSF installed on a Windows host. ssacct Displays accounting statistics about finished LSF session scheduler jobs. ssched Submit tasks through LSF session scheduler. taskman Checks out a license token and manages interactive UNIX applications. tspeek Displays the stdout and stderr output of an unfinished Terminal Services job. tssub Submits a Terminal Services job to LSF. wgpasswd Changes a user’s password for a Microsoft Windows workgroup. wguser Modifies user accounts for a Microsoft Windows workgroup © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection1/bdata.html":{"url":"chapter7/section1/subsection1/bdata.html","title":"bdata","keywords":"","body":"bdata Provides a set of subcommands to query and manage IBM Spectrum LSF Data Manager. If no subcommands are supplied, bdata displays the command usage. Synopsis LSF data management subcommands and option syntax synopsis Subcommands LSF data management subcommands and options Help and version options IBM Spectrum LSF Data Manager help and version display options See also bhist, bjobs, bmod, bstage, bsub, lsf.conf, lsf.datamanager © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection1/Synopsis.html":{"url":"chapter7/section1/subsection1/Synopsis.html","title":"Synopsis","keywords":"","body":"Synopsis LSF data management subcommands and option syntax synopsis bdata subcommand options bdata [-h[elp] | -V] © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection1/Subcommands.html":{"url":"chapter7/section1/subsection1/Subcommands.html","title":"Subcommands","keywords":"","body":"Subcommands LSF data management subcommands and options File-based cache query: bdata cache [-w | -l] [-u all | -u user_name] [-g all | -g user_group_name] [-dmd cluster_name] [host_name**:*]abs_file_path* Job-based cache query: bdata cache [-dmd cluster_name] [-w | -l] job_ID[@*cluster_name*] Job-based cache query: bdata cache [-dmd cluster_name] [-w | -l] job_ID[@*cluster_name*] Change group after stage out: bdata chgrp [-dmd cluster_name] -g user_group_name [host_name**:*]abs_file_path* Change group after stage in: bdata chgrp [-dmd cluster_name] -g user_group_name -tag tag_name Change the file permission mode of a file: bdata chmod [-dmd cluster_name] -mode octal_mode -mode [host_name**:*]abs_file_path* Tag query: bdata tags list [-w] [-u all | -u user_name] [-dmd cluster_name] Tag cleanup: bdata tags clean [-u user_name] [-dmd cluster_name] tag_name Effective configuration query: bdata showconf Connections query: bdata connections [-w] Administration - reconfigure and shut down LSF data manager: bdata admin reconfig bdata admin shutdown [host_name] Common options -w Wide format. Displays the information in a wide format. Use this option only with the cache, connections, and tags list subcommands. -l Long format. Displays additional information about LSF data management files. Use this option only with the cache subcommand. -dmd cluster_name Query the LSF data manager corresponding to the specified remote cluster. Use this option only with the cache, and tags subcommands. -u all | -u user_name Query files in the cache for the specified user. Use -u all or -u user_name with file-based bdata cache and bdata tags list. You can use only -u user_name with bdata tags clean. cache Queries the LSF data management cache. chgrp Changes the user group of a file in the LSF data management cache. chmod Changes the file permission mode of a file that is staged in to the LSF data management cache. tags LSF data management tag query and cleanup options showconf LSF data management effective configuration query option connections Query LSF data management connections. Lists currently connected mbatchd daemons, with master LSF data manager host names, their status, and the outgoing and incoming connections for remote LSF data managers. admin Reconfigure and shut down the LSF data manager daemon (dmd). © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection1/Help and version display.html":{"url":"chapter7/section1/subsection1/Help and version display.html","title":"Help and version display","keywords":"","body":"Help and version display IBM Spectrum LSF Data Manager help and version display options bdata [-h[elp] | -V] -h[help] Displays the command usage of the bdata command to stderr and exits. -V Prints the IBM Spectrum LSF Data Manager release version to stderr and exits. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection1/See also.html":{"url":"chapter7/section1/subsection1/See also.html","title":"See also","keywords":"","body":"See also bhist, bjobs, bmod, bstage, bsub, lsf.conf, lsf.datamanager © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/bjobs.html":{"url":"chapter7/section1/subsection2/bjobs.html","title":"bjobs","keywords":"","body":"bjobs © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/Categories.html":{"url":"chapter7/section1/subsection2/Categories.html","title":"Categories","keywords":"","body":"Categories © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/Categories_filter.html":{"url":"chapter7/section1/subsection2/Categories_filter.html","title":"Category: filter","keywords":"","body":"Category: filter © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/Categories_format.html":{"url":"chapter7/section1/subsection2/Categories_format.html","title":"Category: format","keywords":"","body":"Category: format © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/Categories_state.html":{"url":"chapter7/section1/subsection2/Categories_state.html","title":"Category: state","keywords":"","body":"Category: state © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/Options.html":{"url":"chapter7/section1/subsection2/Options.html","title":"Options","keywords":"","body":"Options © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection2/Description.html":{"url":"chapter7/section1/subsection2/Description.html","title":"Description","keywords":"","body":"Description © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection3/bstage.html":{"url":"chapter7/section1/subsection3/bstage.html","title":"bstage","keywords":"","body":"bstage Stages data files for jobs with data requirements by copying files or creating symbolic links for them between the local staging cache and the job execution environment. You must run bstage only within the context of an LSF job (like blaunch). To access a file with the bstage command, you must have permission to read it. bstage in Stages in data files for jobs with data requirements. bstage copies or symbolically links files from the data manager staging area to the job execution host. bstage out Stages out data files for jobs with data requirements. The bstage command copies or creates symbolic links to files from the job current working directory to the data management cache, then requests a transfer job to copy the file or folder to a host. Help and version options IBM Spectrum LSF Data Manager help and version display options See also bdata, bhist, bjobs, bmod, bsub, lsf.conf, lsf.datamanager © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection3/bstage_in.html":{"url":"chapter7/section1/subsection3/bstage_in.html","title":"bstage in","keywords":"","body":"bstage in © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection3/bstage_out.html":{"url":"chapter7/section1/subsection3/bstage_out.html","title":"bstage out","keywords":"","body":"bstage out © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection3/Help and version display.html":{"url":"chapter7/section1/subsection3/Help and version display.html","title":"Help and version display","keywords":"","body":"Help and version display © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection3/See also.html":{"url":"chapter7/section1/subsection3/See also.html","title":"See also","keywords":"","body":"See also © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection4/bsub.html":{"url":"chapter7/section1/subsection4/bsub.html","title":"bsub","keywords":"","body":"bsub © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection4/Categories.html":{"url":"chapter7/section1/subsection4/Categories.html","title":"Categories","keywords":"","body":"Categories © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection4/Options.html":{"url":"chapter7/section1/subsection4/Options.html","title":"Options","keywords":"","body":"Options © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section1/subsection4/Description.html":{"url":"chapter7/section1/subsection4/Description.html","title":"Description","keywords":"","body":"Description © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/Configuration_reference.html":{"url":"chapter7/section2/Configuration_reference.html","title":"7.2 配置参考","keywords":"","body":"7.2 配置参考 了解有关 IBM Spectrum LSF 功能，文件，事件和环境变量的配置参数。 配置文件 LSF 配置文件参考。 环境变量 了解如何为作业执行，作业调整大小通知命令和会话调度程序（ssched）设置 LSF 环境变量。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection1/Configuration_files.html":{"url":"chapter7/section2/subsection1/Configuration_files.html","title":"配置文件","keywords":"","body":"配置文件 LSF 配置文件参考。 重要 在所有配置文件中，以所有大写字母指定任何域名。 cshrc.lsf and profile.lsf 用户环境 shell 文件 cshrc.lsf 和 profile.lsf 在 LSF 主机上设置 LSF 操作环境。 hosts For hosts with multiple IP addresses and different official host names configured at the system level, this file associates the host names and IP addresses in LSF. install.config The install.config file contains options for LSF installation and configuration. Use the lsfinstall -f install.config command to install LSF with the options that are specified in the install.config file. lim.acct The lim.acct file is the log file for the LSF Load Information Manager (LIM). Produced by the lsmon command, the lim.acct file contains host load information that is collected and distributed by LIM. lsb.acct The lsb.acct file is the batch job log file of LSF. lsb.applications The lsb.applications file defines application profiles. Use application profiles to define common parameters for the same type of jobs, including the execution requirements of the applications, the resources they require, and how they should be run and managed. lsb.events The LSF batch event log file lsb.events is used to display LSF batch event history and for mbatchd failure recovery. lsb.globalpolicies This configuration file defines global policies for multiple clusters. lsb.hosts lsb.modules The lsb.modules file contains configuration information for LSF scheduler and resource broker modules. The file contains only one section, named PluginModule. lsb.params lsb.queues The lsb.queues file defines batch queues. Numerous controls are available at the queue level to allow cluster administrators to customize site policies. lsb.reasons lsb.resources The lsb.resources file contains configuration information for resource allocation limits, exports, resource usage limits, and guarantee policies. This file is optional. lsb.serviceclasses lsb.threshold The lsb.threshold configuration file defines energy-saving and CPU frequency policies. This file is optional. lsb.users lsf.acct lsf.cluster The cluster configuration file. There is one file for each cluster, called lsf.cluster.cluster_name. The cluster_name suffix is the name of the cluster defined in the Cluster section of the lsf.shared file. All LSF hosts are listed in this file, along with the list of LSF administrators and the installed LSF features. lsf.conf The lsf.conf file controls the operation of LSF. The lsf.conf file is created during installation and records all the settings chosen when LSF was installed. The lsf.conf file dictates the location of the specific configuration files and operation of individual servers and applications. lsf.datamanager The lsf.datamanager file controls the operation of IBM Spectrum LSF Data Manager features. Each cluster has one LSF data management configuration file, called lsf.datamanager.cluster_name. The cluster_name suffix is the name of the cluster that is defined in the Cluster section of the lsf.shared file. The file is read by the LSF data management daemon dmd. Since one LSF data manager can serve multiple LSF clusters, the contents of this file must be identical on each cluster that shares LSF data manager. lsf.licensescheduler The lsf.licensescheduler file contains LSF License Scheduler configuration information. All sections except ProjectGroup are required. In cluster mode, the Project section is also not required. lsf.shared The lsf.shared file contains common definitions that are shared by all load sharing clusters defined by lsf.cluster.cluster_name files. lsf.sudoers lsf.task setup.config slave.config The slave.config file contains options for installing and configuring a server host that can be dynamically added or removed. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection2/Environment_variables.html":{"url":"chapter7/section2/subsection2/Environment_variables.html","title":"环境变量","keywords":"","body":"环境变量 了解如何为作业执行，作业调整大小通知命令和会话调度程序（ssched）设置 LSF 环境变量。 设置用于作业执行的环境变量 除了从用户环境继承的环境变量外，LSF 还为批处理作业设置了其他几个环境变量。 用于调整大小的通知命令的环境变量 会话调度程序的环境变量（ssched） 用于数据来源的环境变量 环境变量参考 LSF 环境变量的参考。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection2/Environment variables set for job execution.html":{"url":"chapter7/section2/subsection2/Environment variables set for job execution.html","title":"为作业执行而设置的环境变量","keywords":"","body":"Environment variables set for job execution © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection2/Environment variables for resize notification command.html":{"url":"chapter7/section2/subsection2/Environment variables for resize notification command.html","title":"调整大小通知命令的环境变量","keywords":"","body":"Environment variables for resize notification command © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection2/Environment variables for session scheduler.html":{"url":"chapter7/section2/subsection2/Environment variables for session scheduler.html","title":"会话调度程序的环境变量","keywords":"","body":"Environment variables for session scheduler (ssched) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection2/Environment variables for data provenance.html":{"url":"chapter7/section2/subsection2/Environment variables for data provenance.html","title":"用于数据来源的环境变量","keywords":"","body":"Environment variables for data provenance © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section2/subsection2/Environment variable reference.html":{"url":"chapter7/section2/subsection2/Environment variable reference.html","title":"环境变量参考","keywords":"","body":"Environment variable reference © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter7/section3/API_reference.html":{"url":"chapter7/section3/API_reference.html","title":"7.3 API 参考","keywords":"","body":"7.3 API 参考 请参阅对 IBM Spectrum LSF API 的全面参考。 LSF API reference © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter8/Extend_LSF.html":{"url":"chapter8/Extend_LSF.html","title":" Chapter 8 LSF 拓展","keywords":"","body":"Chapter 8 LSF 拓展 配置和使用 IBM Spectrum LSF 集成。 IBM Spectrum LSF 会话调度程序 IBM Spectrum LSF Session Scheduler 安装，管理和使用 IBM Spectrum LSF Session Scheduler。 通过使用作业级任务调度程序，在单个 LSF 作业的分配范围内，运行大量短期任务的集合，该任务级任务调度程序为该作业分配一次资源，并为每个任务重用分配的资源。 IBM Spectrum LSF Session Scheduler 是运行短作业的理想选择，无论它们是任务列表还是带有参数执行的作业阵列。 带有 IBM Rational ClearCase 的 IBM Spectrum LSF IBM Spectrum LSF with IBM Rational ClearCase 许多站点使用 IBM®Rational®ClearCase®（ClearCase）环境进行修订版源控制和开发。 了解如何通过 IBM Rational ClearCase软件，来安装，配置和使用 IBM Spectrum LSF。 Cray Linux上的IBM Spectrum LSF IBM Spectrum LSF on Cray Linux IBM Spectrum LSF 与 Cray Linux 的集成适用于 LSF 版本8.0或更高版本，并支持与 Cray Linux Environment 4.0或更高版本的集成。 您必须有 LSF Standard Edition 或 LSF Advanced Edition。 LSF Express Edition不支持Cray Linux集成。 带有Apache Spark的IBM Spectrum LSF IBM Spectrum LSF with Apache Spark 在 Apache Spark 应用程序中配置和使用 IBM Spectrum LSF。 带有Apache Hadoop的IBM Spectrum LSF IBM Spectrum LSF with Apache Hadoop 在 Apache Hadoop 应用程序中，使用 IBM Spectrum LSF。 带有IBM Cluster Systems Manager的IBM Spectrum LSF IBM Spectrum LSF with IBM Cluster Systems Manager IBM Cluster Systems Manager（CSM）是一种系统管理工具，旨在对分布式和集群化的 IBM Power Systems 进行简单，低成本的管理。 了解如何在IBM Cluster Systems Manager中安装，配置和使用IBM Spectrum LSF。 具有 IBM Cloud Private 的 IBM Spectrum LSF IBM Spectrum LSF with IBM Cloud Private 可变地使用许可，用于扩展到云的动态计算工作负载，使您可以通过具有成本效益的按需购买即用许可，来优化基于云的资源使用。 管理员可以配置 IBM Spectrum LSF 集群，以通过外部负载索引监视器（ELIM）将CPU内核，CPU插槽，GPU插槽和主机计数，上载到 IBM Cloud Private 中的计量服务。 然后，管理员可以从IBM Cloud Private中的计量仪表板审核IBM Spectrum LSF资源使用情况。 LSF 作业步骤管理器 LSF Job Step Manager 使用JSDL提交作业 Submitting jobs using JSDL IBM Spectrum LSF模拟器 IBM Spectrum LSF Simulator 使用 LSF Simulator 通过在单独的内部环境中，模拟 LSF 集群来分析和调整 LSF 配置。 使用 LSF Simulator，您可以在不中断 LSF 生产环境的情况下使用不同的工作负载跟踪运行实验。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter9/Best_practices_and_tips.html":{"url":"chapter9/Best_practices_and_tips.html","title":"Chapter 9 最佳实践与建议","keywords":"","body":"Chapter 9 经验与建议总结 查看使用 LSF 的各种最佳实践和技巧。 会计文件管理 Accounting file management 将 CPU 分配为并行作业的块 Allocating CPUs as blocks for parallel jobs 并行作业始终要求运行多个CPU。 如果可以将分配的CPU分配为块，则某些作业可以运行得更快。 清理并行作业执行问题 Cleaning up parallel job execution problems 将 IBM Aspera 配置为数据传输工具 Configuring IBM Aspera as a data transfer tool IBM Aspera 是一种数据传输工具，可以在高延迟网络中高效，基于策略地使用网络带宽。 自定义作业查询输出格式 Customizing job query output format 定义基于主机的外部资源 Defining external host-based resources 加强作业内存并与 Linux cgroup 交换 Enforcing job memory and swap with Linux cgroups 作业访问控制 Job access control 将IBM Spectrum LSF与Andrew File System（AFS）结合使用 Using IBM Spectrum LSF with Andrew File System (AFS) 了解 LSF 如何与 Andrew File System（AFS）集成，以便您可以配置 LSF 以适合您的需求。 维持集群性能 Maintaining cluster performance 在 LSF 中管理浮动软件许可证 Managing floating software licenses in LSF 通常，浮动软件许可证池由 LSF 中的数字资源表示。 每个需要许可证的工作都必须在其 rusag 表达式中包括许可证要求，以确保在分发工作时为该工作释放了足够的许可证。 在启用 CPU 频率调节器的情况下优化 LSF 作业处理 Optimizing LSF job processing with CPU frequency governors enabled Oracle Solaris 和 IBM AIX 上的操作系统分区和虚拟化 Operating system partitioning and virtualization on Oracle Solaris and IBM AIX 本文介绍了 LSF 在 OS 分区和虚拟化环境中的工作方式，重点是 Oracle Solaris 容器和 IBM AIX 分区。 基于主机的可用作业位置放置作业 Placing jobs based on available job slots of hosts 运行 checksum 以验证安装映像 Running checksum to verify installation images 跟踪作业依赖性 Tracking job dependencies 了解 mbatchd 的性能指标 Understanding mbatchd performance metrics 使用计算单元进行拓扑调度 Using compute units for topology scheduling 使用作业目录 Using job directories 使用 lsmake 加速 Android 构建 Using lsmake to accelerate Android builds 将 NVIDIA DGX 系统与 LSF 一起使用 Using NVIDIA DGX systems with LSF 将 ssh X11 转发与 IBM Spectrum LSF 一起使用 Using ssh X11 forwarding with IBM Spectrum LSF 为了使启用 X 的应用程序能够按预期运行，必须通过 ssh 来建立 X 连接，这是一种安全的方法。 为LSF API 使用 Python wrapper Using the Python wrapper for LSF API © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:35 "},"chapter10/LSF License Scheduler.html":{"url":"chapter10/LSF License Scheduler.html","title":"Chapter 10 LSF 许可证调度程序","keywords":"","body":"Chapter 10 LSF 许可证调度程序 安装，配置和使用 IBM Spectrum LSF License Scheduler（LSF License Scheduler）。 了解制定策略，来控制组织中不同用户或项目之间，如何共享软件应用程序许可证。 介绍 安装与启动许可证调度程序 LSF 许可证调度器相关概念 配置许可证调度器 查询信息与错误排查 参考 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:28 "},"chapter10/section1/Introduction.html":{"url":"chapter10/section1/Introduction.html","title":"介绍","keywords":"","body":"介绍 概览 LSF License Scheduler版本之间的差异 LSF License Scheduler有两个版本：基本版和标准版。 词汇表 架构 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section1/Overview.html":{"url":"chapter10/section1/Overview.html","title":"概览","keywords":"","body":"概览 将策略应用于共享许可证的方式，既可以提高生产率，又可以节省成本。 更轻松地共享许可证 IBM Spectrum LSF License Scheduler（LSF License Scheduler）使在同一设计中心或全球范围内的项目团队与部门之间共享许可证变得容易。 借助分配和监视许可证的工具，许可证所有者可以共享未使用的许可证，同时仍可确保在需要时立即访问许可证。 通过更有效的共享，所有用户都能感受到更大的许可证池。 确保适当的许可证分配 License Scheduler 启用了灵活的分层共享策略，以反映业务需求。 在安静期间，当没有争用许可证时，可以将许可证分配给需要许可证的任何人，以保持较高的利用率和吞吐量。 在繁忙时期，可以根据对大多数时间或收入至关重要的项目的策略分配许可证的供应。 提高服务水平和生产力 通过确保访问许可证的最小份额，并根据需要在集群之间进行灵活分配，可以更轻松地获得许可证，并且作业等待队列中等待许可证资源的可能性也较小。 这意味着减少了等待时间，提高了生产率，并有助于更快，更高效的设计环境。 减少或避免成本 通过能够为最关键的项目分配稀缺的许可证，并能够在集群资源，用户和项目的上下文中分析许可证的使用情况，计划人员可以更好地发现和消除瓶颈，从而使他们现有的许可证更具生产力。 通过更好地了解许可证的使用方式，他们可以更有效地计划许可证需求，最终帮助控制成本并提高生产率。 许可证计划程序控制组织中的软件许可证共享。 License Scheduler 与 FlexNet™ 和 Reprise License Manager™ 产品配合使用，以控制和监视许可证的使用。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section1/Differences between LSF License Scheduler editions.html":{"url":"chapter10/section1/Differences between LSF License Scheduler editions.html","title":"LSF License Scheduler 版本之间的差异","keywords":"","body":"LSF License Scheduler 版本之间的差异 LSF License Scheduler 有两个版本：基本版和标准版。 LSF Standard Scheduler 和 Advanced Edition 随附了 LSF License Scheduler Basic Edition，并不旨在应用有关如何在集群或项目之间共享许可证的策略。 相反，LSF License Scheduler Basic Edition 旨在代替外部负载信息管理器（elim），以收集由 FlexNet 或 Reprise License Manager 管理的许可证的外部负载指示。为了替代此限制，LSF License Scheduler Basic Edition 限制了单个集群的作业的许可证使用，以防止许可证的过度使用，并通过将许可证签出与这些作业匹配，来跟踪单个作业的许可证使用。 LSF License Scheduler Standard Edition 不仅提供单个集群的集群模式功能，而且还提供完整的 LSF License Scheduler 功能，包括对所有模式（集群模式，项目模式和快速分发项目模式）的支持，多个集群，功能和特性。 组，每个许可证功能有多个服务域。 LSF License Scheduler Standard Edition 还支持 taskman 作业，以及 LSF Advanced Edition（LSF Advanced Edition）中的 LSF/XL 功能。 重要 现在需要 LSF License Scheduler 授权文件（ls.entitlement）才能运行 LSF License Scheduler Standard Edition。 在启动 LSF License Scheduler 作为标准版运行之前，将授权文件（ls.entitlement）复制到 $LSF_ENVDIR 目录。 要安装和运行 LSF License Scheduler Basic Edition，请按照 Install License Scheduler 中的说明，下载并安装LSF License Scheduler 软件包。但请按照安装和配置 LSF License Scheduler Basic Edition（而非 Standard Edition）的所有特定步骤进行操作。 所有 LSF License Scheduler 文档均假定使用 LSF License Scheduler Standard Edition，除非明确指明它用了 LSF License Scheduler Basic Edition。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section1/Glossary.html":{"url":"chapter10/section1/Glossary.html","title":"词汇表","keywords":"","body":"词汇表 blcollect 许可证调度程序守护程序，用于查询许可软件以获取许可证使用情况。 blcollect 从许可证管理器收集信息。 您可以通过在多个 UNIX 主机上运行许可证信息收集守护程序来分散许可证收集的负载。 它也称为收集器。 bld 许可证调度程序批处理守护程序。 cluster mode 许可证令牌由许可证调度程序分配给集群，每个集群内的作业调度由本地 mbatchd 管理。 集群模式仅适用于 License Scheduler 8.0 及更高版本。 每个许可证功能都可以使用集群模式或项目模式，但不能同时使用两者。 lmgrd 主要的 FlexNet 许可守护程序。 通常在 License Scheduler 中分为服务域。 project mode 许可证令牌，由许可证计划程序分配给项目，并且许可证项目的作业计划，在遵循为每个项目配置的许可证分配策略的集群之间进行。 对应于 License Scheduler 版本 7.0.5 及更早版本。 每个许可证功能都可以使用集群模式或项目模式，但不能同时使用两者。 service domain 一组一个或多个 FlexNet 许可证服务器。 您可以使用为网络提供许可证的许可证服务器名称和端口号来配置服务域。 taskman job 由 LSF 之外的 IBM Spectrum LSF（LSF）任务管理器（taskman）工具运行，但由许可计划程序调度的作业。 token 一个许可证令牌代表一个实际的许可证，许可证调度程序使用它来跟踪许可证的使用，并确定接下来要分发的作业。 License Scheduler 管理许可证令牌，而不是直接控制许可证。 他们保留许可证令牌后，将调度作业，然后启动需要许可证的应用程序。 LSF 可用的令牌数量与许可证服务器可用的许可证数量相对应，因此，如果令牌不可用，则不会分派作业。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section1/Architecture.html":{"url":"chapter10/section1/Architecture.html","title":"架构","keywords":"","body":"架构 LSF License Scheduler 管理许可证令牌，而不是直接控制许可证。 使用 LSF License Scheduler，作业在启动应用程序之前会收到许可证令牌。 IBM Spectrum LSF（LSF）和 IBM Spectrum LSF Advanced Edition（LSF Advanced Edition）可用的令牌数量，与许可证服务器上可用的许可证数量相对应，因此，如果令牌不可用，则作业不会启动。 这样，正在运行的作业所请求的许可证数量，不会超过可用许可证的数量。 作业开始时，应用程序不知道 LSF License Scheduler。 应用程序以通常的方式从许可证服务器中签出许可证。 图 1. 守护程序交互 调度策略如何工作 使用 LSF License Scheduler，LSF 可以收集有关待处理作业的许可要求的信息，以有效地分发可用的许可。 其他 LSF 调度策略独立于 LSF License Scheduler 策略。 开始作业时，基本的 LSF 调度排在第一位。 LSF License Scheduler 对作业调度优先级没有影响。 根据每个集群 中配置的优先级策略，将作业视为要分发。 例如，在 LSF License Scheduler 公平共享策略（此作业所属的许可证项目）适用之前，作业必须具有要在其上启动的候选 LSF 主机。 其他 LSF Fairshare 策略基于 CPU 时间，运行时间和使用情况。 如果配置了 LSF Fairshare 调度，则 LSF 确定哪个用户或队列具有最高优先级，然后考虑其他资源。 这样，其他 LSF Fairshare 策略的优先级高于 LSF License Scheduler。 当 mbatchd 脱机时 当集群运行时，mbatchd 保持与 bld 的 TCP 连接。 当集群断开连接时（例如，集群关闭或重新启动时），bld 会删除有关集群中作业的所有信息。 LSF 许可证计划程序将断开连接的集群中的作业签出的许可证，视为非 LSF 许可证使用。 当 mbatchd 重新联机时，bld 会立即收到有关当前分配给集群的令牌数量的更新信息。 当 bld 脱机时 如果 mbatchd 失去了与 bld 的连接，则 mbatchd 无法获得 bld 的令牌分配决定来更新自己的令牌。 但是，由于mbatchd 每分钟都会在 $LSF_TOP/work/data/featureName.ServiceDomainName.dat 文件中记录令牌状态，因此，如果连接断开，则 mbatchd 将使用最后记录的信息来调度作业。 f3.LanServer1.dat # f3 LanServer1 3 2 # p1 50 p2 50 12/3 14:20:38 2 0 2 0 1 0 1 0 12/3 14:21:39 2 0 2 0 1 0 1 0 12/3 14:22:40 3 3 0 0 0 0 0 0 12/3 14:23:41 3 3 0 0 0 0 0 0 12/3 14:24:42 1 0 1 0 2 0 2 0 12/3 14:25:43 1 0 1 0 2 0 2 0 12/3 14:26:44 1 0 1 0 2 0 2 0 12/3 14:27:55 1 0 1 0 2 0 2 0 LanServer1 上的 f3 具有三个令牌和两个项目。 项目 p1 和 p2 共享许可证 50:50。 在 14:27:55，bld 向 p1 分配了一个令牌，该令牌有 0 个正在使用，1 个免费，0 个备用。 同时，bld 向 p2 分配了两个令牌，这些令牌有 0 个正在使用，2 个空闲和 0 个保留。 mbatchd 将继续基于基于 14:27:55 记录的令牌分布的作业进行调度，直到与 bld 的连接重新建立。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/Installing and starting License Scheduler.html":{"url":"chapter10/section2/Installing and starting License Scheduler.html","title":"安装和启动许可证调度程序","keywords":"","body":"安装和启动许可证调度程序 安装许可证计划程序 启动许可证计划程序 许可证调度程序中的 LSF 参数 关于提交工作 配置更改之后 将集群添加到 License Scheduler 配置多个管理员 升级许可证计划程序 防火墙 LSF，许可证调度程序和 taskman 互操作性的配置。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/subsection1/Install License Scheduler.html":{"url":"chapter10/section2/subsection1/Install License Scheduler.html","title":"安装 License Scheduler","keywords":"","body":"Install License Scheduler Perform the pre-installation steps. Choose an installation plan: UNIX: License Scheduler manages licenses for jobs that run through LSF and through applications other than LSF. Windows, in a mixed cluster: A Licenser Scheduler installation requires UNIX hosts to run the bld. Windows hosts in a mixed cluster can run License Scheduler commands. When you have License Scheduler UNIX hosts working with LSF, run License Scheduler on Windows hosts as well. Before you install What the License Scheduler setup script does Install License Scheduler with LSF (UNIX) Install License Scheduler on Windows You can install License Scheduler on Windows hosts when your cluster includes both Windows and UNIX hosts. Troubleshoot Configure LSF License Scheduler Basic Edition Configure LSF and LSF License Scheduler Basic Edition. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:30 "},"chapter10/section2/subsection1/Before you install.html":{"url":"chapter10/section2/subsection1/Before you install.html","title":"安装之前","keywords":"","body":"Before you install About this task LSF must be installed and running before you install License Scheduler. Procedure Log on to any LSF host as root and use lsid to make sure that the cluster is running. If you see the message \"Cannot open lsf.conf file\", verify that the $LSF_ENVDIR environment variable is set correctly. To set your LSF environment: For csh or tcsh: % source LSF_TOP/conf/cshrc.lsf For sh, ksh, or bash: $ . LSF_TOP/conf/profile.lsf © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/subsection1/What the License Scheduler setup script does.html":{"url":"chapter10/section2/subsection1/What the License Scheduler setup script does.html","title":"License Scheduler 设置脚本的作用","keywords":"","body":"What the License Scheduler setup script does Finds the appropriate lsf.conf for the running cluster. Copies the License Scheduler files to your LSF directories: $LSF_ENVDIR: lsf.licensescheduler ls.users $LSF_SERVERDIR: bld blcollect globauth esub.ls_auth $LSF_BINDIR: blstat blcstat blusers blinfo bladmin blstartup blhosts blkill bltasks blparams taskman $LSF_LIBDIR: libglb.a libglb.so liblic.so $LSF_MANDIR: various man pages Finds the appropriate lsf.cluster.cluster_name file for the running cluster. Creates the following additional directories: $LSB_SHAREDIR/cluster_name/db $LSB_SHAREDIR/cluster_name/data Sets your License Scheduler administrators list in the lsf.licensescheduler file. Configures LSF to use License Scheduler. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:31 "},"chapter10/section2/subsection1/Install License Scheduler with LSF UNIX.html":{"url":"chapter10/section2/subsection1/Install License Scheduler with LSF UNIX.html","title":"使用 LSF (UNIX) 安装 License Scheduler","keywords":"","body":"Install License Scheduler with LSF (UNIX) Before you begin You must have write access to the LSF_TOP directories. Procedure Log on as root to the installation file server host. Download, uncompress, and extract the LSF License Scheduler packages for the platforms you need. For example, for x86 64-bit systems that run Linux kernel 2.6.x and compiled with glibc 2.3.x, download lsf10.1_licsched_lnx26-x64.tar.Z. Extract the distribution file. For example: # zcat lsf10.1_licsched_lnx26-x64.tar.Z | tar xvf - Change to the extracted distribution directory. For example: # cd lsf10.1_licsched_linux2.6-glibc2.3-x86_64 Run the setup script as root: # ./setup Note If you are installing License Scheduler into a non-LSF environment or doing a silent install, edit ./setup.config prior to your installation. Enter y (yes) to confirm that the path to lsf.conf is correct. To enter a path to a different lsf.conf, type n (no) and specify the full path to the lsf.conf file you want to use. Enter y to confirm that the path to lsf.cluster.cluster_name is correct. To enter a path to a different lsf.cluster.cluster_name file, type n (no) and specify the full path to the lsf.cluster.cluster_name file you want to use. Enter y to confirm that you want to use the LSF Administrators list for License Scheduler with LSF. To enter a different list of administrators for License Scheduler, enter a space-separated list of administrator user names. You can change your License Scheduler administrators list later, if necessary. If you are installing LSF License Scheduler Standard Edition, copy the LSF License Scheduler entitlement file (ls.entitlement) to the $LSF_ENVDIR directory. If you do not copy the entitlement file to $LSF_ENVDIR before starting LSF License Scheduler, LSF License Scheduler runs as Basic Edition. What to do next Start License Scheduler Note Before starting LSF License Scheduler, if you are installing LSF License Scheduler Basic Edition, configure License Scheduler Basic Edition and LSF as described in Configure License Scheduler Basic Edition . If you are installing LSF License Scheduler Standard Edition, configure LSF License Scheduler Standard Edition and LSF as described in Configuring License Scheduler. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/subsection1/Install License Scheduler on Windows.html":{"url":"chapter10/section2/subsection1/Install License Scheduler on Windows.html","title":"在 Windows 上安装 License Scheduler","keywords":"","body":"Install License Scheduler on Windows You can install License Scheduler on Windows hosts when your cluster includes both Windows and UNIX hosts. The License Scheduler Windows Client package includes: README Commands: blstat.exe blcstat.exe blinfo.exe blusers.exe bladmin.exe blhosts.exe blkill.exe bltasks.exe blparams.exe taskman.exe lsf.licensescheduler: License Scheduler configuration file lsf.conf: LSF configuration file Install License Scheduler with LSF (Windows) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/subsection1/Install License Scheduler with LSF Windows.html":{"url":"chapter10/section2/subsection1/Install License Scheduler with LSF Windows.html","title":"使用 LSF (Windows) 安装 License Scheduler","keywords":"","body":"Install License Scheduler with LSF (Windows) Before you begin You must already have LSF installed on all Windows hosts you intend to install License Scheduler on. About this task This installation option means that License Scheduler manages licenses for jobs that are submitted through LSF and through any other applications. Install License Scheduler on Windows hosts only when your LSF cluster includes both UNIX and Windows hosts. Procedure Download the License Scheduler Client for Windows package. Copy all commands to $LSF_BINDIR (the bin subdirectory in your LSF installation directory) on your Windows hosts. Copy lsf.licensescheduler to $LSF_ENVDIR. Edit lsf.licensescheduler to suit your License Scheduler Master host configuration. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:30 "},"chapter10/section2/subsection1/Troubleshoot.html":{"url":"chapter10/section2/subsection1/Troubleshoot.html","title":"故障排除","keywords":"","body":"Troubleshoot Procedure If you receive the following message, configure your Windows host name and IP address in the /etc/hosts file on the master host: Failed in an LSF library call: Failed in sending/receiving a message: error 0: The operation completed successfully. To enable the blhosts command, make sure that your Windows host can resolve the master host IP address correctly. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:30 "},"chapter10/section2/subsection1/Configure LSF License Scheduler Basic Edition.html":{"url":"chapter10/section2/subsection1/Configure LSF License Scheduler Basic Edition.html","title":"配置 LSF License Scheduler 基础版","keywords":"","body":"Configure LSF License Scheduler Basic Edition Configure LSF and LSF License Scheduler Basic Edition. Parent topic: Install License Scheduler Configuring LSF License Scheduler Basic Edition and LSF About this task Configure LSF to use LSF License Scheduler Basic Edition as a replacement for an elim to collect external load indices where the external resources are licenses managed by FlexNet or Reprise License Manager. The following example assumes that LSF cluster named cluster1 uses an elim for a license resource named f1. Procedure In the LSF environment, disable the existing elim for the license resource by removing the license feature configuration from the lsf.shared and lsf.cluster.cluster_name files. For example, remove the configuration for f1 from the lsf.shared and lsf.cluster.cluster_name files. Configure the lsf.licenscheduler file with the appropriate hosts and the license feature. For example, configure the following sections in lsf.licenscheduler: Begin Parameters PORT=1700 HOSTS=hostA ADMIN=lsadmin LM_STAT_INTERVAL=15 LMSTAT_PATH=/usr/bin End Parameters Begin Clusters CLUSTERS cluster1 End Clusters Begin ServiceDomain NAME=LanServer LIC_SERVERS=((19999@hostA)) End ServiceDomain Begin Feature NAME=f1 CLUSTER_MODE=Y CLUSTER_DISTRIBUTION=LanServer(cluster1) End Feature Start LSF License Scheduler and LSF. For more details, refer to Start License Scheduler. What to do next From LSF, use bsub to submit a job without a duration requesting the f1 resource. For example, bsub -R \"rusage[f1=1]\" myjob Upgrading from LSF License Scheduler Basic Edition to Standard Edition About this task If you use LSF License Scheduler Basic Edition and wish to upgrade to LSF License Scheduler Standard Edition, obtain the LSF License Scheduler entitlement file, then upgrade LSF License Scheduler as follows: Procedure Copy the LSF License Scheduler entitlement file (ls.entitlement) to the LSF_ENVDIR directory. Restart LSF License Scheduler. bladmin reconfig Restart the mbatchd on the LSF master host. badmin mbdrestart Supported parameters for LSF License Scheduler Basic Edition The following is a list of specific lsf.licensescheduler parameters that LSF License Scheduler Basic Edition supports: Parameters section: ADMIN BLC_HEARTBEAT_FACTOR CLUSTER_MODE (LSF License Scheduler Basic Edition only supports CLUSTER_MODE=Y) HEARTBEAT_INTERVAL HEARTBEAT_TIMEOUT HOSTS LIB_CONNTIMEOUT LIB_RECVTIMEOUT LM_STAT_INTERVAL LM_STAT_TIMEOUT LM_TYPE LMSTAT_PATH LOG_EVENT LOG_INTERVAL LS_DEBUG_BLC LS_DEBUG_BLD LS_DEBUG_CMD LS_LOG_MASK LS_MAX_STREAM_FILE_NUMBER LS_MAX_STREAM_SIZE LS_STREAM_SIZE LS_STREAM_FILE MBD_HEARTBEAT_INTERVAL MBD_REFRESH_INTERVAL RLMSTAT_PATH STANDBY_CONNTIMEOUT Clusters section: CLUSTERS (one cluster only, LSF License Scheduler Basic Edition ignores additional clusters) ServiceDomain section (one ServiceDomain section per license feature only, LSF License Scheduler Basic Edition ignores additional ServiceDomain sections in the same license feature): NAME LIC_SERVERS LM_STAT_INTERVAL LM_STAT_TIMEOUT LM_TYPE LIC_COLLECTOR Feature section: NAME CLUSTER_MODE (Optional. This parameter may be specified in the Parameters section instead, but LSF License Scheduler Basic Edition only supports CLUSTER_MODE=Y) LM_LICENSE_NAME (Optional. LSF License Scheduler Basic Edition does not support the specification of multiple license manager feature names to combine into a single alias) CLUSTER_DISTRIBUTION (LSF License Scheduler Basic Edition supports a single cluster with a single service domain only, and ignores any additional clusters or service domains). Tip A specific lsf.licensescheduler configuration template for LSF License Scheduler Basic Edition is available and contains specifications for all supported parameters. This file is named lsf.licensescheduler.basic and is included in the LSF License Scheduler installation package. LSF License Scheduler uses the Standard Edition configuration file by default, but LSF License Scheduler Basic Edition ignores unsupported Standard Edition parameters with a warning message. To ensure that LSF License Scheduler Basic Edition uses only supported parameters and to prevent the logging of the warning messages, back up the lsf.licensescheduler configuration file, then move the lsf.licensescheduler.basic file to the $LSF_ENVDIR directory and rename it to lsf.licensescheduler. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/Start License Scheduler.html":{"url":"chapter10/section2/Start License Scheduler.html","title":"启动 License Scheduler","keywords":"","body":"启动许可证计划程序 任务说明 您可以将 LSF 配置为在 License Scheduler 主机以及可以在网络故障的情况下接管许可证分发的候选 License Scheduler 主机上启动 License Scheduler 守护程序（bld）。 LSF LIM 守护程序会自动启动 bld。 步骤 以 主LSF 管理员身份登录。 设置 LSF 环境: 对于 csh 或 tcsh: % source LSF_TOP/conf/cshrc.lsf 对于 sh, ksh, 或 bash: $. LSF_TOP/conf/profile.lsf 在 LSF_CONFDIR/lsf.conf 中，为 LSF_LIC_SCHED_HOSTS 参数指定以空格分隔的主机列表：LSF_LIC_SCHED_HOSTS =“ hostname_1 hostname_2 ... hostname_n” 其中: hostname_1，hostname_2 和 hostname_n 是 LSF LIM 守护程序在其上启动许可证调度守护程序的主机。 主机名的顺序将被忽略。 Note 将 LSF_LIC_SCHED_HOSTS 参数设置为与 lsf.licensescheduler HOSTS 参数中使用的候选主机相同的列表。 LSF_LIC_SCHED_HOSTS 参数未在任何其他函数中使用。 运行 lsadmin reconfig 以重新配置 LIM。 使用 ps -ef 确保候选主机上正在运行 bld。 运行 badmin mbdrestart 来重启 mbatchd. 如果您在服务域中指定了 LIC_COLLECTOR 名称，请手动启动每个许可证收集器： blcollect -m \"host_list\" -p lic_scheduler_port -c lic_collector_name 其中: host_list 指定以空格分隔的许可证调度程序候选主机列表，许可证信息将发送到该列表。 使用标准主机名。 lic_scheduler_port 对应于 lsf.licensescheduler 中设置的 License Scheduler 侦听端口。 lic_collector_name 指定在 lsf.licensescheduler 的 “service domain” 部分中为 LIC_COLLECTOR 设置的许可证收集器的名称。 例如: blcollect -m \"hostD.designcenter_b.com hostA.designcenter_a.com\" -p 9581 -c CenterB 在您的 LSF WORKDIR 中创建了一个名为 collectors/Center 的文件。 Note 如果您未在许可证调度程序的 ”service domain“ 中指定许可证收集器名称，则 主bld 节点将启动默认的blcollect。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/LSF parameters in License Scheduler.html":{"url":"chapter10/section2/LSF parameters in License Scheduler.html","title":"License Scheduler 中的 LSF 参数","keywords":"","body":"License Scheduler中的 LSF 参数 lsf.conf 中以 LSF_LIC_SCHED 开头的参数与 LSF 和 License Scheduler 都相关： LSF_LIC_SCHED_HOSTS LIM 在候选 License Scheduler 主机上启动 License Scheduler 守护程序（bld）。 CAUTION 如果您的集群是通过 UNIFORM_DIRECTORY_PATH 或 UNIFORM_DIRECTORY_PATH_EGO 安装的，则不能使用 LSF_LIC_SCHED_HOSTS。 不要为新安装或升级安装设置 UNIFORM_DIRECTORY_PATH 或 UNIFORM_DIRECTORY_PATH_EGO。 它们仅用于与早期版本兼容。 LSF_LIC_SCHED_PREEMPT_REQUEUE 重新安排许可证调度程序抢占其许可证的作业。 作业被杀死并重新排队，而不是暂停。 LSF_LIC_SCHED_PREEMPT_SLOT_RELEASE 释放已暂停的 “License Scheduler” 作业的内存和槽位资源。 这些资源仅可用于，请求至少一个与挂起的作业相同的许可证的挂起 License Scheduler 作业。 默认情况下释放作业槽位，但是如果启用了内存抢占（即在 lsb.params 中设置了 PREEMPTABLE_RESOURCES = mem），则也会释放内存资源。 LSF_LIC_SCHED_PREEMPT_STOP 使用作业控件停止被抢占的作业。 设置此参数后，将发送 UNIX SIGSTOP 信号来挂起作业，而不是 UNIX SIGTSTP。 LSF_LIC_SCHED_STRICT_PROJECT_NAMEs 提交作业后，严格检查许可计划程序项目名称。 如果命名的项目拼写错误（区分大小写），则该作业将被拒绝。 License Scheduler 使用的 LSF 参数 LSB_SHAREDIR 每个集群保存作业历史和记帐日志的目录 LSF_LOG_MASK LSF 守护程序的错误消息的记录级别 LSF_LOGDIR LSF 系统日志文件目录 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/About submitting jobs.html":{"url":"chapter10/section2/About submitting jobs.html","title":"关于提交作业","keywords":"","body":"关于提交作业 提交 LSF 作业时，必须在资源需求使用部分（bsub -R“ rusage ...”选项）中保留许可证。 您无法通过运行 bsub -R \"select\" 成功保留许可证。 指定许可证令牌名称（与指定共享资源相同）。 如果使用项目模式，请使用 bsub -Lp 选项指定许可证项目名称。 如果您在 lsf.conf 中也有 LSF_LIC_SCHED_STRICT_PROJECT_NAME = y 并且没有为所需功能配置默认项目，则该作业将被拒绝。 提示 使用 blstat 命令查看有关默认许可证项目的信息。 更新资源需求。 如果队列或作业启动器脚本请求由 LSF ELIM 管理的许可证，则必须更新作业提交脚本，以请求使用许可证令牌名称的许可证。 示例： bsub -R \"rusage[AppB=1]\" -Lp Lp1 myjob 此命令将名为 myjob 的作业提交给许可证项目 Lp1，并请求一个 AppB 许可证 bsub -R \"rusage[AppC=1]\" myjob 此命令提交名为 myjob 的作业，并请求一个 AppC 许可证。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/After configuration changes.html":{"url":"chapter10/section2/After configuration changes.html","title":"配置更改之后","keywords":"","body":"更改配置后 任务说明 如果对 License Scheduler 进行了配置更改，则必须重新配置 License Scheduler 以应用更改。 如果对 LSF 进行配置更改，则还必须重新配置 LSF。 步骤 运行 bld -C 以测试配置错误。 运行 bladmin reconfig all。 如果更改了 lsf.conf 或其他 LSF 配置文件，请运行 badmin mbdrestart 和 lsadmin reconfig。 备注 某些 License Scheduler 配置更改后，必须运行 badmin mbdrestart 才能使更改生效。 以下配置更改要求您运行 badmin mbdrestart： 项目更改，添加或删除 功能更改，添加或删除，包括模式更改 集群位置更改 您还必须运行 lsadmin reconfig 才能使对 LIM 的任何更改生效（例如，如果您更改了 LSF_LIC_SCHED_HOSTS）。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/Add a cluster to License Scheduler.html":{"url":"chapter10/section2/Add a cluster to License Scheduler.html","title":"将集群添加到 License Scheduler","keywords":"","body":"向 License Scheduler 中添加集群 开始之前 必须切换为 License Scheduler 管理员 任务说明 您可以将新集群，添加到现有的许可证计划程序实施中。 添加 LSF Advanced Edition 集群时，只需将提交集群添加到 LSF License Scheduler，因为提交集群会将 LSF License Scheduler 作业转发到其执行集群。 步骤 下载许可证计划程序包。 提示 获取与现有成员集群中使用的主 bld 二进制文件和其他体系结构相同的版本。 在新集群上安装许可计划程序软件包。 使用另一个具有相同 bld 主服务器的集群的 $LSF_ENVDIR 中的现有 lsf.licensescheduler。 将新的集群名称添加到 lsf.licensescheduler的 “Cluster” 部分。 添加或修改 lsf.licensescheduler 中定义的许可证分发策略。 维护一个中央 lsf.licensescheduler 文件，并让所有集群访问该文件。 注意 每个集群中的 lsf.licensescheduler 文件必须相同。 您可以使用以下两种方法之一来完成此任务： 创建从每个集群的 $LSF_ENVDIR 到中央 lsf.licensescheduler 文件的符号链接。 使用基于 CRON 的同步脚本将从中央 lsf.licensescheduler 文件所做的更改，同步到所有集群中相应的lsf.licensescheduler 文件。 检查 lsf.licensescheduler 文件中来自 PORT 的通信，是否存在防火墙或网络问题。 在运行 bld 的所有主机上运行 bladmin reconfig。 在新添加的集群上，运行 lsadmin limrestart，然后运行 badmin mbdrestart。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/Configure multiple administrators.html":{"url":"chapter10/section2/Configure multiple administrators.html","title":"配置多个管理员","keywords":"","body":"配置多个管理员 开始之前 主要 License Scheduler 管理员帐户，必须在主要 LSF 管理员帐户的 LSF 工作目录中具有写权限。 任务说明 管理员帐户使用您在安装许可证计划程序时指定的用户列表。 如果要添加或更改管理员，请编辑此参数。 列表中的第一个用户名是主要的 License Scheduler 管理员。 默认情况下，License Scheduler 创建的所有工作文件和目录均归 Lickaiense Scheduler 主帐户所有。 步骤 以主要的 License Scheduler 管理员身份登录。 如果要更改许可证计划程序管理员，请在 lsf.licensescheduler 中编辑 ADMIN 参数。 您可以指定多个由空格分隔的管理员。 例如： ADMIN = lsfadmin user1 user2 root 运行 bld -C 以测试配置错误。 运行 bladmin reconfig all 以应用您的更改。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/Upgrade License Scheduler.html":{"url":"chapter10/section2/Upgrade License Scheduler.html","title":"升级 License Scheduler","keywords":"","body":"更新 License Scheduler 开始之前 必须先安装 License Scheduler，然后才能进行升级。 您必须是集群管理员。 任务说明 您可以升级到 License Scheduler 的新版本，而无需卸载和重新安装。 步骤 下载 License Scheduler 新发布版本的 tar 文件。 停用所有队列。 取消激活所有队列会使所有正在运行的作业挂起，并阻止新作业被调度。 badmin qinact all 如果已安装 IBM Spectrum LSF Application Center，请关闭它。 pmcadmin stop 根据您站点上的过程备份现有的 LSF_CONFDIR，LSB_CONFDIR 和 LSB_SHAREDIR。 可选的。 要在 LSF License Scheduler 中使用快速调度项目模式，请将 LSF 升级到高于 9.1.1 的版本。 完成升级后，重新启动 LSF。 使用安装脚本来升级 LSF License Scheduler。 在旧的 LSF 集群中 source cshrc.lsf 或 profile.lsf。 导航到 tar 文件的位置并解压缩。 运行安装脚本。 如果要安装 License Scheduler Standard Edition，请将 License Scheduler 授权文件（ls.entitlement）复制到 $LSF_ENVDIR 目录。 如果您没有在启动 License Scheduler 之前将授权文件复制到 $LSF_ENVDIR，则 License Scheduler 将以 Basic Edition 运行。 启动许可证计划程序。 Source cshrc.lsf 或 profile.lsf。 运行 bladmin reconfig。 运行 ps -ef 以确保候选主机上正在运行 bld。 运行 badmin mbdrestart. 激活队列。 badmin qact all 如果已安装 IBM Spectrum LSF Application Center，请重新启动它。 pmcadmin start 提示 IBM Spectrum LSF Application Center 显示项目模式和集群模式的许可计划程序作业负载。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section2/Firewalls.html":{"url":"chapter10/section2/Firewalls.html","title":"防火墙","keywords":"","body":"防火墙 LSF，许可证调度程序和 taskman 互操作性的配置。 设置防火墙通信 任务说明 mbatchd 和 bld 侦听端口（入站连接）必须在防火墙的任一侧打开。 mbatchd: 由 lsf.conf 中的 LSB_MBD_PORT 设置 bld: 由 lsf.licensescheduler 中的 PORT 设置 步骤 如果防火墙位于mbatchd 主机和 bld 主机之间，则两个侦听端口都必须打开。 如果防火墙位于 bld 和 blcollect 主机之间（例如，blcollect 被配置为在许可证服务器上本地运行，而 bld 位于 LSF 主节点上）， bld 侦听端口必须打开。 如果防火墙位于 taskman 和 bld 之间（其中作业使用 taskman 与 License Scheduler 进行接口），则必须打开 bld 侦听端口。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:29 "},"chapter10/section3/LSF License Scheduler concepts.html":{"url":"chapter10/section3/LSF License Scheduler concepts.html","title":"LSF 许可证调度程序概念","keywords":"","body":"LSF 许可证调度程序概念 许可计划程序模式 项目组 许可计划程序中的服务域 发行政策 项目模式抢占 FlexNet 和 Reprise License Manager 的许可证使用情况 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:32 "},"chapter10/section3/License Scheduler modes.html":{"url":"chapter10/section3/License Scheduler modes.html","title":"License Scheduler 模式","keywords":"","body":"License Scheduler 模式 配置许可证计划程序的安装时，必须选择最适合您使用的每个许可证的项目模式和集群模式。 可以在一个安装中配置项目模式和集群模式，但是，作业所需的所有不同许可证必须属于同一模式。 集群模式 将许可证令牌，分发到由 LSF 计划接管的集群。 集群模式强调许可证令牌的高利用率，而不要考虑所有权等其他因素。 许可证所有权和共享仍然可以配置，但是可以在每个集群中进行配置，而不是跨多个集群进行配置。 作业（和许可证）的抢占也发生在每个集群中，而不是跨集群。作业完成后，许可证令牌将由 LSF 重用，而无需等待 lmstat 或 rlmstat 确认许可证令牌可用，并且在下一个 blcollect 周期中报告。 这将提高短期作业的许可证利用率。许可证调度程序 8.0 中引入了集群模式。 项目模式 将许可证令牌，分发给跨所有集群配置的项目。 项目模式强调跨多个集群的特定项目对许可证令牌的所有权。 当 License Scheduler 在项目模式下运行时，License Scheduler 在项目模式下分配许可证令牌之前，会检查所有 LSF 集群中许可证所有者的需求。 收集和评估所有集群中所有项目的需求的过程，会减慢每个计划周期。 在 lmstat 或 rlmstat 确认许可证令牌可用性之后，许可证令牌将在下一个调度周期中分发。在 License Scheduler 8.0 之前，只能使用项目模式。 集群模式和项目模式之间的区别 下图说明了具有相应 lmstat 或 rlmstat 报告时间的短作业，在集群模式下的许可证利用率： 在集群模式下，当一个作业完成运行时，下一个作业将立即获得其许可证，而不必等待下一个 lmstat 或 rlmstat 间隔。 例如，需要许可证 2 的四个作业可以运行，而无需等待 lmstat 或 rlmstat 报告令牌分配。 下图说明了具有 lmstat 或 rlmstat 报告时间的短期作业，在项目模式下的许可证利用率： 在项目模式下，每个作业必须等待 lmstat 或 rlmstat 报告令牌分配，然后才能获得许可证并开始运行。 在此示例中，需要许可证 2 的三个作业，能够在所示的 lmstat 或 rlmstat 间隔内启动。 何时使用集群模式 在以下情况下，集群模式最适合您的需求： 您的主要目标是最大限度地使用许可证。 许可证的所有权是次要的考虑因素。 相对于 blcollect 周期（默认为 60 秒，由 LM_STAT_INTERVAL 设置）而言，许多作业时间较短。 何时使用项目模式 如果满足以下条件，则项目模式最适合您的需求： 您的主要目标是确保小组的所有权。 最大化使用许可证是次要考虑因素。 大多数作业相对于 blcollect 周期较长（默认为 60 秒，由 LM_STAT_INTERVAL 设置）。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:33 "},"chapter10/section3/Project groups.html":{"url":"chapter10/section3/Project groups.html","title":"项目组","keywords":"","body":"项目组 在项目模式下配置许可证计划程序的安装时，可以选择配置项目，或进一步扩展项目配置，以形成分层项目组。 项目组将多个服务域集中在一起，并将它们视为许可证的一个来源，然后将它们分布在分层的公平共享树中。 策略树的叶子是作业可以属于的许可证项目。 树中的每个项目组都有一组值，包括份额和限制。 许可证所有权应用于叶级； 也就是说，在单个许可项目上。 给定内部节点的所有权等于其所有直接子节点的所有权之和。 每个功能都有其自己的层次结构组，但是功能可以共享层次结构。 跨服务域中的每个功能进行分层调度。 projects 仅项目一项就在一个服务域中应用一种分配策略。 相同的本地分发策略可以应用于多个服务域，但可以在本地实施。 groups of projects 项目组在一个服务域内应用一种分配策略，但为项目组分配份额和所有权以提高灵活性。 使用组许可证所有权，当项目使用的许可证少于其拥有的许可证时，或者项目所属的组使用的许可证少于该组拥有的许可证时，项目会触发抢占。 project groups 项目组按照配置的层次结构在多个服务域中应用一种分配策略。 您还可以使用项目组对分配给每个项目的许可证数量施加硬性限制。配置后，同一项目组层次结构可以用于多个功能。 何时使用 groups of projects 在以下情况下，以项目模式将项目分组在一起最适合您的需求： 许可证拥有多个级别，例如，一个部门以及该部门内的项目都拥有。 许可证所有权在一个服务域内。 对于未分组的项目，将在本地对项目组实施分发策略。 何时使用 project groups 在以下情况下，扩展配置以包括项目组最适合您的需求： 许可证所有权跨越服务域。 必须在多个服务域之间应用一种分发策略。 项目限制必须跨集群应用。 提示 如果需要，请使用 LSF 在一个集群中配置许可证项目限制。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:33 "},"chapter10/section3/Service domains in License Scheduler.html":{"url":"chapter10/section3/Service domains in License Scheduler.html","title":"License Scheduler 中的服务域","keywords":"","body":"License Scheduler 中的服务域 服务域是一组一个或多个许可证服务器。 许可证调度程序管理许可证令牌的调度，但是许可证服务器实际上提供了许可证。 您可以使用为网络提供许可证的许可证服务器名称和端口号来配置服务域。 LAN: 为单个集群提供许可证的服务域 WAN: 为多个集群提供许可证的服务域 License Scheduler 假定可以从 License Scheduler 接收令牌的任何用户，都可以使用服务域中的任何许可证。 因此，与分发策略中指定的项目关联的每个用户都必须满足以下要求： 用户可以与服务域中的每个许可证服务器主机，建立网络连接。 用户环境配置有权限，可以从服务域中的每个许可证服务器主机检出许可证。 您必须为许可证计划程序至少配置一个服务域。 它对为 LSF 作业提供许可证的许可证服务器主机进行分组，并在您定义在项目之间共享软件许可证的策略时使用。 如果许可证服务器不属于 License Scheduler 服务域，则其许可证不由 License Scheduler 管理（您在 LSF 中配置的许可证分发策略不适用于这些许可证，并且这些许可证的使用不会影响 LSF 调度决策）。 服务域位置 您可以使用许可证功能的局部性，来将功能从不同的服务域限制到特定的集群，以便许可证计划程序不将令牌授予，许可证中无法合法使用在请求令牌的集群上的许可证作业。 集群模式下使用的 LAN 服务域配置，有单集群本地性。 项目模式 在项目模式下，集群可以从多个服务域访问相同的许可证功能。 如果您的许可证服务器，将许可证令牌的服务限制在特定地理位置，请使用 LOCAL_TO 指定无法在所有位置之间共享的任何功能的许可证令牌的位置。 此参数避免了为不同的服务域定义不同的分发和分配策略，并允许分层项目组配置。 要在项目模式下使用 License Scheduler 令牌，作业提交必须指定 -Lp （许可证项目）选项。 必须在 lsf.licensescheduler 中为请求的功能定义项目。 集群模式 在集群模式下，集群中的每个许可证功能最多可以从一个 WAN 和一个 LAN 服务域访问单个许可证功能。 许可证计划程序不控制应用程序检出行为。 如果可以从 LAN 和 WAN 服务域中获得相同的许可证，则许可证计划程序希望作业首先尝试从 LAN 获取许可证。 并行作业 当 LSF 调度并行作业时，LSF License Scheduler 尝试检出使用用户名和所有执行主机名构造的并行作业中的 user@host密钥，并在服务域上合并相应的检出信息（如果找到）。 例如，在项目模式下，对于在服务域 sd1 中具有两个项目（P1和P2）且具有十个令牌的功能 F1，使用以下命令将并行作业分派给四个执行主机： bsub -n 4 -Lp P1 -R \"rusage[F1=4]\" mycmd 每个执行主机上的作业从 sd1 服务域中签出一个 F1 许可证。 如果四个执行主机分别是 hostA，hostB，hostC 和hostD，则有 user@hostA，user@hostB，user@hostC 和 user@hostD 的签出密钥，并且每个条目都贡献一个签出的令牌。 这些令牌都合并到 F1 功能部件中的 P1 项目的数据中。 因此，运行 blstat 会显示 F1 功能的以下信息： FEATURE: F1 SERVICE_DOMAIN: LanServer TOTAL_INUSE: 4 TOTAL_RESERVE: 0 TOTAL_FREE: 6 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND P1 50.0 % 0 4 0 1 0 P2 50.0 % 0 0 0 5 0 来自四个执行主机的四个 checkout 键合并到 P1 项目中。 如果定义了 MERGE_BY_SERVICE_DOMAIN=Y，则 LSF License Scheduler 还将合并多个 user@host 数据，以用于跨不同服务域的并行作业。 例如，如果您具有与上一个示例相同的设置，但是具有一个额外的服务域 sd2，其中也包含两个项目（P1和P2）和十个令牌，并且您定义了 MERGE_BY_SERVICE_DOMAIN=Y，则运行 blstat 将显示 F1 功能的以下信息： blstat FEATURE: F1 SERVICE_DOMAIN: sd1 TOTAL_INUSE: 2 TOTAL_RESERVE: 0 TOTAL_FREE: 8 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND P1 50.0 % 0 2 0 3 0 P2 50.0 % 0 0 0 5 0 SERVICE_DOMAIN: sd2 TOTAL_INUSE: 2 TOTAL_RESERVE: 0 TOTAL_FREE: 8 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND P1 50.0 % 0 2 0 3 0 P2 50.0 % 0 0 0 5 0 两个签出密钥合并到 sd1 域中的 P1 项目中，而两个签出密钥合并到 sd2 域中的 P1 项目中。 如果定义了 CHECKOUT_FROM_FIRST_HOST_ONLY=Y，则 LSF License Scheduler 在合并许可证使用数据时仅考虑并行作业的第一个执行主机的 user@host 信息。 在各个功能部件部分中的设置将覆盖 “Parameter” 部分中的全局设置。 如果某个功能具有多个功能部分（使用 LOCAL_TO ），则每个部分的 CHECKOUT_FROM_FIRST_HOST_ONLY 必须具有相同的设置。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:33 "},"chapter10/section3/Distribution policies.html":{"url":"chapter10/section3/Distribution policies.html","title":"发行政策","keywords":"","body":"分配政策 许可证调度程序最重要的部分是许可证令牌分发。 许可证分发策略确定如何在项目或集群之间共享许可证令牌。 每当有竞争时，配置的份额分配将确定每个项目或集群有权获得的许可令牌的部分。 我们同时引用许可证和许可证令牌，因为许可证计划程序不直接控制许可证。 相反，它通过跟踪许可证令牌来控制需要通过 LSF 或 Taskman 提交的许可证的作业的分配。 许可证令牌总数 由许可证计划程序为一个服务域中的单个功能管理的许可证令牌总数取决于以下因素： 服务域中活动许可证服务器的数量 由非 LSF 管理的应用程序检出的许可证数量 许可证份额 分配策略中分配的许可证份额决定了项目（在项目模式下）或集群（在集群模式下）接收的许可证总数中的哪一部分。 能够使用许可证功能的每个项目或集群，必须在服务域中共享许可证功能。 对于任何许可证功能，将许多份额转换为许多许可证的公式为： (shares assigned to project or cluster) ______________________________ x (total number of licenses) (sum of all shares assigned) 分配给许可证项目或集群的份额数，只有在将其与分配给其他项目或集群的数目或份额总数进行比较时，才有意义。 当系统中没有作业时，将为每个项目或集群分配基于共享分配的许可证令牌。 集群模式分发策略 静态 根据配置的份额，总许可证的一部分分配给集群。 该数量是静态的，并且不依赖于系统中的工作负载。 动态 总许可证的份额以及缓冲区大小分配给每个集群。 配置的份额设置了每个集群最初接收的许可证数量，但是会根据集群的需求定期调整此数量。每当集群请求分配更新时，许可证分配都会更改，默认情况下每 15 秒更改一次。 在每次更新中，分配可以增加多达缓冲区的大小。 没有减少集群分配的限制。在集群模式下使用动态许可证分发时，可以为每个集群配置最小和最大分配值。 最小分配类似于项目模式下非共享许可证的数量，因为此数量的令牌保留供集群专用。如果配置的最小值超出了集群的份额分配，则仅为集群保留分配的份额。集群共享优先于配置的最小分配。 如果最小分配超出了集群在总令牌中所占的份额，则 bld 给出的集群分配可能小于配置的最小分配。 集群内的保证 许可的保证份额分配给使用 LSF 保证类型 SLA 的集群中的项目。 （可选）可以配置未使用的有保证许可证的共享。保证类似于集群模式的所有权，并且可以与静态和动态分配策略一起使用。 Note 保证类型的 SLA 仅在 LSF 8.0 版或更高版本中可用。 何时使用静态许可证分发 在集群模式下为所有许可证功能配置共享。 静态许可证分发是基本的许可证分发策略，并且通过添加更多配置来建立。 如果跨集群的许可证需求是可预测且不变的，或者许可证由集群严格拥有，或者您始终有额外的许可证，则基本静态配置可以满足您的需求。 何时使用动态许可证分发 如果对许可证的需求在集群之间发生变化，则动态许可证分配可以满足您的需求。 何时将 LSF 保证 SLA 与 License Scheduler 一起使用 如果集群中的许可证是拥有的，并且由许可证所有者优先使用或专有使用，则在 LSF 集群中配置保证 SLA 可以满足您的需求。 项目模式分发策略 公平分享 总许可证的份额分配给每个许可项目。未使用的许可在有需求的任何地方共享，但是，当需求超过许可数量时，将进行份额分配。 作业不被抢先分配许可证； 而是在作业完成运行后重新分配许可证。 所有权和优先权 总许可的份额分配给每个许可项目。 还分配了许可证的拥有份额。未使用的许可证将在有需求的任何地方共享，但是，当需求超过许可证数量时，将通过抢占来收回拥有的份额。仅当尚未使用指定数量的已有许可证且没有可用许可证时，抢占才会发生。 使用所有拥有的许可证后，许可证调度程序将等待许可证变为免费（而不是使用抢占），然后分发更多令牌，直到达到共享。许可证可用后，将自动恢复被许可证计划程序抢占的作业。默认情况下，当许可证计划程序将许可证从作业中抢占时，LSF 会释放已挂起作业的作业槽。 提示 为了使 License Scheduler 将许可证令牌提供给另一个项目，应用程序必须能够在作业暂停后释放其许可证。 主动所有权 主动所有权允许所有权根据项目活动自动调整。 所有权表示为活动项目的总所有权的百分比。 随着更多项目的活跃，每个项目的实际所有权降低。 将所有权百分比值设置为总计超过 100％，以从主动所有权中受益。 非共享许可证 某些许可证被指定为非共享许可证，保留给专有使用，而不是在不使用时共享。非共享许可证的数量包含在拥有许可证的数量中，但是此数量不包含在该项目的计算中。 要将某些许可证指定为非共享，请将非共享编号添加到所拥有的和非共享值中。 何时在项目模式下使用 Fairshare 在项目模式下为所有许可证功能配置 Fairshare。 Fairshare 是基本的许可证分发策略，并且通过添加其他配置来构建。 如果将许可证分配给特定的许可证项目，但不是严格拥有的，则基本的 Fairshare 配置可以满足您的需求，而无需配置其他分发策略。 何时添加所有权（和抢占） 在以下情况下，将许可证配置为拥有的： 许可证归许可证项目所有，但不使用时可以借出。 最大化许可证使用率和许可证所有权都是重要的考虑因素。 必要时，必须尽快将借出的许可证退还给所有者（使用抢占）。 可以抢占借用作业许可证。 何时添加有效所有权 在以下情况下，为拥有的许可证配置活动所有权： 所有权值是动态的，而不是固定值，通常随着更多项目积极寻求许可证而减少。 何时添加非共享许可证 在以下情况下，将许可证配置为非共享许可证： 拥有许可证。 许可证仅供所有者使用。 始终向所有者提供许可证比最大化使用许可证更为重要。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:31 "},"chapter10/section3/subsection1/Project mode preemption.html":{"url":"chapter10/section3/subsection1/Project mode preemption.html","title":"项目模式抢占","keywords":"","body":"项目模式抢占 仅当没有免费许可证时，抢占才会发生。 在抢占期间，项目会将借来的许可证释放给拥有许可证的项目（现在有需求）。 使用支持作业暂停的许可证的作业会释放其令牌，并自动从被暂停的位置恢复。 使用不支持挂起的许可证的作业将被杀死，并从头开始重新启动。 抢占仅适用于项目模式，并且根据您的配置考虑以下因素： 运行时（具有最小运行时间的作业通常会先被抢占） 公平分享设置 所有权 优先权 最小的作业优先权 根据项目的设置方式（无论它们是否处于同一级别），您的抢占权是固定的还是分层的。 抢占限制 LSF 抢占与 License Scheduler 抢占 基本抢占与配置的项目 发生抢占时，许可证调度程序将计算每个项目的令牌使用量。 该计算将考虑正在使用的令牌，必需的令牌以及令牌所有权值。 根据令牌的使用情况，许可证调度程序确定需要令牌的项目以及包含太多令牌的项目。 属于需要令牌的项目的作业将首先进行调度，并按项目 Fairshare 设置进行排序。 如果需要，属于具有额外令牌的项目的作业将被优先抢占，并按项目公平共享设置和每个作业运行的时间长度进行排序。 PRIORITY 部分 如果在 “Project” 部分中配置了项目 “PRIORITY”，则项目的排序顺序基于优先级，其中优先级较高的项目最后被抢占。 PREEMPT_ORDER 部分 如果在 “Feature” 部分中将 “ PREEMPT_ORDER” 设置为 “ BY_OWNERSHIP”，则按所有权对项目进行排序。 首先安排所有权最高的项目。 拥有权最小的项目优先。 此设置将覆盖基本优先级和 PRIORITY。 ENABLE_MINJOB_PREEMPTION 部分 如果 ENABLE_MINJOB_PREEMPTION = Y，则抢占作业的数量将最小化。 具有额外令牌的项目按 PRIORITY （如果已配置）或 fairshare 排序。 然后按 RUSAGE 排序作业。 RUSAGE 较高的作业被抢先，以最大程度减少被抢占的工作。 除基本优先权或 PRIORITY 外，还使用此设置。 配置项目组的分层抢占 配置项目组后，将层次结构引入项目配置中，将应用层次结构优先。 有两种优先级的抢占方法： 自上而下（默认）：抢占发生在表兄弟姐妹之间，而不是兄弟姐妹之间。 结果是在整个项目层次结构之间取得优先权。 自下而上（如果 LS_PREEMPT_PEER = Y）：兄弟姐妹可以互相抢占。 结果是首先要在一系列项目中平衡先发制人。 例如，您的项目设置如下： 在自上而下的抢占中，如果 P8 需要令牌，它将抢占 P1，P2 或 P3（关系更远的关系），而不抢占 P6 或 P7（P8的兄弟姐妹）。 在自下而上的抢占中，P8 抢占了其同级（P6 或 P7）。 限制 分层抢占还受到项目上任何限制的影响。 如果已经达到限制（在层次结构的任何级别），则许可调度程序会考虑将下一个可能的节点抢占。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section3/subsection1/Preemption restrictions.html":{"url":"chapter10/section3/subsection1/Preemption restrictions.html","title":"抢占限制","keywords":"","body":"抢占限制 在以下情况下，不能抢占作业： 抢占受参数限制，例如：MAX_JOB_PREEMPT，PREEMPT_RESERVE 或 LM_REMOVE_INTERVAL。 可抢占作业的服务器不是当前的检查服务域。 提交作业的时间长度已到，该时间长度已过期。 由许可证计划程序管理的使用许可证的 LSF 作业和 taskman 作业都可以被抢占。为确保优先级较低的作业不会被多次抢占，可以使用 LS_ENABLE_MAX_PREEMPT 启用最大抢占次数限制。 许可证调度程序 taskman 作业抢占限制由 lsf.licensescheduler 中的参数 LS_MAX_TASKMAN_PREEMPT 控制。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:33 "},"chapter10/section3/subsection1/LSF preemption with License Scheduler preemption.html":{"url":"chapter10/section3/subsection1/LSF preemption with License Scheduler preemption.html","title":"LSF 抢占与 License Scheduler 抢占","keywords":"","body":"LSF 抢占与 License Scheduler 抢占 对于 LSF 作业，参数 MAX_JOB_PREEMPT 设置可以抢占作业的最大次数。 可以在 lsb.params，lsb.queues 或 lsb.applications 中定义 MAX_JOB_PREEMPT，其中应用程序设置覆盖队列设置，队列设置覆盖集群范围的 lsb.params 定义。 即使在 LSF 中没有可用的槽位，属于在 License Scheduler 中拥有所有权的许可证项目的作业，也可以触发抢占。 与 lsf.conf 中的 LSF_LIC_SCHED_PREEMPT_SLOT_RELEASE=Y 一起配置，许可证作业抢占与基于 LSF 槽位的抢占一起使用。 在 lsb.params 中一起配置 PREEMPTABLE_RESOURCES=mem，在 lsf.conf 中一起配置 LSF_LIC_SCHED_PREEMPT_SLOT_RELEASE=Y，许可证作业抢占与 LSF 内存资源抢占一起工作。 示例 项目 proj1 拥有3个许可 AppX。 在 lsf.conf 中配置了 MXJ = 5, 以及 LSF_LIC_SCHED_PREEMPT_SLOT_RELEASE=Y。 提交了五个作业，并在 proj2 中使用 AppX 启动了这些作业。 然后，将两个作业提交给 proj1，并等待 AppX 许可证令牌。 尽管槽位已满，但请求将发送到 License Scheduler，后者会识别所有权并抢占 proj2 中的两个作业。 作业被挂起，它们的许可证和槽位都被释放，并且 proj1 中的两个作业可以运行。 LSF JOB_CONTROLS 配置 如果 LSF 管理员在 lsb.queues 中定义 JOB_CONTROLS，以使作业控制（例如信号 SIGTSTP ）在许可调度程序抢占发生时生效，则还必须在 lsf.conf 中定义 LSF_LIC_SCHED_PREEMPT_STOP = Y 才能使许可调度程序抢占来工作 。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:33 "},"chapter10/section3/subsection2/License usage with FlexNet and Reprise License Manager.html":{"url":"chapter10/section3/subsection2/License usage with FlexNet and Reprise License Manager.html","title":"FlexNet 和 Reprise License Manager 的许可证使用情况","keywords":"","body":"FlexNet 和 Reprise License Manager 的许可证使用情况 许可证计划程序与不同类型的应用程序的工作方式不同，这取决于应用程序如何使用许可证功能，以及在作业开始时是否已知这些许可证功能。 已知许可要求 未知许可要求 项目模式 集群模式 保留的 FlexNet Manager 许可证 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section3/subsection2/Known license requirements.html":{"url":"chapter10/section3/subsection2/Known license requirements.html","title":"已知许可要求","keywords":"","body":"已知许可要求 对于许多应用程序而言，在作业开始之前，就已经知道运行其作业所需的所有许可证功能。 作业提交，将许可证使用请求传递给 LSF 集群。 LSF 将查询发送到 License Scheduler，以查看是否可以将许可证令牌分配给应用程序。 当许可证调度程序授予许可时，LSF 会向用户应用程序授予授权。 用户应用程序，向许可证管理器发送请求以检出许可证。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section3/subsection2/Unknown license requirements.html":{"url":"chapter10/section3/subsection2/Unknown license requirements.html","title":"未知许可要求","keywords":"","body":"未知许可要求 某些应用程序需要初始功能许可证才能开始作业，而在作业执行过程中，则需要更多功能或子功能许可证。 提交作业的用户知道启动作业所需的主要许可证功能，但可能不知道其他功能名称或所需的其他功能数量。 作业提交时未指定的此附加许可证功能被视为未知许可证使用。 在任何时候，用户应用程序都可以向 LSF 发出请求而无需从 License Scheduler 进行验证，或者可以通过直接将许可证请求发送到许可证服务器来完全绕过 LSF。 用户应用程序向 LSF 发出请求，而无需从 License Scheduler 进行验证。 LSF 向用户应用程序授予授权，因为该请求未指定许可证调度程序验证的需要。 用户应用程序向许可证管理器发送请求以检出许可证。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section3/subsection2/Project mode.html":{"url":"chapter10/section3/subsection2/Project mode.html","title":"项目模式","keywords":"","body":"项目模式 已知许可要求 项目模式支持在作业提交的 “rusage ” 部分中指定的已知许可证要求。 默认情况下，每个许可证功能都是为整个作业保留的。 （可选）使用 “Feature” 部分的参数 DYNAMIC = Y 以启用 rusage 字符串中的持续时间，并在指定的持续时间后释放许可证功能。 未知许可要求 不在 rusage 字符串中的未知许可证要求，被视为不是由 LSF 管理的作业，并且默认情况下不应用许可证分发策略。 （可选）如果作业的 rusage 字符串中指定了至少一个许可证功能，则可以将项目列表中未包含在 rusage 字符串中的许可证要求，作为托管作业负载的一部分进行跟踪。 在 “Feature” 部分中设置参数 ENABLE_DYNAMIC_RUSAGE = Y 以应用项目分发策略，即使未指定许可证使用率也是如此。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section3/subsection2/Cluster mode.html":{"url":"chapter10/section3/subsection2/Cluster mode.html","title":"集群模式","keywords":"","body":"集群模式 已知许可要求 集群模式支持在作业提交的 “rusage” 部分中指定的已知许可证要求。 每个许可证功能都是为整个作业保留的。 在集群模式下，不能在指定的持续时间下提交许可证要求。 如果您仅对作业的预定部分有已知的许可要求，则必须在将其包括在 rusage 中 和保留在整个作业中进行选择，或者将其保留为未知要求。 未知许可要求 rusage 字符串中未包含的未知许可证需求，在集群模式下计为托管工作负载的一部分。 rusage 字符串中没有的许可证功能不是为该作业保留的，但是，分配策略确实适用。 此行为等效于项目模式下的、ENABLE_DYNAMIC_RUSAGE=Y。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section3/subsection2/Reserved FlexNet Manager licenses.html":{"url":"chapter10/section3/subsection2/Reserved FlexNet Manager licenses.html","title":"保留的 FlexNet Manager 许可证","keywords":"","body":"保留的 FlexNet Manager 许可证 LSF License Scheduler 支持 FlexNet Manager 许可证保留关键字（RESERVE），该关键字允许 LSF License Scheduler 包含这些保留的许可证。 FlexNet Manager 保留的许可令牌被视为正在使用（作为 “OTHER” 令牌），而不是免费的（作为 “FREE” 令牌）。 因此，RESERVE 值包括在 blstat 命令输出的 OTHERS 值中，而不包括在 FREE 值中。 要启用对 FlexNet Manager 许可证保留关键字的支持，请在 lsf.licensescheduler 文件中定义LM_RESERVATION 参数。 在 “Parameter” 部分中定义 LM_RESERVATION=Y 以启用全局支持，或者在 “Features” 部分中定义LM_RESERVATION=Y 以启用对单个功能的支持。 如果在功能中定义了 WORKLOAD_DISTRIBUTION 参数，则不能在基于时间的配置中使用此支持，也不能使用此支持。 提示 FlexNet Manager 保留的许可证令牌，必须在 LSF License Scheduler 之外使用。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Configuring License Scheduler.html":{"url":"chapter10/section4/Configuring License Scheduler.html","title":"配置许可证调度程序","keywords":"","body":"配置许可证调度程序 配置集群模式 具有保证配置的集群模式 项目的项目模式 项目模式的可选设置 在具有项目或项目组的项目模式下配置 License Scheduler 之后，您可以包括一些不必要的附加配置，但可能有用。 项目组的项目模式 配置快速调度项目模式 基于时间的自动配置 自动转移 用户认证 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Configure cluster mode.html":{"url":"chapter10/section4/Configure cluster mode.html","title":"配置集群模式","keywords":"","body":"配置集群模式 使用集群模式，可在 LSF 集群之间分配许可证，而让每个 LSF 集群的调度程序调度作业，为集群中的项目分配许可证并抢占作业。 配置参数 步骤 集群模式可以全局设置，也可以针对单个许可证功能设置。 对于某些功能使用集群模式，对于某些功能使用项目模式时，请分别进行设置。 如果将集群模式用于所有许可证功能，请在 lsf.licensescheduler 的 “Parameters” 部分中定义CLUSTER_MODE=Y。 如果您将集群模式用于某些许可证功能，请在 lsf.licensescheduler 的 “Feature” 部分中为单个许可证功能定义 CLUSTER_MODE=Y。 CLUSTER_MODE 的 Feature 区域设置，将覆盖全局 Parameter 区域设置。 列出许可证调度程序主机。 在 LSF 安装中，默认情况下，HOSTS 参数设置为 LSF_MASTER_LIST。 按从最倾向使用到最不倾向的顺序列出主机。 第一台主机是主许可证调度程序主机。 除非所有您的 License Scheduler 客户端都在同一 DNS 域中运行，否则请指定标准主机名，例如 hostX.mycompany.com。 HOSTS=host1 host2 指定许可证调度程序，和许可证管理器之间的数据收集频率。 默认值为 60 秒。 LM_STAT_INTERVAL=秒数值 为所使用的许可证管理器，指定命令的文件路径。 如果使用的是 FlexNet，请指定 lmutil （或 lmstat）命令的路径。 例如，如果 lmstat 位于 /etc/flexlm/bin 中： LMSTAT_PATH=/etc/flexlm/bin 如果使用的是 Reprise License Manager，请指定 rlmutil（或 rlmstat）命令的路径。 例如，如果命令在 /etc/rlm/bin 中： RLMSTAT_PATH=/etc/rlm/bin 配置集群 任务说明 在 lsf.licensescheduler 文件的 “Cluster” 部分中，配置允许使用许可调度程序的集群。 仅当您使用多个集群时才需要配置集群。 步骤 在 “集群” 部分中，列出可以使用许可证计划程序的所有集群。 例如： Begin Clusters CLUSTERS cluster1 cluster2 End Clusters 集群模式服务域 服务域是一组一个或多个许可证服务器。 License Scheduler 管理许可证令牌的调度，但是许可证服务器实际上提供了许可证。 在集群模式下，每个集群可以从一个 WAN 和一个 LAN 服务域访问许可证。 License Scheduler 不控制应用程序检出行为。 如果可以从 LAN 和 WAN 服务域中获得相同的许可证，则 License Scheduler 希望作业首先尝试从 LAN 获取许可证。 配置服务域部分 任务说明 您可以在 lsf.licensescheduler 文件的 ServiceDomain 部分中，使用为网络提供许可证的许可证服务器名称和端口号来配置每个服务域。 稍后在 “ Feature” 部分中指定服务域是 WAN 还是 LAN 服务域。 步骤 添加一个 ServiceDomain 部分，并为每个服务域定义 NAME。 例如： Begin ServiceDomain NAME=DesignCenterA End ServiceDomain 指定该域的许可证服务器主机，包括主机名和许可证管理器端口号。 例如： Begin ServiceDomain NAME=DesignCenterA LIC_SERVERS=((1700@hostA)) End ServiceDomain 对于多个许可证服务器： LIC_SERVERS=((1700@hostA)(1700@hostB)) 对于冗余服务器，括号用于将共享同一 license.dat 文件的三台主机分组： LIC_SERVERS=((1700@hostD 1700@hostE 1700@hostF)) 提示 如果 FlexNet 使用默认范围内的端口，则可以指定不带端口号的主机名。 请参阅 FlexNet 文档以获取默认端口范围的值。 LIC_SERVERS=((@hostA)) 配置远程许可证服务器主机 开始之前 如果将 FlexNet 用作许可证管理器，则在使用 LSF License Scheduler 配置这些主机之前，FlexNet 许可证服务器主机必须在 LMSTAT_PATH 目录中具有 lmutil（或 lmstat）。 如果使用的是 Reprise License Manager，则在使用 LSF License Scheduler 配置这些主机之前，Reprise License Manager 许可证服务器主机必须具有 rlmutil（或 rlmstat）目录。 任务说明 许可证收集器（blcollect）是一个多线程守护程序，可在 LSF License Scheduler 下向所有许可证服务器查询许可证使用信息。 许可证收集器调用 lmutil 或 lmstat（对于 FlexNet 而言），或 rlmutil 或 rlmstat（对于 Reprise License Manager 而言），以从每个许可证服务器收集信息。 当同时存在本地和远程许可证服务器（即，与运行 blcollect 的主机位于不同子网中的许可证服务器）时，从远程许可证服务器收集信息的线程，比从本地许可证服务器收集信息的线程慢。 如果有远程许可证服务器，请在每个域中至少指定一个远程许可证服务器作为远程代理主机。许可证收集器连接到远程代理主机，并在远程代理主机上调用 lmutil，lmstat ，rlmutil 或 rlmstat，并从所有许可证服务器获取许可证信息。 远程代理主机服务。 远程代理主机和远程许可证服务器应位于同一域中以改善访问。 步骤 选择许可证收集器连接到远程主机的连接方法。 LSF License Scheduler 支持使用 ssh ，rsh 和l lsrun 连接到远程主机。 如果使用 lsrun 作为连接方法，则代理主机必须是 LSF 集群中的服务器主机，并且 RES 必须在此主机上启动。 否则，如果使用 ssh 或 rsh作为连接方法，则代理主机不必是 LSF 集群中的服务器主机。 在 “Parameters” 部分中，定义 REMOTE_LMSTAT_PROTOCOL 参数，并指定连接命令（和命令选项，如果需要的话）以连接到远程服务器。 REMOTE_LMSTAT_PROTOCOL=ssh [ssh 命令选项] | rsh [rsh 命令选项] | lsrun [lsrun 命令选项] 默认连接方法是 ssh，没有命令选项。 LSF License Scheduler 使用指定的命令（和可选的命令选项）连接到代理主机。 LSF License Scheduler 会自动将代理主机的名称附加到命令中，因此无需使用命令指定主机。 提示 LSF License Scheduler 无法验证指定的命令，因此必须确保正确指定命令。blcollect 日志文件中会记录所有连接错误。 如果连接方法是 ssh 或 rsh，请验证是否已配置此连接方法，以便运行许可证收集器的主机，可以连接到远程主机而无需指定密码。 定义远程许可证服务器和远程代理主机。 在 ServiceDomain 部分中，定义 REMOTE_LMSTAT_SERVERS 参数： REMOTE_LMSTAT_SERVERS=host_name[(host_name ...)] [host_name[(host_name ...)] ...] 指定一个远程代理主机，然后在括号中指定它服务的所有许可证服务器。 远程代理主机及其服务的许可证服务器，必须位于同一子网中。 如果您自己指定了一个远程代理主机，而没有任何许可证服务器（例如REMOTE_LMSTAT_SERVERS=hostA）, 则该远程代理主机，将被视为是一个以自身为远程代理主机的远程许可证服务器。也就是说，许可证收集器连接到远程代理主机，并且仅在远程代理主机上获取许可证信息。 您可以指定多个远程代理主机来服务多个子网，也可以指定多个远程代理主机来服务同一子网中的特定许可证服务器。 您在此处指定的任何主机必须是在 LIC_SERVERS 中定义的许可证服务器。 REMOTE_LMSTAT_SERVERS中定义的所有主机，如果也未在 LIC_SERVERS 中定义，则将被忽略。 以下示例假定许可证收集器（blcollect）在 LShost1 上运行。 也就是说，在 “Parameters” 部分中指定了以下参数： Begin Parameters ... HOSTS=LShost1 ... End Parameters 一台本地许可证服务器（hostA）和一台远程许可证服务器（hostB）： LIC_SERVERS=((1700@hostA)(1700@hostB)) REMOTE_LMSTAT_SERVERS=hostB 许可证收集器直接在 hostA 上运行 lmutil，lmstat，rlmutil 或 rlmstat，以获取 hostA 上的许可证信息。 因为在没有其他许可证服务器的情况下定义了 hostB，所以 hostB 是仅为其自身提供服务的远程代理主机。 许可证收集器连接到 hostB（使用 REMOTE_LMSTAT_PROTOCOL 参数指定的命令）并运行lmutil ，lmstat，rlmutil 或 rlmstat 以获取在 1700@hostB 上许可证信息 。 一台本地许可证服务器（hostA），一台为一台远程许可证服务器（hostC）服务的远程代理主机（hostB）和一台为两台远程许可证服务器（hostE和hostF）服务的远程代理主机（hostD）： LIC_SERVERS=((1700@hostA)(1700@hostB)(1700@hostC)(1700@hostD)(1700@hostE)(1700@hostF)) REMOTE_LMSTAT_SERVERS=hostB(hostC) hostD(hostE hostF) 许可证收集器直接运行 lmutil，lmstat，rlmutil 或 rlmstat，以从1700@hostA，1700@hostB 和 1700@hostD 获取许可证信息。 许可证收集器连接到 hostB（使用 REMOTE_LMSTAT_PROTOCOL 参数指定的命令），并运行 lmutil，lmstat，rlmutil 或 rlmstat 以获取有关 1700@hostC 的许可证信息。 hostB 和 hostC 应该位于同一子网中以改善访问。 许可证收集器连接到 hostD（使用 REMOTE_LMSTAT_PROTOCOL 参数指定的命令），并运行 lmutil，lmstat，rlmutil 或 rlmstat 以获取有关 1700@hostE 和 1700@hostF 的许可证信息。 hostD，hostE 和 hostF 应该位于同一子网中以改善访问。 一台本地许可证服务器（hostA），一台远程许可证服务器（hostB）和一台为两个远程许可证服务器（hostD和hostE）提供服务的远程代理主机（hostC）： LIC_SERVERS=((1700@hostA)(1700@hostB)(1700@hostC)(1700@hostD)(1700@hostE)) REMOTE_LMSTAT_SERVERS=hostB hostC(hostD hostE) 许可证收集器直接运行 lmutil，lmstat，rlmutil 或 rlmstat，以获取有关 1700@hostA 和 1700@hostC 的许可证信息。 许可证收集器连接到 hostB（使用 REMOTE_LMSTAT_PROTOCOL 参数指定的命令）并运行 lmutil ，lmstat，rlmutil 或 rlmstat 以获取在 1700@hostB 上的许可证信息 。 许可证收集器连接到 hostC（使用 REMOTE_LMSTAT_PROTOCOL 参数指定的命令），并运行 lmutil，lmstat，rlmutil 或 rlmstat 以获取有关 1700@hostD 和 1700@hostE 的许可证信息。 hostC，hostD 和 hostE 应该位于同一子网中以改善访问。 配置局域网服务域（LAN） 任务说明 您可以在 lsf.licensescheduler 的 “Feature” 部分中配置 LAN 服务域。 在每个 “ LAN Feature” 部分中只能指定一个集群和服务域。 来自 LAN 服务域的许可证被静态分配给集群。 步骤 在 Feature 部分，设置 CLUSTER_DISTRIBUTION=service_domain(cluster_name share) 使用在 ServiceDomain 部分中定义的服务域名。 例如： Begin Feature NAME=verilog CLUSTER_DISTRIBUTION=MyLanServer(tokyo_cluster 1) End Feature 配置 WAN 服务域 任务说明 WAN 配置包括共享 WAN 服务域的所有集群。 对于 LAN 服务域，请在 lsf.licensescheduler 文件的 “Feature” 部分的 CLUSTER_DISTRIBUTION 参数中设置此配置。 对于 WAN 服务域，您可以选择基于 WAN 服务域所服务的所有集群之间过去的许可证使用情况，来配置动态许可证共享，并在需要时为每个集群设置最小和最大分配。 步骤 在 CLUSTER_DISTRIBUTION 参数中设置 WAN 服务域名。 CLUSTER_DISTRIBUTION = service_domain(cluster share/min/max...) 使用在 ServiceDomain 部分中定义的服务域名。 配置每个集群。 必须包括有权访问 WAN 服务域许可证的所有集群。 设置集群名称 设置每个集群的份额。 份额是一个非负整数，代表每个集群在静态许可分配中获得的许可份额，以及在动态许可分配中的起始份额。 （可选）在 lsf.licensescheduler 文件的 “Feature” 部分中设置 ALLOC_BUFFER 。 设置后，此参数将启用动态共享策略。 ALLOC_BUFFER = buffer 或者 ALLOC_BUFFER = cluster1 buffer1 cluster2 buffer2...default buffer 如果有额外的许可证令牌可用，则每个集群的分配增加到 PEAK + BUFFER。 BUFFER 值由 “Feature” 部分中的 ALLOC_BUFFER 设置，而 PEAK 值是在 Parameters 或 Feature 部分中 PEAK_INUSE_PERIOD 设置的时间间隔内，动态许可证令牌使用的峰值。 如果集群中未使用分配的令牌，则集群的分配将降为 PEAK + BUFFER。 由于集群中未使用令牌，因此峰值使用值 PEAK 降低，因此 PEAK + BUFFER 也降低。 分配缓冲区，根据需要设置集群分配增长的速率，以及可以不使用的许可证数量。 分配缓冲区可帮助确定随着集群中需求的增加将令牌传输到集群的最大速率。 分配给集群的最大速率由分配缓冲区除以 MBD_REFRESH_INTERVAL。注意不要将分配缓冲区设置得太大，以致于因为将许可证分配给了不能使用它们的集群，而浪费许可证。 （可选）启用动态共享（定义了 ALLOC_BUFFER）后，您可以为每个集群设置最小和最大分配。 最低分配保留许可证令牌供集群专用； 最大分配限制了集群接收的许可证令牌的总数。 集群共享优先于配置的最小分配。 如果最小分配超出了集群在总令牌中所占的份额，则 bld 给出的集群分配可能小于配置的最小分配。 结果 为了允许一个集群仅在另一个集群不需要许可证时，才可以使用许可证，可以将集群的集群分布设置为 0，并为该集群可以请求的令牌数量指定一个分配缓冲区。 例如： Begin Feature CLUSTER_DISTRIBUTION=Wan(CL1 0 CL2 1) ALLOC_BUFFER=5 End Feature 当没有作业在运行时，CL1 的令牌分配为 5。 如果 CL2 不需要令牌，则 CL1 可以获得 5 个以上。 示例 静态示例（未设置分配缓冲区）： Begin Feature NAME=verilog CLUSTER_DISTRIBUTION=MyWanServer(tokyo_cl 1 newyork_cl 1 toronto_cl 2) End Feature 在此示例中，仅基于分配给每个集群的份额数，来静态分配许可证。 如果许可证数量不能被份额数量平均整除，则附加许可证将按指定顺序以 CLUSTER_DISTRIBUTION 的形式循环分发给集群。 因此，如果总共有 98 个许可证，则 tokyo_cl 接收 25 个许可证，newyork_cl 接收 25 个许可证，toronto_cl 接收 48 个许可证。每个集群基于分配的许可证令牌，限制正在运行的作业的总使用率。 动态示例（分配缓冲区集）： Begin Feature NAME=verilog CLUSTER_DISTRIBUTION=MyWanServer(tokyo_cl 1 newyork_cl 1 toronto_cl 2/10/50) ALLOC_BUFFER=tokyo_cl 5 newyork_cl 1 toronto_cl 2 End Feature 在此示例中，最初根据分配的份额分配许可证。 由于设置了分配缓冲区，因此启用了基于过去使用的动态共享。 根据分配缓冲区，当集群内有需求时，toyko_cl 最快会收到许可证令牌。 分配给 toronto_cl 的最小和最大分配为10 和 50，这也占最大份额。 局域网和动态广域网示例： Begin Feature NAME=verilog CLUSTER_DISTRIBUTION=MyWan(c1 1/1/25 c2 1/1/30 c3 2/5/100) MyLan(c1 1) ALLOC_BUFFER=c3 5 default 2 End Feature 在此示例中，verilog 许可证功能可从 WAN 和 LAN 服务域使用，但是只有集群 c1 从两个服务器都接收许可证功能。 最初根据分配的份额，分配 WAN 服务域中的许可证。 由于设置了分配缓冲区，因此启用了基于过去使用的动态共享。 当集群内有需求时，基于分配缓冲区，集群 c3 最快接收许可证令牌。 配置许可证特征 任务说明 每种许可证类型都需要 lsf.licensescheduler 文件中的 “Feature” 部分。 步骤 使用 NAME 参数定义许可证管理器,用来标识许可证类型的功能名称。 （可选）使用 LM_LICENSE_NAME 参数在 LSF License Scheduler 和许可证管理器特征名称之间定义别名。 来指定许可证管理器功能名称和 NAME 参数以定义 LSF License Scheduler 别名。 如果 LSF License Scheduler 令牌名称与许可证管理器特征名称不同，或者对于以数字开头，或包含连字符（-）的许可证管理器特征名称 (这种方式 LSF 不支持)，则只需指定 LM_LICENSE_NAME。 如果许可证管理器特征名称为 AppZ201，并且您打算使用与 LSF License Scheduler 令牌名称相同的名称，则按如下所示定义 NAME 参数： Begin Feature NAME=AppZ201 End Feature 如果许可证管理器特征名称为 201-AppZ，则 LSF 不支持此功能，因为这个特征名称以数字开头并包含连字符。 因此，将 AppZ201 定义为 201-AppZ 许可证管理器特征名称的别名，如下所示： Begin Feature NAME=AppZ201 LM_LICENSE_NAME=201-AppZ End Feature （可选）通过将 LM_LICENSE_NAME 中的多个许可证管理器特征名称，指定为以空格分隔的列表，将多个可互换的许可证管理器特征，组合到一个 LSF 许可证调度程序别名中。 在此示例中，两个名为 201-AppZ 和 202-AppZ 的许可证管理器特征，组合到一个名为 AppZ201 的别名中。 Begin Feature NAME=AppZ201 LM_LICENSE_NAME=201-AppZ 202-AppZ End Feature AppZ201 是同时使用 201-AppZ 和 202-AppZ 令牌的组合功能。 提交带有 rusage 字符串中的 AppZ201 的作业（例如，bsub -Lp Lp1-R “rusage [AppZ201 = 2]” myjob）意味着该作业检查了 201-AppZ 或 202-AppZ 的令牌。 在集群模式下配置Taskman作业 任务说明 （可选）要在集群模式下运行 Taskman（交互式）作业，请在服务域配置中包括交互式的虚拟集群。 步骤 在 Feature 部分中： 在 CLUSTER_DISTRIBUTION 参数中包括交互式的虚拟集群。 为虚拟集群交互设置共享。 （可选）为虚拟集群交互，设置分配缓冲区以启用动态分配。 示例 Begin Feature NAME=licenseA CLUSTER_DISTRIBUTION=MyLanServer(tokyo_cl 1 interactive 1) End Feature Begin Feature NAME=licenseB CLUSTER_DISTRIBUTION=MyWanServer(tokyo_cl 1 newyork_cl 1 interactive 2) End Feature 将许可证分配给非 LSF 作业 任务说明 仅适用于 WAN 服务域。 步骤 在 “Feature” 部分中设置 WORKLOAD_DISTRIBUTION，以分配许可证供非 LSF 使用。 WORKLOAD_DISTRIBUTION=service_domain_name(LSF lsf_distribution NON_LSF non_lsf_distribution) 如果在集群模式下，为 LAN 服务域设置了 WORKLOAD_DISTRIBUTION，则将忽略该参数。 示例 例如，要预留 20％ 的许可证以在 LSF 之外使用： Begin Feature NAME=licenseB CLUSTER_DISTRIBUTION=MyWanServer(tokyo_cl 1 newyork_cl 1) WORKLOAD_DISTRIBUTION=MyWanServer(LSF 8 NON_LSF 2) End Feature 重新启动以实施配置更改 步骤 运行 bladmin reconfig 以重新启动 bld。 如果删除了任何 Feature 部分，请重新启动 mbatchd。 在这种情况下，会将一条消息写入日志文件，提示重新启动。 如果需要，请运行 badmin mbdrestart 重新启动每个 LSF 集群。 查看许可证分配 步骤 运行 blstat -t token_name 以查看特定许可证令牌的信息（在 “Feature” 部分中配置）。 blstat 输出对于集群模式和项目模式有所不同。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Configure cluster mode with guarantees.html":{"url":"chapter10/section4/Configure cluster mode with guarantees.html","title":"保证配置集群模式","keywords":"","body":"保证配置集群模式 集群模式在 LSF 集群之间分配许可证。 要为集群中的项目保证许可证资源，并在不使用许可证资源时借用许可证资源，请使用 LSF 保证类型的 SLA。 集群模式下的保证和借贷，类似于项目模式下的非共享许可证和所有权。 保证为属于集合使用者的作业提供了特定的资源（例如主机）。 作业在可能的情况下以保证的资源运行。 使用保证资源时，将按照配置的其他任何调度功能，在保证范围之外运行作业。 保证条件，在保证资源池中配置。 确保在 LSF 中配置了 SLA。 有关更多信息，请参阅《Administering IBM Spectrum LSF》和《IBM Spectrum LSF Configuration Reference》。 配置服务类别 任务说明 服务类允许访问有保证的资源。 为集群中的每个许可证项目配置服务类。 步骤 在 lsb.serviceclasses 文件中配置每个 ServiceClass 部分。 以 Begin ServiceClass 行开始，以 End ServiceClass 行结束。 对于每个服务类，必须指定： NAME：服务类的名称。 GOALS = [GUARANTEE] ServiceClass 部分的可选参数是 ACCESS_CONTROL，AUTO_ATTACH 和 DESCRIPTION。 您可以根据需要配置任意多个服务类部分。 重要 用于服务类的名称不能与现有主机分区或用户组名称相同。 Begin ServiceClass NAME = sla1 GOALS = [GUARANTEE] ACCESS_CONTROL=LIC_PROJECTS[ proj1 ] DESCRIPTION = A guarantee SLA with access restricted to the license project proj1. End ServiceClass 自动将作业附加到服务类别 任务说明 设置可选参数 AUTO_ATTACH 时，作业将自动附加到服务类。 如果未设置自动附件，则可以通过运行 bsub -sla serviceclass_name 将作业提交到服务类。 如果作业可以访问多个具有自动附件集的 SLA，则该作业将按照配置文件的顺序，附加到第一个有效的 SLA。 步骤 在 lsb.serviceclasses 文件的 ServiceClass 部分中设置 AUTO_ATTACH=Y。 Begin ServiceClass NAME = sla1 GOALS = [GUARANTEE] ACCESS_CONTROL=LIC_PROJECTS[ proj1 ] AUTO_ATTACH=Y DESCRIPTION = A guarantee SLA with access restricted to the license project proj1. Jobs submitted to proj1 are attached to the SLA automatically and run on guaranteed resources if possible. End ServiceClass 配置许可证令牌的资源池 任务说明 有保证的资源池为消费者提供了最低的资源保证，并且可以有选择地借出未使用的有保证的资源。 保证的资源池在 lsb.resources 中定义，并由 lsb.serviceclasses 的 ServiceClass 部分中定义的使用者使用。 步骤 在 lsb.resources 中配置 GuaranteedResourcePool 部分。从 Begin GuaranteedResourcePool 行开始，到 End GuaranteedResourcePool 行结尾。 指定以下参数： NAME: 保证资源池的名称。 TYPE: 担保类型。 对于许可证，请使用资源类型，并包括许可证功能的名称。 DISTRIBUTION: 使用资源池共享所有服务类的分配。 可以是百分比或绝对数字。 资源的 GuaranteedResourcePool 部分的可选参数是 LOAN_POLICIES 和 DESCRIPTION。 您可以根据需要配置任意数量的资源池。 一个资源池可由多个 SLA 使用，一个 SLA 可以访问多个资源池。 Begin GuaranteedResourcePool NAME = hspice_guarantees TYPE = resource[hspice] DISTRIBUTION = ([proj1_sc,50%][proj2_sc,50%]) DESCRIPTION = A resource pool of hspice licenses controlled by License Scheduler and used by proj1_sc and proj2_sc. End GuaranteedResourcePool 配置借贷 任务说明 使用集群模式时，建议使用未使用的担保借贷。 禁用借贷后，请使用静态许可证分配策略。 配置后，未使用的许可证资源将根据借用策略借出。 借用策略允许特定队列访问保证资源池中未使用的资源。 步骤 在 lsb.resources 中使用所需的 NAME，TYPE 和 DISTRIBUTION 参数配置保证的资源池。 将借贷策略添加到保证的资源池中。 使用 LOAN_POLICIES=QUEUES [queue_name] 指定哪些队列可以访问借出的资源。 使用关键字 all 可以从任何队列中借用到作业。 例如，以下策略允许从队列 my_queue 借给作业： Begin GuaranteedResourcePool ... LOAN_POLICIES = QUEUES[my_queue] ... End GuaranteedResourcePool 配置短作业的借贷 任务说明 可以根据作业运行时间或估计的运行时间来限制借贷。 步骤 将策略 DURATION [minutes] 添加到 lsb.resources 中保证的资源池配置中，其中 minutes 是整数。 使用 DURATION 设置作业借用资源的最大作业运行时限制（或估计的运行时间，以较短者为准）。 完全省略 DURATION，以允许具有任何运行时间的作业从担保中借用。 例如，以下策略允许从运行时间在 10 分钟或更短时间内的任何队列中的作业借出资源： Begin GuaranteedResourcePool ... LOAN_POLICIES = QUEUES[all] DURATION[10] ... End GuaranteedResourcePool 配置借贷以在作业等待有保证的资源时停止 任务说明 可以限制借贷，以便仅当拥有未使用保证资源的使用者，没有待处理的负载时，作业才能访问借出的资源。 在运行需要多个许可证的作业时，限制借贷非常有用。 启用受限借贷后，借出单个许可证，不会延迟等待许可证资源累积的作业。 步骤 将策略 CLOSE_ON_DEMAND 添加到 lsb.resources 中的保证资源池配置中。 Begin GuaranteedResourcePool ... LOAN_POLICIES = QUEUES[queue1] CLOSE_ON_DEMAND ... End GuaranteedResourcePool 配置有权访问所有保证资源的队列 任务说明 可以配置具有高优先级的队列（例如管理员测试队列）以访问所有保证的资源，而不管 SLA 的需求如何。 步骤 使用 SLA_GUARANTEES_IGNORE = Y 参数在 lsb.queues 文件中配置队列。 提示 使用 SLA_GUARANTEES_IGNORE = Y 不能达到保证资源的目的。 请仅对低流量队列使用此参数。 重新启动以使更改生效 任务说明 必须启用集群模式，并且必须重新启动 LSF 集群才能使 LSF 配置更改生效。 步骤 在 lsf.licensescheduler 的 Parameters 部分中，确认启用了集群模式（CLUSTER_MODE = Y）。 运行 badmin mbdrestart 重新启动每个 LSF 集群。 运行 bladmin reconfig 重新启动 bld。 查看保证的资源池 任务说明 保证的资源池配置包括资源类型，以及在相应服务类中定义的使用者之间的分配。 步骤 运行 bresources -g -l -m 以查看保证的资源池配置的详细信息，包括资源池中当前主机的列表。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Project mode with projects.html":{"url":"chapter10/section4/Project mode with projects.html","title":"项目模式与项目","keywords":"","body":"Project mode with projects You can configure license distribution when you are running license projects in project mode. Each distribution policy is applied locally, within service domains. TipAlthough license projects are not the same as LSF projects, you can map your license project names to LSF project names for easier monitoring. Configure parameters Procedure Project mode can be set globally, or for individual license features. Set individually when you are using project mode for some features and cluster mode for some features. If you are using project mode for all license features, define CLUSTER_MODE=N in the Parameters section of lsf.licensescheduler. If you are using project mode for some license features, define CLUSTER_MODE=N for individual license features in the Feature section of lsf.licensescheduler. The Feature section setting of CLUSTER_MODE overrides the global Parameter section setting. List the License Scheduler hosts. By default with an LSF installation, the HOSTS parameter is set to the LSF_MASTER_LIST. List the hosts in order from most preferred to least preferred. The first host is the master license scheduler host. Specify a fully qualified host name such as hostX.mycompany.com unless all your License Scheduler clients run in the same DNS domain. HOSTS=host1 host2 Specify the data collection frequency between License Scheduler and the license manager. The default is 30 seconds. LM_STAT_INTERVAL=seconds Specify the file paths to the commands for the license managers that you are using. If you are using FlexNet, specify the path to the FlexNet command lmutil (or lmstat). For example, if lmstat is in /etc/flexlm/bin: LMSTAT_PATH=/etc/flexlm/bin If you are using Reprise License Manager, specify the path to the Reprise License Manager command rlmutil (or rlmstat). For example, if rlmstat is in /etc/rlm/bin: RLMSTAT_PATH=/etc/rlm/bin Configure clusters About this task Configure the clusters that are permitted to use License Scheduler in the Clusters section of the lsf.licensescheduler file. This configuration is only required if you are using more than one cluster. Procedure In the Clusters section, list all clusters that can use License Scheduler. For example: Begin Clusters CLUSTERS cluster1 cluster2 End Clusters Configure projects About this task Each project that is defined in a Projects section of lsf.licensescheduler can have a distribution policy that is applied in the Feature section, where projects can be associated with license features. Procedure Define the projects with or without priority. Begin Projects PROJECTS PRIORITY Lp1 3 Lp2 1 Lp3 2 default 0 End Projects The higher the number, the higher the priority. When two projects have the same priority number that is configured, the first listed project has a higher priority. Priority is taken into account when license preemption occurs, where lower priority projects are preempted first. If not explicitly configured, the default project has the priority of 0. A default project is used when no license project is specified during job submission. Add project description About this task Optionally, you can add a project description of up to 64 characters to your projects to help identify them. Procedure In the Project section of lsf.licensescheduler, find the project and add a description in the DESCRIPTION column. For example: Begin Projects PROJECTS PRIORITY DESCRIPTION p1 10 \"Engineering project 123\" p2 9 \"QA build project 2C\" P3 8 \"\" End Projects Results When you are running blinfo -Lp or blinfo -G, any existing project descriptions display. Project mode service domains A service domain is a group of one or more license servers. License Scheduler manages the scheduling of the license tokens, but the license server actually supplies the licenses. You must configure at least one service domain for License Scheduler. In project mode, each cluster can access licenses from multiple WAN and LAN service domains. License Scheduler collects license availability and usage from license server hosts, and merges this information with license demand and usage information from LSF clusters to make distribution and preemption decisions. NoteUnless you require multiple service domains for some specific reason, configure both modes with at most one LAN and one WAN for each feature in a cluster. Because License Scheduler does not control license checkout, running with one cluster that is accessing multiple service domains is not optimal. Configure service domains About this task You configure each service domain, with the license server names and port numbers that serve licenses to a network, in the ServiceDomain section of the lsf.licensescheduler file. Procedure Add a ServiceDomain section, and define NAME for each service domain. For example: Begin ServiceDomain NAME=DesignCenterA End ServiceDomain Specify the license server hosts for that domain, including the host name and license manager port number. For example: Begin ServiceDomain NAME=DesignCenterA LIC_SERVERS=((1700@hostA)) End ServiceDomain For multiple license servers: LIC_SERVERS=((1700@hostA)(1700@hostB)) For redundant servers, the parentheses are used to group the three hosts that share license.dat file: LIC_SERVERS=((1700@hostD 1700@hostE 1700@hostF)) NoteIf FlexNet uses a port from the default range, you can specify the host name without the port number. See the FlexNet documentation for the values of the default port range. LIC_SERVERS=((@hostA)) Configure remote license server hosts Before you begin If you are using FlexNet as a license manager, the FlexNet license server hosts must have lmutil (or lmstat) in the LMSTAT_PATH directory before configuring these hosts with LSF License Scheduler. If you are using Reprise License Manager, the Reprise License Manager license server hosts must have rlmutil (or rlmstat) directory before configuring these hosts with LSF License Scheduler. About this task The license collector (blcollect) is a multi-threaded daemon that queries all license servers under LSF License Scheduler for license usage information. The license collector calls lmutil or lmstat (for FlexNet), or rlmutil or rlmstat (for Reprise License Manager) to collect information from each license server. When there are both local and remote license servers (that is, license servers that are in a different subnet from the host running blcollect), the threads that collect information from the remote license servers are slower than the threads that collect information from local license servers. If there are remote license servers, designate at least one remote license server within each domain as a remote agent host. The license collector connects to the remote agent host and calls lmutil, lmstat, rlmutil, or rlmstat on the remote agent host and gets license information from all license servers that the remote agent host serves. The remote agent host and the remote license servers should be in the same domain to improve access. Procedure Select the connection method for the license collector to connect to remote hosts. LSF License Scheduler supports the use of ssh, rsh, and lsrun to connect to remote hosts. If using lsrun as the connection method, the agent host must be a server host in the LSF cluster and RES must be started on this host. Otherwise, if using ssh or rsh as the connection method, the agent host does not have to be a server host in the LSF cluster. In the Parameters section, define the REMOTE_LMSTAT_PROTOCOL parameter and specify the connection command (and command options, if required) to connect to remote servers. REMOTE_LMSTAT_PROTOCOL=ssh [ssh_command_options] | rsh [rsh_command_options] | lsrun [lsrun_command_options] The default connection method is ssh with no command options. LSF License Scheduler uses the specified command (and optional command options) to connect to the agent host. LSF License Scheduler automatically appends the name of the agent host to the command, so there is no need to specify the host with the command. NoteLSF License Scheduler does not validate the specified command, so you must ensure that you correctly specify the command. Any connection errors are noted in the blcollect log file. If the connection method is ssh or rsh, verify that this connection method is configured so the host running the license collector can connect to remote hosts without specifying a password. Define remote license servers and remote agent hosts. In the ServiceDomain section, define the REMOTE_LMSTAT_SERVERS parameter: REMOTE_LMSTAT_SERVERS=host_name[(host_name ...)] [host_name[(host_name ...)] ...] Specify a remote agent host, then any license servers that it serves in parentheses. The remote agent host and the license servers that it serves must be in the same subnet. If you specify a remote agent host by itself without any license servers (for example, REMOTE_LMSTAT_SERVERS=hostA), the remote agent host is considered to be a remote license server with itself as the remote agent host. That is, the license collector connects to the remote agent host and only gets license information on the remote agent host. You can specify multiple remote agent hosts to serve multiple subnets, or multiple remote agent hosts to serve specific license servers within the same subnet. Any host that you specify here must be a license server defined in LIC_SERVERS. Any hosts defined in REMOTE_LMSTAT_SERVERS that are not also defined in LIC_SERVERS are ignored. The following examples assume that the license collector (blcollect) is running on LShost1. That is, the following parameter is specified in the Parameters section: Begin Parameters ... HOSTS=LShost1 ... End Parameters One local license server (hostA) and one remote license server (hostB): LIC_SERVERS=((1700@hostA)(1700@hostB)) REMOTE_LMSTAT_SERVERS=hostB The license collector runs lmutil, lmstat, rlmutil, or rlmstat directly on hostA to get license information on hostA. Because hostB is defined without additional license servers, hostB is a remote agent host that only serves itself. The license collector connects to hostB (using the command specified by the REMOTE_LMSTAT_PROTOCOL parameter) and runs lmutil, lmstat, rlmutil, or rlmstat to get license information on 1700@hostB. One local license server (hostA), one remote agent host (hostB) that serves one remote license server (hostC), and one remote agent host (hostD) that serves two remote license servers (hostE and hostF): LIC_SERVERS=((1700@hostA)(1700@hostB)(1700@hostC)(1700@hostD)(1700@hostE)(1700@hostF)) REMOTE_LMSTAT_SERVERS=hostB(hostC) hostD(hostE hostF) The license collector runs lmutil, lmstat, rlmutil, or rlmstat directly to get license information from 1700@hostA, 1700@hostB, and 1700@hostD. The license collector connects to hostB (using the command specified by the REMOTE_LMSTAT_PROTOCOL parameter) and runs lmutil, lmstat, rlmutil, or rlmstat to get license information on 1700@hostC hostB and hostC should be in the same subnet to improve access. The license collector connects to hostD (using the command specified by the REMOTE_LMSTAT_PROTOCOL parameter) and runs **lmutil**, **lmstat**, **rlmutil**, or **rlmstat** to get license information on 1700@hostE and 1700@hostF . hostD, hostE, and hostF should be in the same subnet to improve access. One local license server ( hostA ), one remote license server ( hostB ), and one remote agent host ( hostC ) that serves two remote license servers ( hostD and hostE ): ``` LIC_SERVERS=((1700@hostA)(1700@hostB)(1700@hostC)(1700@hostD)(1700@hostE)) REMOTE_LMSTAT_SERVERS=hostB hostC(hostD hostE) - The license collector runs **lmutil**, **lmstat**, **rlmutil**, or **rlmstat** directly to get license information on 1700@hostA and 1700@hostC. - The license collector connects to hostB (using the command specified by the **REMOTE_LMSTAT_PROTOCOL** parameter) and runs **lmutil**, **lmstat**, **rlmutil**, or **rlmstat** to get license information on 1700@hostB. - The license collector connects to hostC (using the command specified by the REMOTE_LMSTAT_PROTOCOL parameter) and runs **lmutil**, **lmstat**, **rlmutil**, or **rlmstat** to get license information on 1700@hostD and 1700@hostE . hostC, hostD, and hostE should be in the same subnet to improve access. ## Configure license features ### About this task Each type of license requires a Feature section in the lsf.licensescheduler file. The Feature section includes the license distribution policy. ### Procedure 1. Define the feature name that is used by the license manager to identify the type of license by using the **NAME** parameter. Optionally, define an alias between LSF License Scheduler and the license manager feature names by using the **LM_LICENSE_NAME** parameter to specify the license manager feature name and the **NAME** parameter to define the LSF License Scheduler alias. You only need to specify **LM_LICENSE_NAME** if the LSF License Scheduler token name is not identical to the license manager feature name, or for license manager feature names that either start with a number or contain a hyphen character (-), which are not supported in LSF. If the license manager feature name is AppZ201 and you intend to use this same name as the LSF License Scheduler token name, define the **NAME** parameter as follows: Begin Feature NAME=AppZ201 End Feature If the license manager feature name 201-AppZ, this is not supported in LSF because the feature name starts with a number and contains a hyphen. Therefore, define AppZ201 as an alias of the 201-AppZ license manager feature name as follows: Begin Feature NAME=AppZ201 LM_LICENSE_NAME=201-AppZ End Feature 2. Optionally, combine multiple interchangeable license manager features into one LSF License Scheduler alias by specifying multiple license manager feature names in **LM_LICENSE_NAME** as a space-delimited list. In this example, two license manager features named 201-AppZ and 202-AppZ are combined into an alias named AppZ201. Begin Feature NAME=AppZ201 LM_LICENSE_NAME=201-AppZ 202-AppZ End Feature AppZ201 is a combined feature that uses both 201-AppZ and 202-AppZ tokens. Submitting a job with AppZ201 in the rusage string (for example, bsub -Lp Lp1 -R \"rusage[AppZ201=2]\" myjob) means that the job checks out tokens for either 201-AppZ or 202-AppZ. 3. Define a distribution policy. A distribution policy defines the license fairshare policy in the format: DISTRIBUTION = ServiceDomain1 (project1 share_ratio project2 share_ratio ...) ServiceDomain2 (project3 share_ratio ...) For example, a basic configuration assigns shares: Begin Feature LM_LICENSE_NAME=201-AppZ NAME=AppZ201 DISTRIBUTION = DesignCenterA (LpA 2 LpB 1 default 1) End Feature LpA has the right to twice as many licenses as LpB. Jobs that are submitted without a license project that is specified can run under the default project. 4. Optionally, add owned licenses to the distribution policy in the format: DISTRIBUTION = ServiceDomain1 (project1 share_ratio/number_owned project2 share_ratio/number_owned ...) ServiceDomain2 (project3 share_ratio ...) If LS_FEATURE_PERCENTAGE=Y or LS_ACTIVE_PERCENTAGE=Y in lsf.licensescheduler, number_owned is expressed as a percentage of the total licenses. Example 1: DISTRIBUTION = LanServer(Lp1 1 Lp2 1/10) This example assumes that there are 10 licenses in total, all owned by Lp2. The two License Scheduler projects, Lp1 and Lp2, and share the licenses, but grant ownership of the licenses to one of the projects (Lp2). When Lp2 has no work to be done, Lp1 can use the licenses. When Lp2 has work to do, Lp1 must return the license immediately to Lp2. The license utilization is always at the maximum, showing that all licenses are in use even while the license distribution policies are being enforced. Example 2: DISTRIBUTION=LanServer1(Lp1 1 Lp2 2/6) Lp1 is set to use one third of the available licenses and Lp2 to use two thirds of the licenses. However, Lp2 is always entitled to six licenses and preempts other license project jobs when licenses are needed immediately. If the projects are competing for a total of 12 licenses, Lp2 is entitled to eight (six on demand, and two more as soon as they are free). If the projects are competing for only six licenses in total, Lp2 is entitled to all of them, and Lp1 can use licenses only when Lp2 does not need them. ## Track partial and unspecified license use ### About this task When you want to manage licenses not included in job resource requirements or have applications that you know use licenses for only part of the length of each job, use these optional settings. ### Procedure 1. Optionally, specify **DYNAMIC=Y** to consider the license feature as a dynamic resource when it is only used for part of the job. Set **DYNAMIC=Y** for applications with known license use that do not use the license for the entire length of the job. Jobs are submitted with **duration** specified, then release the license when not in use. Begin Feature NAME = p1_2 DISTRIBUTION= Lan1 (a 1 b 1 c 1 default 1) DYNAMIC=Y End Feature For example, a **taskman** job submission with **duration**: taskman -R \"rusage[p1_2=1:duration=2]\" myjob 2. Optionally, set **ENABLE_DYNAMIC_RUSAGE=Y** in the Feature section of lsf.licensescheduler to track license use of license features not specified at job submission. For example: Begin Feature NAME = feat2 DISTRIBUTION = LanServer(proj1 1 default 1) ENABLE_DYNAMIC_RUSAGE = y End Feature Submit a job to run the application, specifying the license feature name: bsub -R \"rusage[feat1=1]\" -Lp proj1 app1 The job runs and license feat1 is checked out: blstat FEATURE: feat1 SERVICE_DOMAIN: LanServer TOTAL_INUSE: 1 TOTAL_RESERVE: 0 TOTAL_FREE: 4 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND proj1 50.0 % 0 1 0 2 0 default 50.0 % 0 0 0 2 0 FEATURE: feat2 SERVICE_DOMAIN: LanServer TOTAL_INUSE: 0 TOTAL_RESERVE: 0 TOTAL_FREE: 10 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND proj1 50.0 % 0 0 0 5 0 default 50.0 % 0 0 0 5 0 blusers -l FEATURE SERVICE_DOMAIN USER HOST NLICS NTASKS OTHERS DISPLAYS PIDS feat1 LanServer user1 hostA 1 1 0 (/dev/tty) (16326) blusers -J JOBID USER HOST PROJECT CLUSTER START_TIME 1896 user1 hostA proj1 cluster1 Aug 9 10:01:25 RESOURCE RUSAGE SERVICE_DOMAIN INUSE EFFECTIVE_PROJECT feat1 1 LanServer 1 proj1 Later, app1 checks out feature feat2. Since it was not specified at job submission, feat2 is a class C license checkout. But since it is configured with **ENABLE_DYNAMIC_RUSAGE=Y**, jobs that require feat2 are considered managed workload, and subject to the distribution policies of project proj1: blstat FEATURE: feat1 SERVICE_DOMAIN: LanServer TOTAL_INUSE: 1 TOTAL_RESERVE: 0 TOTAL_FREE: 4 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND proj1 50.0 % 0 1 0 2 0 default 50.0 % 0 0 0 2 0 FEATURE: feat2 SERVICE_DOMAIN: LanServer TOTAL_INUSE: 1 TOTAL_RESERVE: 0 TOTAL_FREE: 9 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND proj1 50.0 % 0 1 0 4 0 default 50.0 % 0 0 0 5 0 blusers -l FEATURE SERVICE_DOMAIN USER HOST NLICS NTASKS OTHERS DISPLAYS PIDS feat1 LanServer user1 hostA 1 1 0 (/dev/tty) (16326) feat2 LanServer user1 hostA 1 1 0 (/dev/tty) (16344) blusers -J JOBID USER HOST PROJECT CLUSTER START_TIME 1896 user1 hostA proj1 cluser1 Aug 9 10:01:25 RESOURCE RUSAGE SERVICE_DOMAIN INUSE EFFECTIVE_PROJECT feat1 1 LanServer 1 proj1 feat2 1 LanServer 1 proj1 ## Restart to implement configuration changes ### Procedure 1. Run bladmin reconfig to restart the **bld**. 2. If you deleted any Feature sections, restart **mbatchd**. In this case, a message is written to the log file, prompting the restart. If required, run badmin mbdrestart to restart each LSF cluster. ## View projects and descriptions ### Procedure Run **blinfo -Lp** to view projects and descriptions. For example: blinfo -Lp PROJECT PRIORITY DESCRIPTION p1 10 Engineering project 123 p2 9 QA build project 2C P3 8``` View license allocation Procedure Run blstat -t token_name to view information for a specific license token (as configured in a Feature section). blstat output differs for cluster mode and project mode. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Project mode optional settings.html":{"url":"chapter10/section4/Project mode optional settings.html","title":"项目模式可选设置","keywords":"","body":"Project mode optional settings After you configure License Scheduler in project mode with projects or project groups, you can include some additional configuration that is not required, but can be useful. Active ownership With ownership defined, projects with demand for licenses are able to reclaim licenses up to the assigned ownership share for the project. With active ownership enabled, ownership is expressed as a percent of the total ownership for active projects, and the actual ownership for each project decreases as more projects become active. Active ownership allows ownership to automatically adjust based on project activity. Active ownership can be used with projects, groups of projects, and project groups. Set percentage ownership values to total more than 100% to benefit from active ownership. Configure active ownership About this task When active ownership is enabled, ownership settings for inactive projects are disregarded during license token distribution. Procedure Set LS_ACTIVE_PERCENTAGE=Y in the Feature section. All ownership values for inactive projects are set to zero, and if the total ownership percent exceeds 100%, the total ownership is adjusted. LS_FEATURE_PERCENTAGE=Y is automatically set, and owned and non-shared values are expressed in percent. If used with project groups, OWNERSHIP, LIMITS and NON_SHARED are expressed in percent. Set the percentage of owned licenses in the DISTRIBUTION parameter (Feature section) for a total percentage that exceeds 100%. ... DISTRIBUTION=wanserver (Lp1 2/50 Lp2 1/30 Lp3 2/30 Lp4 3/30) LS_ACTIVE_PERCENTAGE=Y ... In this example, all four license projects are configured with a share and an owned value. Lp1 has the greatest number of owned licenses, and can use preemption to reclaim the most licenses. If only Lp1 is active, Lp1 owns 50% of licenses. Total active ownership is 50%, so no adjustment is made. If Lp1 and Lp2 are active, Lp1 owns 50% and Lp2 owns 30%. Total active ownership is 80%, so no adjustment is made. If Lp1, Lp2, and Lp3 are active, Lp1 owns 50%, Lp2 owns 30%, and Lp3 owns 30%. Total active ownership is 110%, so ownership is scaled to result in Lp1 owning 46%, Lp2 owning 27%, and Lp3 owning 27% (Exact numbers are rounded). If all projects are active, the total active ownership is 140%. Ownership is scaled to result in Lp1 owning 37%, Lp2 owning 21%, Lp3 owning 21%, and Lp4 owning 21% (Exact numbers are rounded). Default projects Jobs requiring a license feature but not submitted to a license project for that feature are submitted to the default project. For jobs to run, a share of license tokens must be assigned to the default project. If you do not want the default project to get shares of license tokens, you do not have to define a default project in the distribution policy for a feature, however jobs in the default project become pending by default. To avoid having jobs that are submitted without a project pend, either assign shares to the default project, or disable default projects so jobs are rejected. Configure default project shares About this task Jobs cannot run in the default project unless shares are assigned. Procedure Define a default project in the Feature section DISTRIBUTION parameter. Any job that is submitted without a project name that is specified by -Lp can now use tokens from the default project. Disable default projects About this task License token jobs that are submitted without a project that is specified are accepted and assigned to the default project, unless your configuration specifies that such jobs be rejected. Procedure Optionally, set LSF_LIC_SCHED_STRICT_PROJECT_NAME=y in lsf.conf. Jobs that are submitted without a project that is specified are rejected, and the default license project is not used. Groups of projects If you configure groups of projects, you can set shares and ownership for each group and distribute license features to groups of projects. Configure a license project to belong only to one group. Preemption first occurs between groups of projects, and then occurs between projects. Preemption with groups of projects The following tables show changes in preemption behavior that is based on ownership that is configured for groups of projects, with a total of 20 licenses. With groups of projects that are configured, GroupA is able to preempt to reclaim 10 owned licenses. Since Lp2 is not using all five owned licenses, Lp1 can use more than the share it owns. Project license ownership only License project Licenses owned Licenses used Lp1 5 6 Lp2 5 0 Lp3 5 7 Lp4 5 7 Groups of projects with license ownership Group License projects Project licenses owned Licenses that are used after preemptions GroupA Lp1Lp2 55 91 GroupB Lp3Lp4 55 64 Configure group license ownership Procedure In lsf.licensescheduler, set the GROUP parameter in the Feature section. Set up groups and members. Begin Feature NAME = AppY DISTRIBUTION = LanServer1(Lp1 5/5 Lp2 5/5 Lp3 5/5 Lp4 5/5) GROUP = GroupA(Lp1 Lp2) GroupB (Lp3 Lp4) End Feature In this example, Lp1 and Lp2 belong to the group GroupA. Lp3 and Lp4 belong to the GroupB group. Configure interactive (taskman) jobs About this task By default, interactive (taskman) jobs do not receive a share of the license token allocation, while all clusters receive equal shares. You can allocate a share of all license features to interactive jobs in the Parameters section. Procedure To globally enable a share of the licenses for interactive tasks, you must set the ENABLE_INTERACTIVE in lsf.licensescheduler. In lsf.licensescheduler, edit the Parameters section: Begin Parameters ... ENABLE_INTERACTIVE = y ... End Parameters When the change in configuration takes effect, interactive tasks are allocated the same share (by default) as each cluster. Configure cluster and interactive allocations About this task By default in project mode, each cluster receives one allocation share from a license feature, and interactive tasks receive no shares. You can modify the allocation of license shares across clusters and to interactive tasks in individual Feature sections. Procedure In the Features section of lsf.licensescheduler, set the ALLOCATION parameter. ALLOCATION=project_name (cluster_name [number_shares] ... ) Allocation examples For example, this ALLOCATION setting matches the default when ALLOCATION is undefined and interactive jobs are enabled with ENABLE_INTERACTIVE=Y. An equal share is allocated to each cluster and to interactive jobs. Begin Feature NAME = AppX DISTRIBUTION = LanServer1 (Lp1 1) ALLOCATION = Lp1 (Cluster1 1 Cluster2 1 interactive 1) End Feature In this example, licenses are shared equally between cluster1 and interactive tasks, with cluster2 receiving nothing: Begin Parameters ... ENABLE_INTERACTIVE = y ... End Parameters Begin Feature NAME = AppY DISTRIBUTION = LanServer (Lp1 1) ALLOCATION = Lp1(cluster1 2 cluster2 0 interactive 2) End Feature In the following example, even though the global allocation to interactive jobs is disabled (ENABLE_INTERACTIVE = N), ALLOCATION defined in the Feature section can assign a share to interactive jobs for this license feature. Begin Feature NAME = AppZ DISTRIBUTION = LanServer (Lp1 1) ALLOCATION = Lp1(cluster1 0 cluster2 1 interactive 2) End Feature Given a total of 12 licenses, 4 are allocated to cluster2 and 8 are allocated to interactive tasks. Configure feature groups About this task Feature groups that are configured in one FeatureGroup section allow you to view the information for multiple features, which are grouped together. Procedure In lsf.licensescheduler, configure a FeatureGroup section, listing the license features associated with that license. Each FeatureGroup section must have a unique name. The feature names in FEATURE_LIST must already be defined in Feature sections. FEATURE_LIST cannot be empty or contain duplicate feature names. Features can be in more than one FeatureGroup section. Begin FeatureGroup NAME = Corporate FEATURE_LIST = ASTRO VCS_Runtime_Net Hsim Hspice End FeatureGroup Begin FeatureGroup NAME = Offsite FEATURE_LIST = Encounter NCSim NCVerilog End FeatureGroup Restart to implement configuration changes About this task Changes that are made in lsf.licensescheduler require restarting the bld. Changes that are made in lsf.conf require restating the LSF clusters. Procedure Run badmin mbdrestart to restart each LSF cluster. Run lsadmin limrestart or bladmin reconfig to restart the bld. View license feature group information About this task When FEATURE_LIST is configured for a group of license features in lsf.licensescheduler, you can view detailed information about the groups. Procedure Run blinfo -g or blstat -g. For example, if the feature group called myFeatureGroup1 has the members feature2 and feature3: blstat -g \"myFeatureGroup1\" Information displays for feature2 and feature3 in descending alphabetical order. Run blstat -g alone or with options -Lp, -t, -D ,-G, -s. Run blinfo -g alone or with options -a, -t, -C, and -A. License feature locality Use license feature locality to limit features from different service domains to a specific cluster so that License Scheduler does not grant tokens to jobs from license that legally cannot be used on the cluster that is requesting the token. How locality works Setting locality means that license resources requested from different clusters are mapped to different tokens in License Scheduler Features with different locality are treated as different tokens by License Scheduler. You must configure separate feature sections for same feature with different localities. NoteYou must make sure that your features are configured so that the applications always first try to check out licenses locally. When License Scheduler receives license requests from LSF, it knows where the request is from, and it interprets the request into demands for tokens usable by that cluster. For example, if clusterA sends a request to the bld asking for one hspice license, License Scheduler marks the demand for both hspice@clusterA and hspice. When the job gets either token to run, the demand is cleaned up for both tokens. Configure locality About this task Specify LOCAL_TO to limit features from different service domains to specific clusters, so License Scheduler grants tokens of a feature only to jobs from clusters that are entitled to them. For example, if your license servers restrict the serving of license tokens to specific geographical locations, use LOCAL_TO to specify the locality of a license token if any feature cannot be shared across all the locations. This specification avoids having to define different distribution and allocation policies for different service domains, and allows hierarchical group configurations. License Scheduler manages features with different localities as different resources. Procedure In lsf.licensescheduler’s Feature section, configure LOCAL_TO. For example: LOCAL_TO=Site1(clusterA clusterB) configures the feature for more than one cluster, where the cluster names are already defined in the Clusters section of lsf.licensescheduler. LOCAL_TO=clusterA configures locality for only one cluster. This is the same as LOCAL_TO=clusterA(clusterA). License Scheduler now treats license features that are served to different locations as different token names, and distributes the tokens to projects according to the distribution and allocation policies for the feature. (Optional) View locality settings. Run blinfo -A. The feature allocation by cluster locality displays. FEATURE PROJECT ALLOCATION hspice Lp1 [clusterA, 25.0%] [clusterB, 25.0%] [clusterC, 25.0%] [interactive, 25.0%] Lp2 [clusterA, 50.0%] [clusterB, 50.0%] hspice@clusterA Lp1 [clusterA, 100.0%] Lp2 [clusterA, 100.0%] hspice@siteB Lp1 [clusterB, 80.0%] [clusterC, 20%] Lp2 [clusterB, 80.0%] [clusterC, 20%] hspice@clusterC Lp1 [clusterC, 60.0%] [interactive, 40.0%] Lp2 [clusterC, 60.0%] [interactive, 40.0%] Lp3 [clusterC, 60.0%] [interactive, 40.0%] vcs Lp1 [clusterA, 33.0%] [clusterB, 33.0%] [interactive, 33.0%] Lp2 [clusterA, 50.0%] [clusterB, 50.0%] vcs@clusterA Lp1 [clusterA, 100.0%] Lp2 [clusterA, 100.0%] vcs@siteB Lp1 [clusterB, 80.0%] [clusterC, 20%] Lp2 [clusterB, 80.0%] [clusterC, 20%] vcs@clusterC Lp1 [clusterC, 60.0%] [interactive, 40.0%] Lp2 [clusterC, 60.0%] [interactive, 40.0%] Lp3 [clusterC, 60.0%] [interactive, 40.0%] Run blinfo -C. The cluster locality information for the features displays. NAME: hspice LM_LICENSE_NAME: hspice CLUSTER_NAME FEATURE SERVICE_DOMAINS clusterA hspice SD3 SD4 hspice@clusterA SD1 clusterB hspice SD3 SD4 hspice@siteB SD3 clusterC hspice SD3 SD4 hspice@siteB SD3 hspice@clusterC SD5 NAME: vcs LM_LICENSE_NAME: VCS_Runtime CLUSTER_NAME FEATURE SERVICE_DOMAINS clusterA vcs SD3 SD4 vcs@clusterA SD1 clusterB vcs SD3 SD4 vcs@siteB SD3 clusterC vcs SD3 SD4 vcs@siteB SD3 vcs@clusterC SD5 Run blusers. FEATURE SERVICE_DOMAIN USER HOST NLICS NTASKS hspice@clusterA SD1 user1 host1 1 1 hspice@siteB SD2 user2 host2 1 1 Run blstat. FEATURE: hspice SERVICE_DOMAIN: SD3 SD4 TOTAL_INUSE: 0 TOTAL_RESERVE: 0 TOTAL_FREE: 22 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND Lp1 50.0 % 0 0 0 11 0 Lp2 50.0 % 0 0 0 11 0 FEATURE: hspice@clusterA SERVICE_DOMAIN: SD1 TOTAL_INUSE: 0 TOTAL_RESERVE: 0 TOTAL_FREE: 25 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND Lp1 50.0 % 0 0 0 12 0 Lp2 50.0 % 0 0 0 13 0 FEATURE: hspice@siteB SERVICE_DOMAIN: SD2 TOTAL_INUSE: 0 TOTAL_RESERVE: 0 TOTAL_FREE: 65 OTHERS: 0 PROJECT SHARE OWN INUSE RESERVE FREE DEMAND Lp1 50.0 % 0 0 0 32 0 Lp2 50.0 % 0 0 0 33 0 Run bhosts -s. Different resource information displays depending on the cluster locality of the features. From clusterA: RESOURCE TOTAL RESERVED LOCATION hspice 36.0 0.0 host1 From clusterB in siteB: RESOURCE TOTAL RESERVED LOCATION hspice 76.0 0.0 host2 Example configuration: two sites and four service domains Some of your service domains may have geographical restrictions when the domains are serving licenses. In this example, two clusters in one location can run hspice jobs. and four service domains are defined for the hspice feature: SD1 is a local license file for clusterA with 25 hspice licenses SD2 is a local license file for clusterB with 65 hspice licenses SD3 is a WANable license with 15 hspice licenses SD4 is a globally WANable license with seven hspice licenses The geographical license checkout restrictions are: Jobs in clusterA can check out licenses from SD1 SD3 and SD4 but not SD2 Jobs in clusterB can check out licenses from SD2 SD3 and SD4 but not SD1 Begin Feature NAME = hspice DISTRIBUTION = SD1 (Lp1 1 Lp2 1) LOCAL_TO = clusterA End Feature Begin Feature NAME = hspice DISTRIBUTION = SD2 (Lp1 1 Lp2 1) LOCAL_TO = clusterB End Feature Begin Feature NAME = hspice DISTRIBUTION = SD3 (Lp1 1 Lp2 1) SD4 (Lp1 1 Lp2 1) End Feature Or use the hierarchical group configuration (GROUP_DISTRIBUTION): Begin Feature NAME = hspice GROUP_DISTRIBUTION = group1 SERVICE_DOMAINS = SD1 LOCAL_TO = clusterA End Feature Begin Feature NAME = hspice GROUP_DISTRIBUTION = group1 SERVICE_DOMAINS = SD2 LOCAL_TO = clusterB End Feature Begin Feature NAME = hspice GROUP_DISTRIBUTION = group1 SERVICE_DOMAINS = SD3 SD4 End Feature Submit jobs that use locality Before you begin LOCAL_TO is configured in lsf.licensescheduler. About this task Job submission is simplified when locality is configured. Procedure Specify the resource usage string with the same resource name you see in bhosts -s. No OR rusage string is needed. For example: bsub -Lp Lp1 -R \"rusage[hspice=1]\" myjob How locality works with other settings The following table shows various combinations of LOCAL_TO and other feature section parameters: NAME LM_LICENSE_NAME 1 AppX - 2 AppZ201 201-AppZ 3 AppB_v1 AppB You can define different License Scheduler tokens for the same FlexNet feature. The service domain names (in either the DISTRIBUTION line or the SERVICE_DOMAINS for group configurations) of the same FlexNet feature in different feature sections must be exclusive. They cannot overlap. When LOCAL_TO is configured for a feature, you can define different License Scheduler tokens for the same FlexNet feature with different localities. The constraints are: For the same FlexNet feature, service domains must be exclusive. The location name of LOCAL_TO defines the locality of that feature, so the name must be unique for all tokens with same FlexNet feature. Use same location name for different FlexNet features with the same pattern of locality, but License Scheduler does not check whether the same location name of a different feature contains the same list of clusters. Features must either have a different NAME or have LOCAL_TO defined. The service domains for each License Scheduler token of same FlexNet feature must be exclusive. How locality works with ALLOCATION and ENABLE_INTERACTIVE The LOCAL_TO parameter simplifies the ALLOCATION configuration. Most of the time you are only interested in who can participate to share a particular token. LOCAL_TO gives the equal share for all the clusters that are defined in LOCAL_TO and applies to all the projects. Use ALLOCATION to fine-tune the shares for individual projects between different clusters: Except for the keyword interactive, all the cluster names that are defined in ALLOCATION must also be defined in the LOCAL_TO parameter. The global parameter ENABLE_INTERACTIVE and ALLOCATION with interactive share defined works same as before. If ALLOCATION is configured, it ignores the global setting of the ENABLE_INTERACTIVE parameter. If ALLOCATION is not defined, but LOCAL_TO is defined, the default value for ALLOCATION is equal shares for all the clusters defined in LOCAL_TO parameter. This share applies to all license projects defined in DISTRIBUTION or GROUP_DISTRIBUTION. If both ALLOCATION and LOCAL_TO are defined, ALLOCATION parameter can be used to fine-tune the shares between the clusters for different projects. The following table shows example configurations with two clusters and 12 hspice licenses distributed as follows: DISTRIBUTION = LanServer (Lp1 1 Lp2 1) ENABLE_INTERACTIVE LOCAL_TO ALLOCATION No SiteA(clusterA interactive) - No clusterA Lp1(clusterA 1 clusterB 0) No clusterA Lp1(clusterA 1)Lp2(clusterA 1) About interactive taskman jobs The License Scheduler command taskman is a job starter for taskman jobs to use License Scheduler without bsub. taskman checks out a license token and manages interactive UNIX applications. You can use the logical AND operator (:) to combine rusage strings and the logical OR operator (||) to separate rusage string siblings. For example: taskman -Lp P1 -R \"rusage[f1=1:f2=1||f1=5:f3=1||f4=1]\" myjob If you specify multiple rusage string siblings, LSF License Scheduler checks each of the rusage string siblings from left to right. If at least one of the rusage string sibling requirements are met, the task can start. If none of the rusage string sibling requirements are met, LSF License Scheduler sends the DEMAND of all the unsatisfied rusage string siblings. If a particular unsatisfied resource is specified in multiple rusage string siblings, only the highest value for DEMAND is sent. For example: taskman -Lp P1 -R \"rusage[f1=1:f2=2||f1=3:f2=1]\" myjob The f1 resource requirement is 1 for the first rusage string sibling and 3 for the second rusage string sibling. If the f1 resource is not satisfied, the demand of f1 is 3, not 3+1. This task will not start until at least one of the requirements of the rusage string siblings is met. If LOCAL_TO is specified for a feature, taskman jobs must specify feature names with locality information similar to submission with bsub. You must know which token can be used from the location where task is going to run. For example: taskman -Lp P1 -R \"rusage[hspice@siteB=1]\" myjob taskman -Lp P1 -R \"rusage[hspice=1]\" myjob taskman -Lp P1 -R \"rusage[hspice@clusterA=1]\" myjob © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Project mode with project groups.html":{"url":"chapter10/section4/Project mode with project groups.html","title":"项目组的项目模式","keywords":"","body":"Project mode with project groups Project groups use a ProjectGroup section to build a hierarchical project structure, which you can use to set limits on projects that span multiple clusters. Depending on your license usage, you can configure different project groups for different license features, or reuse the same hierarchical structure. Each license feature in project mode can either use projects or project groups. Changing from projects to project groups involves adding a ProjectGroup section and changing the license token distribution that is configured in the Feature section. Other configuration remains the same. Parent topic: Configuring License Scheduler Configuring project groups About this task ProjectGroup sections use configured projects (each with a Projects section in the lsf.licensescheduler file) to form a hierarchical structure for each feature. Note The Feature section GROUP parameter is used to group projects together, simplifying configuration, and is not the same as a ProjectGroup section. Procedure Add a ProjectGroup section to the lsf.licensescheduler file: Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED End Projectgroup If LS_FEATURE_PERCENTAGE=Y or LS_ACTIVE_PERCENTAGE=Y in lsf.licensescheduler, values for OWNERSHIP, LIMITS, and NON_SHARED are expressed as a percentage of the total licenses, not as an absolute number. For each branch in the hierarchy, add a line to the ProjectGroup section. Under the heading GROUP, indicate the project that branches, and direct descendants in the hierarchy (group(member ...)). Under the heading SHARES, set the integer share for each member project. Under the heading OWNERSHIP, set the integer ownership for each bottom-level group member (leaf node), with a dash (-) representing no ownership. The OWNERSHIP value must be greater than or equal to the NON_SHARED value. Under the heading LIMITS set the integer license limit for each member project, with a dash (-) representing unlimited. The LIMITS value must be greater than or equal to the OWNERSHIP value. Under the heading NON_SHARED, set the integer number of non-shared licenses each bottom-level group member (leaf node) uses exclusively, with ’-’ representing none. Optionally, under the heading DESCRIPTION, add a description up to 64 characters long, using a backslash () to extend to multiple lines. For example, the branch g4 splits into three members: GROUP SHARES OWNERSHIP LIMITS NON_SHARED (g4 (p4 p5 p6)) (1 1 1) (1 1 1) () (- 3 -) In the Feature section, set parameter GROUP_DISTRIBUTION to the top level of the ProjectGroup section hierarchy. The DISTRIBUTION parameter that is used for projects is no longer used. In the Feature section, list service domains in the SERVICE_DOMAINS parameter. Unlike for projects, service domains are not included in the distribution for project groups. Project group examples This hierarchy is implemented by the project group configuration: Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED (topgrp (g1 g2)) (1 1) () (10 10) () (g1 (g3 g4)) (1 1) () (10 10) () (g2 (g5 g6)) (1 1) () (- 5) () (g3 (p1 p2 p3)) (1 1 2) () (3 4 5) () (g4 (p4 p5 p6)) (1 1 1) (1 3 1) () (0 3 0) (g5 (p7 p8 p9)) (1 1 1) (2 0 2) () (1 0 1) (g6 (p10 p11 p12)) (1 1 1) (2 2 2) (4 4 4) (1 0 1) End ProjectGroup License feature configuration that uses this project group: Begin Feature NAME = AppZ GROUP_DISTRIBUTION = topgrp SERVICE_DOMAINS = LanServer WanServer End Feature Use the LIMITS column to limit token use, so tokens are sometimes not distributed even if they are available. By default, License Scheduler distributes all available tokens if possible. For example, if total of six licenses are available: Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED (Root(A B)) (1 1) () () () (A (c d)) (1 1) () (1 1) () (B (e f)) (1 1) () () () End ProjectGroup When there is no demand for license tokens, License Scheduler allocates only five tokens according to the distribution. License Scheduler gives three tokens to group A and three tokens to group B, but project c and project d are limited to one token each, so one token is not allocated within group A. As more demand comes in for project e and project f, the tokens that are not allocated are distributed to group B. Configuring preemption priority within project groups About this task The optional PRIORITY parameter in the ProjectGroup section, if defined, is used for preemption instead of basing preemption on the accumulated inuse for each project. Procedure Under the heading PRIORITY, set the integer priority for each group member, with ’0’ being the lowest priority. PRIORITY can be set for all members in the project group hierarchy. Example For example: Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED PRIORITY (root(A B C)) (1 1 1) () () () (3 2 -) (A (P1 D)) (1 1) () () () (3 5) (B (P4 P5)) (1 1) () () () () (C (P6 P7 P8)) (1 1 1) () () () (8 3 -) (D (P2 P3)) (1 1) () () () (2 1) End ProjectGroup By default, priority is evaluated from top to bottom. The priority of any specific node is first decided by the priorities of its parent nodes. The values are only comparable between siblings. The following figure illustrates the example configuration: The priority of each node is shown beside the node name. If priority is not defined, by default is set to 0 (nodes P4 and P5 under node B). To find the highest priority leaf node in the tree, License Scheduler traverses the tree from root to node A to node D to project P2. To find the lowest priority leaf node in the tree, License Scheduler traverses the tree from root to node C to project P8. When two nodes have the same priority, for example, projects P4 and P5, priority is determined by accumulated inuse usage at the time the priorities are evaluated. When a leaf node in branch A wants to preempt a token from branch B or C, branch C is picked because it has a lower priority than branch B. Viewing hierarchical configuration Procedure Use blinfo -G to view the hierarchical configuration: For the previous example: blinfo -G GROUP SHARES OWNERSHIP LIMITS NON_SHARED DESCRIPTION (topgrp (g1 g2)) (1 1) () (10 10) () () (g1 (g3 g4)) (1 1) () (10 10) () () (g2 (g5 g6)) (1 1) () (- 5) () () (g3 (p1 p2 p3)) (1 1 2) () (3 4 5) () () (g4 (p4 p5 p6)) (1 1 1) (1 3 1) () (0 3 0) () (g5 (p7 p8 p9)) (1 1 1) (2 0 2) () (1 0 1) () (g6 (p10 p11 p12)) (1 1 1) (2 2 2) (4 4 4) (1 0 1) () Viewing information about project groups Procedure Use blstat -G to view the hierarchical dynamic license information. blstat -G FEATURE: p1_f1 SERVICE_DOMAINS: TOTAL_INUSE: 0 TOTAL_RESERVE: 0 TOTAL_FREE: 4 OTHERS: 0 SHARE_INFO_FOR: /topgrp GROUP/PROJECT SHARE OWN INUSE RESERVE FREE DEMAND g2 100.0 % 0 0 0 4 0 SHARE_INFO_FOR: /topgrp/g2 GROUP/PROJECT SHARE OWN INUSE RESERVE FREE DEMAND p3 50.0 % 0 0 0 2 0 p4 50.0 % 0 0 0 2 0 FEATURE: p1_f2 SERVICE_DOMAINS: TOTAL_INUSE: 0 TOTAL_RESERVE: 0 TOTAL_FREE: 4 OTHERS: 0 SHARE_INFO_FOR: /topgrp GROUP/PROJECT SHARE OWN INUSE RESERVE FREE DEMAND g2 100.0 % 0 0 0 4 0 SHARE_INFO_FOR: /topgrp/g2 GROUP/PROJECT SHARE OWN INUSE RESERVE FREE DEMAND p3 50.0 % 0 0 0 2 0 p4 50.0 % 0 0 0 2 0 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection1/Configure fast dispatch project mode.html":{"url":"chapter10/section4/subsection1/Configure fast dispatch project mode.html","title":"配置快速调度项目模式","keywords":"","body":"Configure fast dispatch project mode Use fast dispatch project mode to increase license utilization for project licenses. Fast dispatch project mode has the scheduling performance of cluster mode with the functionality of project mode, and is most appropriate for your needs if: Your primary goals are to maximize license use and ensure ownership of groups Most jobs are short relative to the blcollect cycle (60 seconds by default, set by LM_STAT_INTERVAL). In fast dispatch project mode, LSF License Scheduler does not have to run the FlexNet command lmstat (or lmutil) or the Reprise License Manager command rlmutil (or rlmstat) to verify that a license is free before each job dispatch. As soon as a job finishes, the cluster can reuse its licenses for another job of the same project, which keeps gaps between jobs small. However, because LSF License Scheduler does not run lmutil, lmstat, rlmutil, or rlmstat to verify that the license is free, there is an increased chance of a license checkout failure for jobs if the license is already in use by a job in another project. Hierarchical project group paths By default, hierarchical project groups in fast dispatch project mode are the same as hierarchical project groups in project mode. Fast dispatch project mode also supports the use of hierarchical project group paths, which helps LSF License Scheduler dispatch more jobs in fast dispatch project mode. To use hierarchical project group paths, you need LSF, Version 9.1.1, or later. The following hierarchical group structure illustrates hierarchical project group paths: Enabling hierarchical project group paths enables the following: Features can use hierarchical project groups with project and project group names that are not unique, as long as the projects or project groups do not have the same parent. That is, you can define projects and project groups in more than one hierarchical project group. For example, the p4 project can be defined for project groups g4 and g6, each with its specific resource allocation within the project groups. NoteDo not define a project group as a child of itself, because this results in a loop. For example, if project group g3 is a child of project group g1, do not define project g1 as a child of g3, as this will result in a loop of g1 and g3 being child project groups of one another. When specifying -Lp license_project , you can use paths to describe the project hierarchy without specifying the root group. For example, if you have topgrp as your root group, which has a child project group named g1 with a child project group named g3, which has a project named p1, you can use -Lp /g1/g3/p1 to specify this project. Hierarchical project groups have a default project named others with a default share value of 0. Any projects that do not match the defined projects in a project group are assigned into the others project. If the others project has a share value of 0, this project can still use licenses if the defined projects with shares are not using the licenses. Therefore, by default, the others project has the lowest priority within a project group. For example, if you have topgrp as your root group, which has a child project group named g1 with a child project group named g3, which has a project named p1, if you specify -Lp /g1/g3/project3 (which does not match a project), the effective license project is /g1/g3/others project. Similarly, specifying -Lp /g1/g3/gA/gB/gC/project3 results in an effective license project of /g1/g3/others because there are no subsequent child project groups under /g1/g3. If there is already a project named others, the preexisting others project specification overrides the default project. Defining hierarchical project groups for fast dispatch project mode is the same as for project mode, allowing for project and project group names that are not unique. For example, to define the previously-illustrated hierarchical project group in lsf.licensescheduler: Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED (topgrp (g1 g2) (1 1) () () () (g1 (g3 g4 others) (1 1 1) () () () (g2 (g5 g6) (1 2) () () () (g3 (p1 p2 others) (2 2 1) () () () (g4 (p3 p4) (2 1) () () () (g5 (p5 p1) (1 3) () () () (g6 (p6 p4) (1 1) () () () End ProjectGroup Begin Feature NAME=f1 GROUP_DISTRIBUTION=topgrp SERVICE_DOMAINS=LanServer End Feature The others projects are explicitly defined for g1 and g3 (with a specific share), while the other project groups use the default others projects with 0 share. The p1 project is defined for both g3 and g5, with a larger share if specified through the g5 project group. The p4 project is defined for both g4 and g6, with a larger share if specified through the g6 project group. You can also specify different project groups with different root groups. Different features can use different root groups (as defined by the GROUP_DISTRIBUTION parameter), each with its own project group hierarchy and share policies. When a job requests multiple features in fast dispatch project mode, LSF License Scheduler generates an effective license project for each feature. This means that it is possible for one job to have multiple effective license projects if the features use different project group hierarchies. LSF License Scheduler and LSF will calculate the effective license project for the feature based on its related project group hierarchy. The effective project is the path of the project resulting from the -Lp specification. When specifying a project name without a hierarchical project group path in fast dispatch project mode with hierarchical group paths enabled, LSF License Scheduler uses the shortest path to the left that ends with the name of the project, as long as the cluster that submitted the job is authorized to use the selected project. If LSF License Scheduler cannot find such a project in the hierarchy, LSF License Scheduler uses the /others project. For example, the following hierarchical group structure illustrates which clusters (c1 and c2) are authorized to use each project: If you specify -Lp p2 from the c2 cluster (by submitting bsub -Lp p2 -R \"rusage[f1=1]\" myjob) without specifying a hierarchical group path, c2 is authorized to use /g2/p2 and /g2/g3/p2. The shortest path to the left that leads to p2 is /g2/p2, so the job is associated with the /g2/p2 hierarchical project. Configure lmremove or rlmremove preemption Enable and configure lmremove (for FlexNet) or rlmremove (for Reprise License Manager) as a preemption action. Parent topic: Configuring License Scheduler Configure parameters Before you begin Before configuring fast dispatch project mode, ensure that you enabled and configured project mode using projects or project groups. However, you can only specify one service domain per feature in fast dispatch project mode. Procedure Fast dispatch project mode can be set globally, or for individual license features. Set individually when using fast dispatch project mode for some features and cluster mode or project mode for other features. If you are using fast dispatch project mode for all license features, define FAST_DISPATCH=Y in the Parameters section of lsf.licensescheduler. If you are using fast dispatch project mode for some license features, define FAST_DISPATCH=Y for individual license features in the Feature section of lsf.licensescheduler. The Feature section setting of FAST_DISPATCH overrides the global Parameter section setting. Set the limit to which LSF License Scheduler considers the demand by each project in each cluster when allocating licenses. The default is 5. DEMAND_LIMIT=integer Define DEMAND_LIMIT in the Parameters section of lsf.licensescheduler to set the limit for all license features, or define DEMAND_LIMIT in the Feature section for individual license features. Setting in the Feature section overrides the global setting in the Parameters section. Periodically, each cluster sends a demand for each project. This is calculated in a cluster for a project by summing up the rusage of all jobs of the project pending due to lack of licenses. Whether to count a job's rusage in the demand depends on the job's pending reason. In general, the demand reported by a cluster only represents a potential demand from the project. It does not take into account other resources that are required to start a job. For example, a demand for 100 licenses is reported for a project. However, if LSF License Scheduler allocates 100 licenses to the project, the project does not necessarily use all 100 licenses due to slot available, limits, or other scheduling constraints. mbatchd in each cluster sends a demand for licenses from each project. In fast dispatch project mode, DEMAND_LIMIT limits the amount of demand from each project in each cluster that is considered when scheduling. To enable hierarchical project group paths, define PROJECT_GROUP_PATH=Y in the Parameters section of lsf.licensescheduler. NoteTo use PROJECT_GROUP_PATH, you need LSF, Version 9.1.1, or later. Restart to implement configuration changes Procedure Run bladmin reconfig to restart the bld. If you deleted any Feature sections, restart mbatchd. In this case, a message is written to the log file, prompting the restart. If required, run badmin mbdrestart to restart each LSF cluster. View license allocation Procedure Run blstat -c token_name to view information for a specific license token (as configured in a Feature section). blstat -c output differs for fast dispatch project mode, project mode, and cluster mode. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection1/Configure lmremove or rlmremove preemption.html":{"url":"chapter10/section4/subsection1/Configure lmremove or rlmremove preemption.html","title":"配置 lmremove 或 rlmremove 抢占","keywords":"","body":"Configure lmremove or rlmremove preemption Enable and configure lmremove (for FlexNet) or rlmremove (for Reprise License Manager) as a preemption action. About this task Preemption is enabled by configuring license ownership for a project. When a project has ownership of licenses that are occupied by another project, these licenses can be preempted by the project that has ownership when it needs to use the licenses. Begin Feature NAME = lic1 FAST_DISPATCH= Y DISTRIBUTION = serviceDomain1(projectA 1/10 projectB 1 ) End Feature The default preemption action is to send a TSTP signal to the job. Some applications will respond well to this action, and will free up their licenses and suspend their processes. If your applications respond well to the TSTP signal, leave this default as the preemption action. For applications that do not respond well to the TSTP signal, an alternative preemption action for projects in fast dispatch project mode is to suspend the job’s processes, then use lmremove or rlmremove to remove licenses from the application. lmremove or rlmremove causes the license manager daemon and vendor daemons to close the TCP connection with the application. Once the application is resumed, it will try to reacquire the licenses. In general, lmremove or rlmremove will fail to remove licenses from an application for a period of time after the licenses are checked out. This period depends on the application itself. When LSF License Scheduler calls lmremove or rlmremove, it may remove licenses from running jobs when the running jobs share the same user and host as a suspended job. LSF License Scheduler will continue to reserve the licenses (from the rusage) for the running job. Therefore, when the running job tries to reacquire its licenses, there will be licenses available for it. LSF License Scheduler calls lmremove or rlmremove in the same directory as it calls the lmstat or rlmstat command (as defined in the LMSTAT_PATH or RLMSTAT_PATH parameter). Procedure Enable lmremove or rlmremove as a preemption action by specifying the LM_REMOVE_SUSP_JOBS parameter in the Parameters or Feature section of lsf.licensescheduler. LM_REMOVE_SUSP_JOBS = seconds Set this parameter for a license feature in its corresponding Feature section as long as it is using the fast dispatch project mode. If you set this parameter in the Parameters section, this applies to all license features using the fast dispatch project mode. When this parameter is configured for a license feature, LSF License Scheduler will periodically use lmremove or rlmremove to try to remove the license feature from each recently-suspended job. For a given application, set LM_REMOVE_SUSP_JOBS to a value greater than the period following a license checkout that lmremove or rlmremove will fail for that application. In this way, you can be sure that when a job is suspended, its licenses will be released. The length of this period depends on the application. LSF License Scheduler will continue to try removing the license feature for the specified number of seconds after the job is first suspended. For example, if you define LM_REMOVE_SUSP_JOBS = 10, when a job is suspended due to preemption,LSF License Scheduler will continue to try removing the license feature for up to ten seconds after the job is first suspended. Enable LSF License Scheduler to preempt a job immediately after a license checkout by defining LM_REMOVE_INTERVAL = 0 in the Parameters section of lsf.licensescheduler. LM_REMOVE_INTERVAL = 0 Defining this parameter to a larger value prevents LSF License Scheduler from preempting a job for a period of time after LSF License Scheduler first detects a license checkout by the job (the default value is 180 seconds). Defining LM_REMOVE_INTERVAL = 0 ensures that LSF License Scheduler can preempt a job immediately after checkout. After the job is suspended, LSF License Scheduler calls lmremove or rlmremove to release licenses from the job. To limit the amount of time between subsequent forks of child processes to run lmremove or rlmremove, define the LM_REMOVE_SUSP_JOBS_INTERVAL parameter in the Parameters or section of lsf.licensescheduler. LM_REMOVE_SUSP_JOBS_INTERVAL = seconds By default, LSF License Scheduler forks a child process to run lmremove or rlmremove every time it receives an update from a license collector daemon (blcollect). Defining this parameter controls the minimum amount of time between subsequent forks. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/Automatic time-based configuration.html":{"url":"chapter10/section4/Automatic time-based configuration.html","title":"基于时间的自动配置","keywords":"","body":"Automatic time-based configuration Variable time-based configuration is used in both project mode and cluster mode to automatically change configuration that is set in lsf.licensescheduler based on time windows. For example, if you have design centers in remote locations, one use of time-based configuration is to switch ownership of license tokens that are based on local time of day. You define automatic configuration changes in lsf.licensescheduler by using if-else constructs and time expressions. After you change the files, reconfigure the cluster with the bladmin reconfig command. The expressions are evaluated by License Scheduler every 10 minutes based on bld start time. When an expression evaluates true, License Scheduler dynamically changes the configuration that is based on the associated configuration statements and restarts bld The #if, #else, #endif keywords are not interpreted as comments by License Scheduler, but as if-else constructs. Syntax time = hour | hour:minute | day:hour:minute hour Specify an integer from 0 to 23, representing the hour of the day. minute integer from 0 to 59, representing the minute of the hour.If you do not specify the minute, License Scheduler assumes the first minute of the hour (:00). day Specify an integer from 0 to 7, representing the day of the week, where 0 represents every day, 1 represents Monday, and 7 represents Sunday.If you do not specify the day, License Scheduler assumes every day. If you do specify the day, you must also specify the minute. Specify time values Procedure Specify at least the hour. Day and minutes are optional. Specify time windows Procedure Specify two time values that are separated by a hyphen (-), with no space in between. time_window = time1-time2 time1 is the start of the window and time2 is the end of the window. Both time values must use the same syntax. Use one of the following ways to specify a time window: hour-hour hour:minute-hour:minute day:hour:minute-day:hour:minute For example: Daily window To specify a daily window, omit the day field from the time window. Use either the hour-hour or hour:minute-hour:minute format. For example, to specify a daily 8:30 a.m. to 6:30 p.m. window: 8:30-18:30 Overnight window To specify an overnight window, make time1 greater than time2. For example, to specify 6:30 p.m. to 8:30 a.m. the following day: 18:30-8:30 Weekend window To specify a weekend window, use the day field. For example, to specify Friday at 6:30 p.m to Monday at 8:30 a.m.: 5:18:30-1:8:30 Specify time expressions About this task Time expressions use time windows to specify when to change configurations. Procedure Define a time expression. A time expression is made up of the time keyword followed by one or more space-separated time windows that are enclosed in parentheses. Use the &&, ||, and ! logical operators to combine time expressions. expression = time(time_window[ time_window ...]) | expression && expression | expression || expression | !expression For example: Both of the following expressions specify weekends (Friday evening at 6:30 p.m. until Monday morning at 8:30 a.m.) and nights (8:00 p.m. to 8:30 a.m. daily). time(5:18:30-1:8:30 20:00-8:30) time(5:18:30-1:8:30) || time(20:00-8:30) Create if-else constructs About this task The if-else construct can express single decisions and multi-way decisions by including elif statements in the construct. Procedure Define an if-else expression. #if time(expression) statement #else statement #endif The #endif part is mandatory and the #else part is optional. Define an elif expression. The #elif expressions are evaluated in order. If any expression is true, the associated statement is used, and this terminates the whole chain. The #else part handles the default case where no other conditions are satisfied. #if time(expression) statement #elif time(expression) statement #elif time(expression) statement #else statement #endif When you use #elif, the #else and #endif parts are required. Restart to implement configuration changes About this task All time-based configuration is within the lsf.licensescheduler file, so restarting the bld applies all changes. Procedure Run bladmin ckconfig to check configuration. Run lsadmin limrestart or bladmin restart to restart the bld. Verify configuration About this task Verify time-based configuration by viewing License Scheduler information. Procedure Run blinfo. Run blstat. Examples Project configuration in project mode Begin Feature NAME = f1 #if time(5:16:30-1:8:30 20:00-8:30) DISTRIBUTION=Lan(P1 2/5 P2 1) #elif time(3:8:30-3:18:30) DISTRIBUTION=Lan(P3 1) #else DISTRIBUTION=Lan(P1 1 P2 2/5) #endif End Feature Project group configuration in project mode # # ProjectGroup section # Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED (group1 (A B)) (1 1) (5 -) () () End ProjectGroup Begin ProjectGroup GROUP SHARES OWNERSHIP LIMITS NON_SHARED (group2 (A B)) (1 1) (- 5) () () End ProjectGroup # # Feature section # Begin Feature NAME = f1 #if time(5:16:30-1:8:30 20:00-8:30) GROUP_DISTRIBUTION=group1 #elif time(3:8:30-3:18:30) GROUP_DISTRIBUTION=group2 #else GROUP_DISTRIBUTION=group2 #endif SERVICE_DOMAINS=Lan1 Lan2 End Feature Cluster distribution configuration in cluster mode Begin Feature NAME = f1 #if time(5:16:30-1:8:30 20:00-8:30) CLUSTER_DISTRIBUTION=Wan(Cl1 1 Cl2 1) #elif time(3:8:30-3:18:30) CLUSTER_DISTRIBUTION= Wan(Cl1 2 Cl2 1/2/100) Lan(Cl2 1) #else CLUSTER_DISTRIBUTION= Wan(Cl1 10 Cl2 1/1/10) Lan(Cl1 1) #endif End Feature © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/Failover.html":{"url":"chapter10/section4/subsection2/Failover.html","title":"故障转移","keywords":"","body":"故障转移 许可证最大化 License Scheduler 的内置功能有助于确保始终有效地使用您的许可证。例如，如果 sbatchd 遇到任何问题，则作业将获得状态 UNKNOWN。 但是，许可证调度程序可确保继续分配所有使用中的许可证，但将其收取 “OTHERS” 类别的费用，直到恢复 sbatchd 并再次知道作业状态为止。 故障转移主机 主候选主机运行许可证调度程序守护程序（bld），并且如果主许可证调度程序主机发生故障，或失去与网络的连接（在 LAN 或 WAN 环境中），则可以接管许可证管理。 故障转移配置 在主机发生故障或网络故障时，配置故障转移主机列表。 可以将 License Scheduler 配置为在 LAN 和 WAN 中进行故障转移配置。 局域网的故障转移配置 WAN 的故障转移配置 设置 fod © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/Failover provisioning for LANs.html":{"url":"chapter10/section4/subsection2/Failover provisioning for LANs.html","title":"局域网的故障转移配置","keywords":"","body":"Failover provisioning for LANs About this task Configuring failover ensures enhanced performance and reliable license distribution. You only need one host to run License Scheduler, but you can configure your site for a failover mechanism with multiple candidate hosts to take over the scheduling if there is a failure. This configuration can be used in a local network or across multiple sites in a wider network. Procedure Define the list of License Scheduler hosts in LSF_CONFDIR/lsf.conf and lsf.licensescheduler for your LAN (Designer Center A in this example). lsf.conf: Specify a space-separated list of hosts for the LSF_LIC_SCHED_HOSTS parameter: LSF_LIC_SCHED_HOSTS=\"hostA.designcenter_a.com hostB.designcenter_a.com hostC.designcenter_a.com\" TipList the hosts in order of preference for running License Scheduler, from most preferred to least preferred. lsf.licensescheduler: Specify a space-separated list of hosts for the HOSTS parameter: HOSTS=hostA.designcenter_a.com hostB.designcenter_a.com hostC.designcenter_a.com List the hosts in the same order as lsf.conf. The LIM starts the bld (License Scheduler daemon) on each host in the LSF_LIC_SCHED_HOSTS list. Every host in defined in LSF_LIC_SCHED_HOSTS is a failover candidate and runs the bld daemon. hostA.designcenter_a.com is the License Scheduler host, and the remaining hosts are candidate hosts that are running the bld daemon, ready to take over the management of the licenses if there is a network failure Each host contains the list of candidate hosts in memory Each candidate License Scheduler host communicates with the License Scheduler host, License Scheduler (hostA) If the License Scheduler host fails, each candidate host checks to see if a more eligible host is running the License Scheduler daemon. If not, it becomes the failover host and inherits the communication links that existed between the original License Scheduler host and each candidate host. In this example, if License Scheduler on hostA fails, candidate License Scheduler hostB is the next most eligible host, and takes over the license scheduling. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/Failover provisioning for WANs.html":{"url":"chapter10/section4/subsection2/Failover provisioning for WANs.html","title":"外网的故障转移配置","keywords":"","body":"Failover provisioning for WANs Similar to LANs, you can configure your site for a failover mechanism across multiple sites in a wide network. You need only one host to run License Scheduler, but you can configure your site for a failover mechanism with multiple candidate hosts to take over the scheduling in a failure. License scheduling across sites can be streamlined because License Scheduler supports service provisioning during breaks in wide area network connections. This support means that you can run License Scheduler from one host that controls license scheduling across multiple sites. Configure and start License Scheduler in a WAN WAN example Service provisioning at the host and network levels © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/Configure and start License Scheduler in a WAN.html":{"url":"chapter10/section4/subsection2/Configure and start License Scheduler in a WAN.html","title":"在 WAN 中配置并启动 License Scheduler","keywords":"","body":"Configure and start License Scheduler in a WAN About this task In a WAN configuration: Procedure As the root user, install License Scheduler on each cluster in the WAN configuration and select one cluster to be the main cluster. In the cluster that contains the WAN license server, log on as the primary License Scheduler administrator. Edit the following items in LSF_CONFDIR/lsf.licensescheduler: Specify a space-separated list of hosts for the HOSTS parameter: HOSTS=hostname_1 hostname_2 ... hostname_n Where: hostname_1 is the most preferred host for running License Scheduler. hostname_n is the least preferred host for running License Scheduler. In the Clusters section, specify the names of the clusters in the WAN. For example: Begin Clusters CLUSTERS design_SJ design_BOS End Clusters In the cluster that contains the WAN license server, as the LSF primary administrator, edit LSF_CONFDIR/lsf.conf. Lines that begin with # are comments: Specify a space-separated list of hosts for the LSF_LIC_SCHED_HOSTS parameter: LSF_LIC_SCHED_HOSTS=\"hostname_1 hostname_2 ... hostname_n\" Where: hostname_1, hostname_2, ..., hostname_n are hosts on which the LSF LIM daemon starts the License Scheduler daemon (bld). The first host that is listed in the HOSTS list is the default master License Scheduler host for the WAN. The order of the host names in LSF_LIC_SCHED_HOSTS is ignored. In the other clusters in the WAN: Configure the LSF_LIC_SCHED_HOSTS parameter in lsf.conf with a local list of candidate hosts. Configure the HOSTS parameter in the Parameters section lsf.licensescheduler with the following list of hosts: Start the list with the same list of candidate hosts as the HOSTS parameter in the cluster that contains the WAN license server. Continue the list with the local cluster’s list of hosts from the LSF_LIC_SCHED_HOSTS parameter in lsf.conf. In the cluster that contains the WAN license server and the other clusters in the WAN, run the following commands: Run bld -C to test for configuration errors. Run bladmin reconfig to configure License Scheduler. Run lsadmin reconfig to reconfigure LIM. Use ps -ef to make sure that bld is running on the candidate hosts. Run badmin reconfig to reconfigure mbatchd. TipAlthough the bld daemon is started by LIM, bld runs under the account of the primary License Scheduler administrator. If you did not configure the LIM to automatically start the bld daemon on your License Scheduler hosts, run $LSF_BINDIR/blstartup on each host to start the bld daemon. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/WAN example.html":{"url":"chapter10/section4/subsection2/WAN example.html","title":"WAN 示例","keywords":"","body":"WAN example A design center contains the following hosts configuration in a WAN: LIM starts bld on the following hosts: lsf.conf in Design Center A LSF_LIC_SCHED_HOSTS=\"hostA1.designcenter_a.com hostA2.designcenter_a.com hostA3.designcenter_a.com\" lsf.conf in Design Center B LSF_LIC_SCHED_HOSTS=\"hostB1.designcenter_b.com hostB2.designcenter_b.com hostB3.designcenter_b.com\" License Scheduler candidate hosts are listed in the following order of preference: lsf.licensescheduler in Design Center A HOSTS=hostB1.designcenter_b.com hostB2.designcenter_b.com hostA1.designcenter_a.com hostA2.designcenter_a.com hostA3.designcenter_a.com lsf.licensescheduler in Design Center B HOSTS=hostB1.designcenter_b.com hostB2.designcenter_b.com hostB3.designcenter_b.com The following diagram shows hostB1.designcenter_b.com, the License Scheduler host for the WAN containing Design Center A and Design Center B. How it works The LSF LIM daemon starts the License Scheduler daemon (bld) on each host that is listed in LSF_LIC_SCHED_HOSTS in Design Center A and Design Center B. Each host in the HOSTS list in Design Center A is a potential License Scheduler candidate in Design Center A and is running the bld daemon, but only one host becomes the License Scheduler host: the first host in the HOSTS list that is up and that is running the bld daemon. Similarly, the License Scheduler host in Design Center B is the first host in the HOSTS list that is up and that is running the bld daemon. License Scheduler manages the licenses in Design Center A and Design Center B as follows: Both design centers list hostB1.designcenter_b.com at the top of their HOSTS lists. hostB1.designcenter_b.com is the License Scheduler host for Design Center A and for Design Center B. The rest of the hosts in both design centers remain on standby as candidate License Scheduler hosts. License Scheduler manages the license scheduling across the WAN connection. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/Service provisioning at the host and network levels.html":{"url":"chapter10/section4/subsection2/Service provisioning at the host and network levels.html","title":"主机和网络级别的服务供应","keywords":"","body":"Service provisioning at the host and network levels In the following example configuration, there are two potential points of failure: host and network. Host failure If hostB1.designcenter_b.com fails, and bld stops running, a candidate License Scheduler host must take over the license management. The next host on the HOSTS list in both Design Center A and Design Center B is hostB2designcenter_b.com. License Scheduler fails over to this host if it is up and running. Network failure If the network connection between Design Center A and Design Center B breaks, Design Center A can no longer communicate with the hosts in Design Center B, so hostB1.designcenter_b.com and hostB2.designcenter_b.com are no longer candidate license scheduling hosts for Design Center A. The next candidate host for Design Center A is hostA1.designcenter_a.com. License management then runs locally in Design Center A on hostA1.designcenter_a.com. In Design Center B, hostB1.designcenter_b.com continues to run License Scheduler, but only manages the local network if the wide area network connection is down. The local License Scheduler host, hostA1.designcenter_a.com, checks for a heartbeat from hostB1.designcenter_b.com at regular intervals, then returns license management back to hostB1.designcenter_b.com when the network connection returns. Parent topic: Failover provisioning for WANs © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/subsection2/Set up fod.html":{"url":"chapter10/section4/subsection2/Set up fod.html","title":"设置 fod","keywords":"","body":"Set up fod About this task The fod daemon manages failover for the blcollect daemons. fod can restart any failed blcollect processes if the local host (and thus the local fod) is down. The failover host fod starts new blcollect daemons until the primary host comes back online and the primary fod contacts the secondary fod. The fod files are in the License Scheduler package, but must be copied, configured, and started manually. Procedure Install the failover daemon (fod) files on each host. Create a directory to hold the fod files, with subdirectories bin, conf, etc, and man. For example: /usr/local/fod Copy all user command files and the fod.shell file to .../bin. Copy the fod.conf file to .../conf. Copy the fod file to .../etc. Copy the fodapps.1, fodhosts.1 and fodid.1 files to .../man/man1. Copy the fod.conf.5 file to .../man/man5. Copy the fodadmin.8 file to .../man/man8. Edit the fod.shell file, and set the FOD_ROOT parameter to the name of your new directory. For example: FOD_ROOT=/usr/local/fod Set the environment variables. Set the PATH environment variable to include the /bin directory. Set the FOD_ENVDIR environment variable to $FOD_ROOT/conf. Set the MANPATH environment variable to include the /man directory. In fod.conf, set the required parameters. FOD_ADMIN: The License Scheduler administrator FOD_PORT: The TCP listening port and UDP port for the failover daemon FOD_WORK_DIR: The working directory FOD_LOG_DIR: The log directory For example: FOD_CLUSTERNAME = fod FOD_ADMIN = lsadmin FOD_PORT = 9583 FOD_WORK_DIR = /usr/local/fod/work FOD_LOG_DIR = /usr/local/fod/work In the Hosts section of fod.conf, specify the hosts where the failover daemons run. If your hosts run in different DNS domains, you must use a fully qualified domain name when you specify the host name. The first host in the Hosts section is the first host on which the failover daemon runs (the master failover daemon host). Begin Hosts HOSTNAME fodhost1.domain_name fodhost2 End Host Modify the Applications section of fod.conf. Begin Applications NAME PATH PARAMS FATAL_EXIT_VALUE blcollect /pcc/apps/lsf6/6.0/sparc-sol7-64/etc (-2 -m \"sasun3 augustus claudius\" -p 9581 -c lan -i 20 -D /sparc-sol7-64/etc) (-) End Applications Start fod on each host. Log on as the IBM Spectrum LSF License Scheduler administrator. Source the LSF environment. For csh or tsh run source LSF_TOP/conf/cshrc.lsf For sh, ksh, or bash, run . LSF_TOP/conf/profile.lsf Launch the failover daemons by running the fod.shell file. Check the progress of a successful launch by running ps -ef. View the fod log under $LSF_LOGDIR. Check configuration from $FOD_ROOT/etc by running fod -C. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section4/User authentication.html":{"url":"chapter10/section4/User authentication.html","title":"用户认证","keywords":"","body":"用户认证 当用户声称某作业属于某个项目时， LSF License Scheduler 会检查该用户是否属于该项目，因为项目会分配公平份额优先级，而抢占则基于所有权。 当用户向不属于他们的许可项目提交作业时，请求将被拒绝，或者将作业放入共享数量很少，或根本没有共享的默认存储桶中。 管理员可以控制谁可以运行什么项目。 默认情况下，未启用这种身份验证以与 LSF License Scheduler 的早期版本兼容。 启用后，用户身份验证具有以下行为： 如果用户属于项目，则 LSF License Scheduler 允许许可请求。 如果用户不属于该项目，或者该项目与配置中的任何项目都不匹配，则 LSF License Scheduler 拒绝该请求。 如果在 LSF License Scheduler 用户验证配置文件 ls.users 中配置了默认项目，则 LSF License Scheduler 会将项目更改为 default，并允许许可证请求。 如果项目是默认项目，则无需身份验证，并且 LSF License Scheduler 允许该请求。 启用用户身份验证 步骤 要为 LSF 作业启用用户身份验证，请将 LSF 配置为使用身份验证 esub（esub.ls_auth）。 在 lsf.conf 中定义 LSB_ESUB_METHOD=lsauth。 要为 taskman 作业启用用户身份验证，请在 lsf.licensescheduler 中定义 AUTH = Y。 在 LSF_CONFDIR/ls.users 文件中配置用户及其相关项目。 该文件使用以下格式每行定义一个项目： project_name:::[user_name][,user_name2 ...] 例如： Project1:::user1,user2 default::: 提示 确保 ls.users 中的项目（包括默认项目）符合 lsf.licensescheduler 配置。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/Viewing information and troubleshooting.html":{"url":"chapter10/section5/Viewing information and troubleshooting.html","title":"查看信息和故障排除","keywords":"","body":"查看信息和故障排除 关于查看可用许可证 关于错误日志 故障排除 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection1/About viewing available licenses.html":{"url":"chapter10/section5/subsection1/About viewing available licenses.html","title":"关于查看可用许可证","keywords":"","body":"关于查看可用许可证 许可证服务器从物理服务器收集许可证功能信息，并将此数据合并到服务域中。 合并数据后，将保留各个许可证服务器信息，您可以将其与物理服务器信息一起查看。 您的项目从 FlexNet 中签出了使用中的许可证。 免费许可证和项目保留的许可证尚未从 FlexNet 中检出。 许可证总数可能会随着许可证过期或添加而改变。 当非 LSF 用户签出许可证时，blstat 中的 OTHERS 计数增加，而 TOTAL_FREE 计数减少。 每当 LSF 在竞争项目之间重新分配许可证令牌时，每个项目的许可证数量都会更改。 查看传递给作业的许可证服务器和许可证功能信息 定制动态许可证信息输出 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection1/View license server and license feature information passed to jobs.html":{"url":"chapter10/section5/subsection1/View license server and license feature information passed to jobs.html","title":"查看传递给作业的许可证服务器和许可证功能信息","keywords":"","body":"查看传递给作业的许可证服务器和许可证功能信息 任务说明 您可以显示分配给许可证功能的每个服务域所使用的许可证服务器。 步骤 运行 blstat -S. blstat -S FEATURE: feature1 SERVICE_DOMAIN: domain1 SERVERS INUSE FREE server1 1 0 server2 0 1 TOTAL 1 1 SERVICE_DOMAIN: domain2 SERVERS INUSE FREE server3 1 0 TOTAL 1 0 许可证功能部件 1 被分配给 domain1 服务域中的 server1 和 server2，以及 domain2 服务域中的 server3。当作业以 “ rusage [feature1 = 1]” 作为 rusage 字符串提交时，该作业将使用 feature1 许可功能。 查看许可证使用情况 步骤 运行 blstat -s 以显示许可证使用情况。 blstat -s FEATURE: p1_f2 SERVICE_DOMAIN: app_1 TOTAL_LICENSE: 10 LSF_USE LSF_DESERVE LSF_FREE NON_LSF_USE NON_LSF_DESERVE NON_LSF_FREE 0 10 10 0 0 0 FEATURE: p1_f1 SERVICE_DOMAIN: app_1 TOTAL_LICENSE: 5 LSF_USE LSF_DESERVE LSF_FREE NON_LSF_USE NON_LSF_DESERVE NON_LSF_FREE 0 5 5 0 0 0 如果有任何违反分发策略的行为，blstat 会在行的开头用星号（*）标记这些违反行为。 查看作业负载分配信息 步骤 运行 blinfo -a 以显示 WORKLOAD_DISTRIBUTION 信息。 blinfo -a FEATURE MODE SERVICE_DOMAIN TOTAL DISTRIBUTION g1 Project LS 10 [p1, 50.0%] [p2, 50.0%] WORKLOAD_DISTRIBUTION [LSF 66.7%, NON_LSF 33.3%] 排序许可证功能信息 任务说明 您可以按字母顺序，按总许可证或可用许可证，对许可证功能信息进行排序。 总许可证的价值，是根据向该功能部件提供许可证的所有服务域应得到的 LSF 工作负载应获得的许可证数量计算的，而不管非 LSF 工作负载是否从 LSF 工作负载借用了许可证。 步骤 按字母顺序排序: blstat -o alpha 按总许可证排序: blstat -o total 总许可证数量最多的功能会首先显示。 按可用许可证排序: blstat -o avail 首先显示具有可用许可证数量最多的功能。 您也可以运行带有选项 -Lp, -t, -D, -G, -s, -S 的 blstat -o。 提示 当 blstat -o 与不同选项一起使用时，“total licenses” 和 “licenses available” 的值计算方式不同： 选项 -Lp, -t, -D, -G: 许可证总数，是指从已配置的所有服务域分配给 LSF 作业负载的许可证总数，为该功能提供许可证。 由非 LSF 作业负载借用的许可证，将从该总和中减去。 选项 -s, -S: 许可证总数，是指配置为向该功能提供许可证的所有服务域中的所有许可证（由许可证供应商守护程序提供）。 查看执行主机上运行的多个作业的限制 如果用户提交的多个作业在同一执行主机上运行，则 blstat 可能不会显示正确的许可证使用信息。 这是因为 lmstat 仅提供每个许可证签出的用户和主机信息，而没有为 LSF License Scheduler 提供将许可证签出与特定 LSF 作业匹配的其他信息。 LSF License Scheduler 尝试根据用户，执行主机和 rusage 字符串，将许可证签出与每个 LSF 作业进行匹配。如果在同一执行主机上运行的多个作业，是由同一用户提交并请求相同的许可证，则 lmstat 提供的信息不足以使 LSF License Scheduler 为每个 LSF 作业提供完全匹配的信息。 LSF License Scheduler 估计作业，但这可能不正确。 例如： 有多个提供相同功能的服务域，并且用户提交在同一执行主机上运行的多个作业。 尽管 LSF License Scheduler 正确分配了令牌，但是 blstat 可能不会显示正确的令牌用法（例如 TOTAL_INUSE，TOTAL_RESERVE 或 TOTAL_FREE）。 不正确的令牌计入 OTHERS。 一台许可证服务器具有多个项目，一个用户提交多个作业，其中一些作业一次保留令牌。 保留令牌的作业与其他 LSF License Scheduler 作业在同一主机上运行。 尽管 LSF License Scheduler 正确分配了令牌，但 blstat 可能会显示相反的令牌用法，因此某些 INUSE 令牌计入 RESERVED 中，而某些 RESERVED 令牌计入 INUSE 中。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection1/Customize dynamic license information output.html":{"url":"chapter10/section5/subsection1/Customize dynamic license information output.html","title":"自定义动态许可证信息输出","keywords":"","body":"自定义动态许可证信息输出 默认情况下，blstat 命令显示一组预定义的动态许可证信息。虽然您可以使用各种 blstat 选项显示特定的动态许可证信息，但也可以使用 -f 选项来自定义 blstat 显示的特定字段。使用 -f 选项可创建特定的 blstat 输出格式，从而使您可以通过使用自定义脚本，轻松解析所需的信息或以预定义的格式显示信息。 blstat ... -f \"field_name[:[-][output_width]] ... [delimiter='character']\" 指定要显示的 blstat 字段（或别名，而不是完整的字段名称），显示的顺序和宽度。 仅指定 blstat 字段名称或别名，以将其输出设置为无限宽度和左对齐。 指定不带宽度的冒号（:)，以将输出宽度设置为该字段的建议宽度。 用宽度指定冒号（:)，以设置该字段显示的最大字符数。 当值超过此宽度时，blstat 将截断输出。 当 blstat 显示特定字段的输出时，请指定连字符（-）以设置右对齐。 如果未指定，则默认设置为设置左对齐。 使用 delimiter= 设置定界字符以在不同的标题和字段之间显示。 该定界符必须是单个字符。 默认情况下，定界符为空格。 输出自定义仅适用于不带任何选项的 blstat 命令，并适用于带有以下选项的 blstat 的输出：-Lp，-D，-t。 -f 选项不能与任何其他 blstat 选项一起使用。 以下是用于指定要显示的 blstat 字段的字段名称，建议的宽度，可以用来代替字段名称的别名，适用的 LSF License Scheduler 模式以及该字段的简要说明： Field 名称 宽度 别名 模式 描述 service_domain_name 19 sd_name 所有 服务域名称 service_domain_others 9 sd_others 服务域中 “others” 令牌的总数 service_domain_lsf_total 12 sd_lsf_total LSF 可以从服务域使用的令牌总数 service_domain_alloc 8 sd_alloc 集群模式 服务域分配的令牌总数 service_domain_use 7 sd_use 服务域中已使用令牌的总数 service_domain_inuse 8 sd_inuse 项目和快速调度模式 服务域中正在使用的令牌总数 service_domain_reserve 7 sd_rsv 服务域中保留令牌的总数 service_domain_free 7 sd_free 来自服务域的免费令牌总数 project_name 15 proj_name 项目名称 project_share 7 proj_share 项目份额数 project_own 7 proj_own 项目拥有的令牌数量 project_inuse 7 proj_inuse 项目正在使用的令牌数量 project_reserve 7 proj_rsv 项目保留的令牌数量 project_free 7 proj_free 项目的免费令牌数量 project_demand 8 proj_demand 项目所需的令牌数量 project_limit 7 proj_limit 项目限额 project_non_share 11 proj_non_share 项目的非共享令牌数 cluster_name 15 cl_name 所有 集群名称 cluster_share 7 cl_share 集群模式 集群中的份额数 cluster_alloc 7 cl_alloc 集群和快速调度模式 分配给集群的令牌数量 cluster_target 8 cl_target 集群的目标令牌数 cluster_inuse 7 cl_inuse 所有 集群中正在使用的令牌数 cluster_reserve 7 cl_rsv 集群中保留的令牌数 cluster_free 7 cl_free 集群中的免费令牌数量 cluster_over 7 cl_over 集群和快速调度模式 集群中过度使用的令牌数 cluster_demand 8 cl_demand 所有 集群需求 cluster_peak 7 cl_peak 集群模式 集群中已使用令牌的峰值数量 cluster_buffer 8 cl_buff 在集群中分配缓冲区 cluster_acum_use 10 cl_acum_use 项目模式 集群中已使用令牌的累计数量 cluster_scaled_acum 13 cl_scaled_acum 集群中已累积使用令牌的缩放数量 cluster_avail 7 cl_avail 集群中可用令牌的数量 feature_name 18 feat_name 所有 功能名称 feature_mode 20 feat_mode 特征模式 feature_lm_license_name 17 feat_lm_lic_name FlexNet 服务器中许可证功能的名称 提示 字段名称和别名不区分大小写。 输出宽度的有效值为任何正整数 1-4096。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection2/About error logs.html":{"url":"chapter10/section5/subsection2/About error logs.html","title":"关于错误日志","keywords":"","body":"关于错误日志 错误日志维护有关 License Scheduler 操作的重要信息。 提示 日志文件会随着时间增长。 偶尔清除或备份这些文件（手动或使用自动脚本）。 每次记录消息时都会重新打开日志文件，因此，如果重命名或删除守护程序日志文件，则守护程序会自动创建一个新的日志文件。 日志文件的位置通过 lsf.conf 中的参数 LSF_LOGDIR 指定。 LSF License Scheduler 系统守护程序的错误日志文件名是： bld.log.host_name blcollect.log.host_name 关于 blcollect 日志消息 blcollect 记录的消息包括以下信息： Time: 消息记录时间。 blcollect 名称: 服务域名，即许可证服务器主机名，由 blcollect 根据 lsf.licensescheduler 中的定义进行访问。 功能收集的状态报告：blcollect 信息是否成功收集。 详细信息：blcollect 收集的令牌数量，令牌名称，许可证令牌的许可证服务器名称。 管理日志文件 临时更改日志级别 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection2/Manage log files.html":{"url":"chapter10/section5/subsection2/Manage log files.html","title":"管理日志文件","keywords":"","body":"管理日志文件 任务说明 License Scheduler 会记录不同级别的错误消息，以便您可以选择记录所有消息，也可以仅记录被视为严重的消息。 步骤 将 lsf.licensescheduler 中的 LS_LOG_MASK 设置为所需的日志记录级别。 提示 如果未定义 LS_LOG_MASK，则使用 lsf.conf 中的 LSF_LOG_MASK 的值。 如果未定义 LS_LOG_MASK 或 LSF_LOG_MASK，则默认值为 LOG_WARNING。 日志级别（从最高到最低）： LOG_WARNING: 默认。 仅基本错误消息。 LOG_DEBUG: 调试消息的数量最少，对于调试问题很有用。 LOG_DEBUG1: 调试消息多于 LOG_DEBUG。 LOG_DEBUG2: 最常用的调试级别。 LOG_DEBUG3: 所有调试消息。 谨慎使用。 将记录指定级别和更高级别记录的消息，而较低级别的消息将被丢弃。 定期清理或备份日志文件。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection2/Temporarily change the log level.html":{"url":"chapter10/section5/subsection2/Temporarily change the log level.html","title":"临时更改日志级别","keywords":"","body":"临时更改日志级别 开始之前 您必须从运行守护程序的主机提交命令（仅适用于 bld）。 任务说明 您可以临时更改 bld 和 blcollect 守护程序的类或消息日志级别，而无需更改 lsf.licensescheduler。 所设置的消息日志级别自设置之时起一直有效，直到您将其关闭或守护程序停止运行为止（以较早者为准）。 如果重新启动该守护程序，则其消息日志级别将重置为 LS_LOG_MASK 的值，并且日志文件存储在 LSF_LOGDIR 指定的目录中。 步骤 设置 bld 的日志级别。 bladmin blddebug [-l debug_level] [-c class_name] 示例: bladmin blddebug -l 1 -c \"LC_TRACE LC_FLEX\" 记录在本地主机上运行的 bld 的消息，并将日志消息级别设置为 LOG_DEBUG1。 日志类为 LC_TRACE LC_FLEX。 将日志级别设置为 blcollect。 bladmin blcdebug [-l debug_level] collector_name ... | all 示例: bladmin blcdebug -l 3 all 所有收集器的日志掩码都更改为 LOG_DEBUG3。 将调试设置恢复为其配置值（在 lsf.licensescheduler 中用 LS_LOG_MASK 设置）。 bladmin blddebug -o bladmin blcdebug -o 示例 有关这些命令及其选项的详细说明，请参阅 IBM Spectrum LSF Command 参考. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection3/Troubleshooting.html":{"url":"chapter10/section5/subsection3/Troubleshooting.html","title":"故障排除","keywords":"","body":"故障排查 技巧 运行 blstat 以检查当前的许可证使用信息。 运行 blusers 以检查当前作业和许可证使用情况。 此信息是许可调度程序作业和 FlexNet 信息的已设置交集。 运行 blinfo 命令以检查当前的许可计划程序配置。 运行 bld -c 以检查配置是否正确。 带有 LOG_DEBUG 的此操作，将详细的配置设置写入调试日志。 通过设置 LSF_LOG_MASK = LOG_DEBUG 并使用 bladmin reconfig all 重新配置守护程序来打开调试。 在 lsf.conf 中为 mbatchd 调试（LSB_DEBUG_MBD ）设置日志类：LC_LICSCHED。 在 lsf.conf 中使用 LSB_TIME_SCH = timelevel（类似于 LSB_TIME_MBD）启用日志记录计时信息。 运行 bhosts -s 以检查资源是否已正确报告给 LSF。 文件位置 以下文件对于故障排除很有用。 检查 blstat 是否支持 lmstat 有些问题是由于 LSF License Scheduler 收集器不支持许可证管理器所致。 除非您定义了 LSF License Scheduler elim，否则不要删除 lsb.tokens lsb.tokens 文件包含有关分配给集群的功能的分配和使用情况信息。 除非您为 LSF License Scheduler 定义了elim，以确保即使 mbatchd 和 bld 之间的连接断开，LSF 仍可以计划 LSF License Scheduler作业，否则请勿删除或修改此文件。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection3/File locations.html":{"url":"chapter10/section5/subsection3/File locations.html","title":"文件位置","keywords":"","body":"文件位置 以下文件对于故障排除很有用。 BLD 日志位于标准 $LSF_LOGDIR 中。 BLCOLLECT 日志位于守护程序正在运行的主机上的 /tmp 或 $LSF_LOGDIR 中。 来自 BLD，BLCOLLECT，mbatchd，lim 和 mbsched 的核心文件位于守护程序本地主机上的 /tmp 中。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection3/Check that lmstat is supported by blcollect.html":{"url":"chapter10/section5/subsection3/Check that lmstat is supported by blcollect.html","title":"检查 blstat 是否支持 lmstat","keywords":"","body":"检查 blstat 是否支持 lmstat 有些问题是由于 LSF License Scheduler 收集器不支持许可证管理器所致。 步骤 创建shell脚本以输出（例如，echo）目标 lmstat 输出。 将 lsf.licensescheduler 中的 LMSTAT_PATH 指向 shell 脚本。 如果未设置 LIC_COLLECTOR，请重新启动 bld 以重新启动 blcollect。 如果设置了 LIC_COLLECTOR ，请杀死 blcollect，然后手动重启 blcollect。 观察 blcollect 日志以查看是否存在任何错误，以确定 blcollect 是否能够正确解析 lmstat 输出。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section5/subsection3/Do not delete lsb.tokens unless you defined a LSF License Scheduler elim.html":{"url":"chapter10/section5/subsection3/Do not delete lsb.tokens unless you defined a LSF License Scheduler elim.html","title":"除非您定义了 LSF License Scheduler elim，否则不要删除lsb.tokens","keywords":"","body":"除非您定义了 LSF License Scheduler elim，否则不要删除lsb.tokens lsb.tokens 文件包含有关分配给集群的功能的分配和使用情况信息。除非您为 LSF License Scheduler 定义了 elim 以确保即使 mbatchd 和 bld 之间的连接断开，LSF 仍可以调度 LSF License Scheduler 作业，否则不要删除或修改此文件。 当 mbatchd 与 bld 连接以获得负载更新时，mbatchd 从 bld 获取许可证使用信息并创建 lsb.tokens 文件 (位于 LSB_SHAREDIR/cluster_name/logdir) 以保存分配给集群的所有功能的分配和使用情况。lsf.licensescheduler 中的 MBD_REFRESH_INTERVAL 参数控制加载更新之间的最小间隔，还指定了 lsb.tokens 文件的更新间隔。 该文件可确保即使 mbatchd 和 bld 之间的连接断开，LSF 仍可以使用 lsb.tokens 中的最后许可证使用信息，来计划新的 LSF License Scheduler 作业。 如果删除 lsb.tokens，则 LSF 无法调度 LSF License Scheduler 作业。 创建 LSF 许可证调度程序 elim 如果 mbatchd 和 bld 之间的连接由于网络故障或 bld 断开而断开，您还可以定义一个动态资源，其名称与 LSF License Scheduler 功能相同，并且 创建一个 elim 以获取免费许可证的数量，以确保 LSF 仍可以计划待执行的LSF License Scheduler 作业。 在这种情况下，如果 bld 关闭或断开连接，则即使 bld 关闭，LSF 许可证调度程序资源也会覆盖 LSF 动态资源，并且任何作业都无法保留该资源。 为避免此问题，请在 LSF License Scheduler 关闭或断开连接时删除 lsb.tokens 文件。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"chapter10/section6/Reference.html":{"url":"chapter10/section6/Reference.html","title":"参考","keywords":"","body":"参考 lsf.licensescheduler lsf.licensescheduler 文件包含 LSF License Scheduler 配置信息。 除 ProjectGroup 之外的所有部分都是必需的。 在集群模式下，也不需要 “Project” 部分。 bladmin IBM Spectrum LSF 许可证计划程序的管理工具。 blcollect LSF License Scheduler 的许可证信息收集守护程序。 blcollect 守护程序收集许可证使用信息。 blcstat 显示来自 LSF License Scheduler 的 blcollect 守护程序的动态更新信息。 blhosts 显示正在运行 LSF License Scheduler 守护程序（bld）的所有主机的名称。 blinfo 显示静态 LSF License Scheduler 配置信息 blkill 终止交互式（taskman）LSF License Scheduler 任务。 blparams 显示有关在文件 lsf.licensescheduler 和 lsf.conf 中定义的可配置 LSF License Scheduler 参数的信息。 blstat 显示动态许可证信息。 bltasks 显示 LSF License Scheduler 交互式任务信息。 blusers 显示 LSF License Scheduler 的许可证使用信息。 fod.conf fod.conf 文件包含 FOD 配置信息。 所有部分都是必需的。 fodadmin 在 FOD 下启动应用程序或关闭 FOD fodapps 显示由 FOD 管理的应用程序的状态。 fodhosts 显示 FOD 主机的状态。 fodid 显示 FOD 主站主机和版本信息。 taskman 签出许可证令牌并管理交互式 UNIX 应用程序。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-24 21:46:34 "},"NOTE.html":{"url":"NOTE.html","title":"后记","keywords":"","body":"后记 了解作者 作者技术博客：BYA's Blog 邮箱：bya@mail.ustc.edu.cn; baiyongan1507@163.com 更新说明 原则： 具体内容优先级，按工作需求，择重点进行翻译，总结。 长期不定时更新关于前置知识，行业经验的总结。 要求： 翻译：主要借助谷歌翻译，保证文字流畅。 排版格式：主要参照 《中文技术文档写作规范》 Github Repo：baiyongan/LSF_Doc 工具： 电子书编辑：gitbook、calibre markdown：typora 版本控制：git 内容评估： 更新频率：平均每天一个 section / subsection chapter 1：5 section chapter 2：略 chapter 3：4 section chapter 4：3 section， 6 subsection chapter 5：3 section，7 subsection chapter 6：14 section，66 subsection chapter 7：只链接，不翻译 chapter 8：10 section，9 subsection chapter 9：23 section chapter 10：6 section，8 subsection 译作记录 2020.7.2 开始，配置 gitbook， 预计一个月左右，业余时间完成翻译初稿。将会是一个漫长、寂寞而枯燥的翻译、遣词造句、排版、美化的过程。 | 内容 | 优先级 | 初稿 | 定稿 | | ------------------------------------------------------------ | ------ | ------------------------------ | ---- | | 说明、后记等，目录框架调整 | 高 | 7/2~7/4，7/5 afternoon | Y | | chapter 1 快速入门部分--section 7 | 高 | 7/7 1:00 am | Y | | 每一章的简介 README 部分 | 高 | 7/8 0:30 am, 9:00 am | Y | | chapter 1 增加第一小节 LSF 简介，更新参考资料 | 低 | 7/8 15:00 pm | Y | | chapter 3 普通用户操作基本章节 共4 section | 高 | 7/8 21:00 pm | Y | | 添加附录、参考文献等，临时新增 草稿页面 | 低 | 7/9 9:00 am | N | | chapter 3 section2 | 高 | 7/9 10:00 am | Y | | chapter 3 section 2 & 3 & 草稿页面 | 高 | 7/10 11:00 am 11:30 pm | Y | | chapter 3 section 4 & chapter 4 content | 高 | 7/11 9:00 am | Y | | chapter 2 & 6 content | 中 | 7/12 8:30 am | Y | | chapter 4 section1 | 高 | 7/13 9:30am | Y | | chapter4 section2 subsection1 & chapter 10 added | 高 | 7/21 9:30am | Y | | chapter4 section2 subsection2 & chapter 10 content | 高 | 7/22 9:30am | Y | | chapter 10 content | 高 | 7/23 11:20 pm | Y | | chapter 10 section1 part of section2 | 高 | 7/27 9:00 am | N | | chapter 10 section 2 overview | 高 | 7/28 14:20 pm | Y | | chapter 10 section6 & chapter4 section2 subsection3 | 高 | 7/30 17:30 pm 20:30pm | Y | | chapter 10 section 2 | 高 | 7/31 20:00 pm | N | | chapter 10 section 3 chapter 4 section 2 subsection3 & 4 & 5 | 高 | 8/1 10:00 am 16:00 pm 20:00 pm | Y | | chapter 4 section 2 subsection6 chapter 10 section4 | 高 | 8/3 14:00 pm | Y | | chapter 10 section 4 part | 高 | 8/4 20:30 pm | N | | chapter 10 section 4 part | 高 | 8/5 10:00 am | Y | | chapter 10 section 5 & section 4 part | 高 | 8/6 10:00 am | Y | | | | | | | | | | | | | | | | | | | | | | | | | | | chapter 4 管理员基本操作 共 3 section 6subsection | 高 | | | | chapter 5 作业调度管理章节 共 14 section | 高 | | | | chapter 6 集群管理高级操作 | 中高 | | | | chapter 11~ 12 个人经验部分 | 中高 | | | | chapter 7 参考文档 | 中 | | | | chapter 8 LSF 拓展 | 中低 | | | | chapter 9 最佳实践 | 低 | | | | chapter 2 安装、迁移 | 低 | | | | chapter 1 入门介绍完善 | 低 | | | | 全体校验、发布电子版 | 低 | | | 2020.8 开始着重于 同行业应用 的结合，与经验总结。 2020.9 长期优化更新，搜集反馈。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-27 22:21:48 "},"APPENDIX.html":{"url":"APPENDIX.html","title":"附录","keywords":"","body":"附录 部分关注企业/机构 超算平台 中科大超算中心 全球 Top 500 超级计算机 HPC 云平台 华为云｜高性能计算解决方案 阿里云｜弹性高性能计算 E-HPC © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-27 22:05:52 "},"REFERENCE.html":{"url":"REFERENCE.html","title":"参考资料","keywords":"","body":"参考资料 主要参考 IBM Spectrum LSF V10.1 documentation Slurm version 20.02 GitBook 文档（中文版） IBM 系列 IBM Knowledge Center IBM 知识中心 IBM Developer IBM 开发者平台 IBM Support Product List IBM 支持团队产品目录 Platform Computing Legacy Documentation Platform Computing PDF 文档 相关 github 项目 IBM Spectrum Computing Public GitHub Respository SchedMD/slurm baiyongan/hpc-logfile-analysis © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2022-09-27 22:06:16 "}}